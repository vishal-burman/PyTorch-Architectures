{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_MLPMixer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhSbJrx0G1d3pfE2BOgjV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_MLPMixer/test_sample_MLPMixer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tru12rP_2qsX"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KRyA3xJRs-Y"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZt9XTA1fzqM"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lkq69Sf-gl"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56W8XTw18afv"
      },
      "source": [
        "# ! rm -rf PyTorch-Architectures/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj7IRx5hscag"
      },
      "source": [
        "! kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A55wOMksgO5M"
      },
      "source": [
        "# ! kaggle datasets download -d dansbecker/hot-dog-not-hot-dog\n",
        "# ! unzip -q hot-dog-not-hot-dog.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fupdC_yhgSmx"
      },
      "source": [
        "! unzip -q train.zip\n",
        "! rm train.zip\n",
        "! rm test1.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALnI5szug5Q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71da00c-79a0-421a-874c-e3e1e89ed756"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/modeling_MLPMixer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'PyTorch-Architectures' already exists and is not an empty directory.\n",
            "/content/PyTorch-Architectures/modeling_MLPMixer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou9KOPh7h3AQ"
      },
      "source": [
        "! mv /content/train .\n",
        "# ! mv /content/test1 ."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaAFKDUhgVR0"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from model import MLPMixer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "840tJ5lIo6p2",
        "outputId": "38c51c95-7e2e-477a-bb3d-89989a7e03f5"
      },
      "source": [
        "samples = os.listdir('train')\n",
        "print('Total Samples: ', len(samples))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPG3bLxsrg2T"
      },
      "source": [
        "parent_path = '/content/PyTorch-Architectures/modeling_MLPMixer/train/'\n",
        "samples = [parent_path + str(sample) for sample in samples]\n",
        "samples = [(sample, 1) if 'dog' in sample else (sample, 0) for sample in samples]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTuNDBqVq2Ig",
        "outputId": "d37a4334-3e27-43fe-db0d-8c44baee4d73"
      },
      "source": [
        "random.shuffle(samples)\n",
        "\n",
        "split = 90 * len(samples) // 100\n",
        "train_samples = samples[:split]\n",
        "valid_samples = samples[split:]\n",
        "\n",
        "print('Train Samples: ', len(train_samples))\n",
        "print('Valid Samples: ', len(valid_samples))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples:  22500\n",
            "Valid Samples:  2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKqNrgl3j_O7"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, path_images, transforms=None):\n",
        "    self.path_images = path_images\n",
        "    self.transforms = transforms\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.path_images)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    images, labels = self.path_images[idx][0], self.path_images[idx][1]\n",
        "    images = Image.open(images)\n",
        "    if self.transforms is not None:\n",
        "      images = self.transforms(images)\n",
        "    return {\n",
        "        'img': images,\n",
        "        'labels': torch.tensor(labels),\n",
        "    }\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "                                 transforms.Resize(size=(256, 256)),\n",
        "                                 transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cECLb_5uyeXP",
        "outputId": "d7af92ef-d2da-4359-9300-635723339c87"
      },
      "source": [
        "sample_dataset = CustomDataset(path_images=train_samples, transforms=transformations)\n",
        "sample_loader = DataLoader(dataset=sample_dataset, batch_size=2)\n",
        "for sample in sample_loader:\n",
        "  print(sample['img'].shape, sample['labels'].shape)\n",
        "  print(sample['labels'])\n",
        "  break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 256, 256]) torch.Size([2])\n",
            "tensor([1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OXLq3NN5Cxw"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MLPMixer(image_size=256,\n",
        "                 patch_size=16,\n",
        "                 channel=3,\n",
        "                 dim=512,\n",
        "                 depth=8,\n",
        "                 num_classes=2)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3No8h1hqWnf",
        "outputId": "c68a7403-d7ff-4091-c699-a33cb45ede4f"
      },
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Trainable Parameters: ', params)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable Parameters:  3613698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqXz6OVCqgpC"
      },
      "source": [
        "# Hyperparameter section\n",
        "EPOCHS = 3\n",
        "LR = 3e-4\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EILP_jTUrVA7",
        "outputId": "ec42f098-7786-4f12-a7c9-9b28b2d8011a"
      },
      "source": [
        "train_dataset = CustomDataset(path_images=train_samples, transforms=transformations)\n",
        "valid_dataset = CustomDataset(path_images=valid_samples, transforms=transformations)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print('Length of Train Loader: ', len(train_loader))\n",
        "print('Length of Valid Loader: ', len(valid_loader))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train Loader:  352\n",
            "Length of Valid Loader:  40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbc6y7qir3Aq"
      },
      "source": [
        "# Sanity check forward pass\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False):\n",
        "  imgs = sample['img'].to(device)\n",
        "  labels = sample['labels'].to(device)\n",
        "  outputs = model(img=imgs)\n",
        "  assert outputs.size(0) == imgs.size(0), \"Batch sizes don't match\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOCScwSe1B-F"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Ymwb_o1mmY",
        "outputId": "b2eb12d6-1d55-48c0-eca1-d2cca0d9c382"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "  correct, total = 0, 0\n",
        "  with torch.set_grad_enabled(False):\n",
        "    for sample in data_loader:\n",
        "      imgs = sample['img'].to(device)\n",
        "      labels = sample['labels'].to(device)\n",
        "      logits = model(imgs)\n",
        "      prob = F.softmax(logits, dim=-1)\n",
        "      _, preds = torch.max(prob, 1)\n",
        "      correct += (preds == labels).sum()\n",
        "      total += labels.size(0)\n",
        "  return correct.float() / total * 100\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  for idx, sample in enumerate(train_loader):\n",
        "    imgs = sample['img'].to(device)\n",
        "    labels = sample['labels'].to(device)\n",
        "    logits = model(imgs)\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print('Epoch: %04d/%04d || Batch: %04d/%04d || Loss: %.2f' % (epoch+1,\n",
        "                                                                    EPOCHS,\n",
        "                                                                    idx,\n",
        "                                                                    len(train_loader),\n",
        "                                                                    loss.item()))\n",
        "  model.eval()\n",
        "  train_acc = compute_accuracy(model, train_loader, device)\n",
        "  valid_acc = compute_accuracy(model, valid_loader, device)\n",
        "  print('Train Accuracy: %.2f%% || Valid Accuracy: %.2f%%' % (train_acc.item(),\n",
        "                                                              valid_acc.item()))\n",
        "  epoch_elapsed_time = (time.time() - start_time) / 60\n",
        "  print('Epoch Elapsed Time: %.2f min' % (epoch_elapsed_time))\n",
        "total_training_time = (time.time() - start_time) / 60\n",
        "print('Total Training Time: %.2f min' % (total_training_time))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001/0003 || Batch: 0000/0352 || Loss: 0.69\n",
            "Epoch: 0001/0003 || Batch: 0100/0352 || Loss: 0.70\n",
            "Epoch: 0001/0003 || Batch: 0200/0352 || Loss: 0.72\n",
            "Epoch: 0001/0003 || Batch: 0300/0352 || Loss: 0.72\n",
            "Train Accuracy: 62.27% || Valid Accuracy: 62.20%\n",
            "Epoch Elapsed Time: 8.39 min\n",
            "Epoch: 0002/0003 || Batch: 0000/0352 || Loss: 0.69\n",
            "Epoch: 0002/0003 || Batch: 0100/0352 || Loss: 0.62\n",
            "Epoch: 0002/0003 || Batch: 0200/0352 || Loss: 0.63\n",
            "Epoch: 0002/0003 || Batch: 0300/0352 || Loss: 0.68\n",
            "Train Accuracy: 66.47% || Valid Accuracy: 64.92%\n",
            "Epoch Elapsed Time: 16.79 min\n",
            "Epoch: 0003/0003 || Batch: 0000/0352 || Loss: 0.65\n",
            "Epoch: 0003/0003 || Batch: 0100/0352 || Loss: 0.58\n",
            "Epoch: 0003/0003 || Batch: 0200/0352 || Loss: 0.59\n",
            "Epoch: 0003/0003 || Batch: 0300/0352 || Loss: 0.61\n",
            "Train Accuracy: 72.18% || Valid Accuracy: 69.92%\n",
            "Epoch Elapsed Time: 25.20 min\n",
            "Total Training Time: 25.20 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF99HyX42dVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}