{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"!git clone https://github.com/vishal-burman/PyTorch-Architectures.git","execution_count":1,"outputs":[{"output_type":"stream","text":"Cloning into 'PyTorch-Architectures'...\nremote: Enumerating objects: 371, done.\u001b[K\nremote: Counting objects: 100% (371/371), done.\u001b[K\nremote: Compressing objects: 100% (229/229), done.\u001b[K\nremote: Total 802 (delta 198), reused 275 (delta 117), pack-reused 431\u001b[K\nReceiving objects: 100% (802/802), 8.42 MiB | 12.68 MiB/s, done.\nResolving deltas: 100% (474/474), done.\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"%cd PyTorch-Architectures/modeling_gpt2/","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/working/PyTorch-Architectures/modeling_gpt2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install transformers\n! pip install datasets","execution_count":3,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: tokenizers==0.9.2 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers) (3.13.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.94)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf->transformers) (1.14.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf->transformers) (46.1.3.post20200325)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nCollecting datasets\n  Downloading datasets-1.1.2-py3-none-any.whl (147 kB)\n\u001b[K     |████████████████████████████████| 147 kB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.10)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.18.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.10)\nRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.45.0)\nCollecting pyarrow>=0.17.1\n  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n\u001b[K     |████████████████████████████████| 17.7 MB 32.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.23.0)\nCollecting xxhash\n  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 57.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\nInstalling collected packages: pyarrow, xxhash, datasets\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 0.16.0\n    Uninstalling pyarrow-0.16.0:\n      Successfully uninstalled pyarrow-0.16.0\nSuccessfully installed datasets-1.1.2 pyarrow-2.0.0 xxhash-2.0.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer\nfrom model import GPT2Classify\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset = load_dataset('imdb')\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = '[PAD]'\nmodel = GPT2Classify().to(device)","execution_count":4,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1902.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871defa72598496fbcfb95cbff0c1936"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1004.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842e34ea654a463f8293c31d77d58571"}},"metadata":{}},{"output_type":"stream","text":"\nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=84125825.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaad46ffff68495fa1072a2e5938e637"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3. Subsequent calls will reuse this data.\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc767fb709742d29c0fe5be3d93d6b6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fa7b71672f4479ae2491ad48b7fda1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters = \", total_params)","execution_count":5,"outputs":[{"output_type":"stream","text":"Total Parameters =  60650496\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, tokenizer, list_texts, list_labels, max_seq_length=128):\n        self.tokenizer = tokenizer\n        self.list_texts = list_texts\n        self.list_labels = list_labels\n        self.max_seq_length = max_seq_length\n        self.list_train = []\n        self.build()\n    \n    def __len__(self):\n        return len(self.list_train)\n    \n    def __getitem__(self, index):\n        input_ids = self.list_train[index]['input_ids']\n        attention_mask = self.list_train[index]['attention_mask']\n        label = self.list_train[index]['label']\n        return {\n            'ids': torch.tensor(input_ids, dtype=torch.long),\n            'mask': torch.tensor(input_ids, dtype=torch.long),\n            'label': torch.tensor(label),\n        }\n    \n    def build(self):\n        for text, label in zip(self.list_texts, self.list_labels):\n            tokens = self.tokenizer(text, max_length=self.max_seq_length, padding='max_length', truncation=True)\n            self.list_train.append({'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask'], 'label': label})","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts_train = dataset['train']['text'][:22500]\nlabels_train = dataset['train']['label'][:22500]\n\ntexts_valid = dataset['train']['text'][22500:]\nlabels_valid = dataset['train']['label'][22500:]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TextDataset(tokenizer, texts_train, labels_train)\nvalid_dataset = TextDataset(tokenizer, texts_valid, labels_valid)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(dataset=valid_dataset, shuffle=False, batch_size=batch_size * 2)\ntest_loader = DataLoader(dataset=valid_dataset, shuffle=False, batch_size=batch_size * 2)\n\nprint(\"Length of Train loader: \", len(train_loader))\nprint(\"Length of Valid loader: \", len(valid_loader))\nprint(\"Length of Test loader: \", len(test_loader))","execution_count":9,"outputs":[{"output_type":"stream","text":"Length of Train loader:  1407\nLength of Valid loader:  79\nLength of Test loader:  79\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Train Loader:\nfor sample in train_loader:\n    ids = sample['ids'].to(device)\n    mask = sample['mask'].to(device)\n    labels = sample['label'].to(device)\n    print(ids.shape, mask.shape, labels.shape)\n    break","execution_count":10,"outputs":[{"output_type":"stream","text":"torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 5e-5\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 3\n\ndef compute_accuracy(model, data_loader, device):\n    correct, total = 0, 0\n    model.eval()\n    with torch.set_grad_enabled(False):\n        for sample in data_loader:\n            ids = sample['ids'].to(device)\n            mask = sample['mask'].to(device)\n            labels = sample['label'].to(device)\n            outputs = model(input_ids=ids, attention_mask=mask)\n            logits = outputs[0]\n            probas = F.softmax(logits, dim=-1)\n            _, predicted_labels = torch.max(probas, 1)\n            correct += (predicted_labels == labels).sum()\n            total += labels.size(0)\n    return correct.float() / total * 100\n        \nstart_time = time.time()\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch_idx, sample in enumerate(train_loader):\n        ids = sample['ids'].to(device)\n        mask = sample['mask'].to(device)\n        labels = sample['label'].to(device)\n\n        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n        loss = outputs[0]\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # LOGGING\n        if batch_idx % 200 == 0:\n            print(\"Batch: %04d/%04d || Epoch: %04d/%04d\" % (batch_idx, len(train_loader), epoch+1, EPOCHS))\n    \n    model.eval()\n    with torch.set_grad_enabled(False):\n        # TODO define function\n        train_accuracy = compute_accuracy(model, train_loader, device)\n        valid_accuracy = compute_accuracy(model, valid_loader, device)\n        print(\"Train Accuracy: %.2f || Valid Accuracy: %.2f\" % (train_accuracy, valid_accuracy))\n    elapsed_time = (time.time() - start_time) / 60\n    print(\"Epoch Elapsed Time: \", elapsed_time)\nelapsed_time = (time.time() - start_time) / 60\nprint(\"Total Training Time: \", elapsed_time)","execution_count":12,"outputs":[{"output_type":"stream","text":"Batch: 0000/0250 || Epoch: 0001/0003\nBatch: 0200/0250 || Epoch: 0001/0003\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-faee8e8a6b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# LOGGING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}