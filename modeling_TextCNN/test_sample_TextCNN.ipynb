{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_TextCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJCCs1IOGObJSWwQqJgWqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_TextCNN/test_sample_TextCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anwa-WESKrHz"
      },
      "source": [
        "# ! rm -rf PyTorch-Architectures/\n",
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/modeling_TextCNN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4rOBJgnvKc"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXO5JBVVndIC"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from model import TextCNN\n",
        "from transformers import BertTokenizer\n",
        "from datasets import load_dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHU_eflQoSb1",
        "outputId": "faccdd7e-6499-4a95-d98a-6d7089ecc635"
      },
      "source": [
        "dataset = load_dataset(\"tweets_hate_speech_detection\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACQB711QoUMA",
        "outputId": "459f5a73-d608-4142-e7d8-59675722a3bc"
      },
      "source": [
        "sentences = []\n",
        "for sample in dataset['train']:\n",
        "  text = sample['tweet']\n",
        "  label = sample['label']\n",
        "  sentences.append({\n",
        "      'text': text,\n",
        "      'label': label,\n",
        "  })\n",
        "print('Length of total samples: ', len(sentences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of total samples:  31962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4JZXHx_oWGo"
      },
      "source": [
        "random.shuffle(sentences)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoqp4SuCoYw3",
        "outputId": "38d4d3ae-5a71-4e59-e07b-fab766f131ab"
      },
      "source": [
        "lim = 90 * len(sentences) // 100\n",
        "train_sentences = sentences[:lim]\n",
        "valid_sentences = sentences[lim:]\n",
        "\n",
        "print('Length of Train samples: ', len(train_sentences))\n",
        "print('Length of Valid samples: ', len(valid_sentences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train samples:  28765\n",
            "Length of Valid samples:  3197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XL0-Yh7oaVL"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, tokenizer, list_samples, max_input_length=4):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.list_samples = list_samples\n",
        "    self.max_input_length = max_input_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.list_samples)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    samples = self.list_samples[idx]\n",
        "    texts = samples['text']\n",
        "    labels = samples['label']\n",
        "    tokens = self.tokenizer(texts, max_length=self.max_input_length, add_special_tokens=False, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids']\n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'tgt': torch.tensor(labels),\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDHil8Bodwx"
      },
      "source": [
        "# Define BERT tokenizer without special [PAD] or [CLS] token in CustomDataset\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn3T_o9DomUn"
      },
      "source": [
        "# Hyperparameters\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "PAD_IDX = tokenizer.pad_token_id\n",
        "EMBEDDING_SIZE = 8\n",
        "NUM_FILTERS = 3\n",
        "FILTER_SIZES = [2, 2, 2]\n",
        "NUM_CLASSES = 2\n",
        "MAX_INPUT_LENGTH = 8\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzSjALs6rdbu",
        "outputId": "bb3c64f3-99e6-4681-d3f7-da16fa157539"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextCNN(num_filters=NUM_FILTERS,\n",
        "                filter_sizes=FILTER_SIZES,\n",
        "                vocab_size=VOCAB_SIZE,\n",
        "                embedding_size=EMBEDDING_SIZE,\n",
        "                sequence_length=MAX_INPUT_LENGTH,\n",
        "                num_classes=NUM_CLASSES,\n",
        "                padding_idx=PAD_IDX)\n",
        "model.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextCNN(\n",
              "  (W): Embedding(30522, 8, padding_idx=0)\n",
              "  (Weight): Linear(in_features=9, out_features=2, bias=False)\n",
              "  (filter_list): ModuleList(\n",
              "    (0): Conv2d(1, 3, kernel_size=(2, 8), stride=(1, 1))\n",
              "    (1): Conv2d(1, 3, kernel_size=(2, 8), stride=(1, 1))\n",
              "    (2): Conv2d(1, 3, kernel_size=(2, 8), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qL7O9EJsNf9",
        "outputId": "24e20a29-1326-4f43-ce63-6def2dfd0ee3"
      },
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Trainable Parameters: ', params)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable Parameters:  244349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC5Cd_KIsZe0",
        "outputId": "380da963-8fdb-46d7-e946-a0e2173bf63a"
      },
      "source": [
        "train_dataset = CustomDataset(tokenizer, train_sentences, max_input_length=MAX_INPUT_LENGTH)\n",
        "valid_dataset = CustomDataset(tokenizer, valid_sentences, max_input_length=MAX_INPUT_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Length of Training Loader: \", len(train_loader))\n",
        "print(\"Length of Valid Loader: \", len(valid_loader))\n",
        "\n",
        "# Sanity check loaders\n",
        "for sample in train_loader:\n",
        "  assert sample['ids'].squeeze(1).dim() == 2\n",
        "  assert sample['tgt'].size(0) == sample['ids'].size(0)\n",
        "  break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Training Loader:  899\n",
            "Length of Valid Loader:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PTH1k1_sq3g"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGXMWDyTsxLr",
        "outputId": "62ee9dff-bf49-42e0-9858-9793cdd2f8fa"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "  correct_preds, total_examples = 0, 0\n",
        "  with torch.set_grad_enabled(False):\n",
        "    for sample in data_loader:\n",
        "      ids = sample['ids'].squeeze(1).to(device)\n",
        "      tgt = sample['tgt']\n",
        "      logits = model(ids)\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      _, predicted_labels = torch.max(probs, 1)\n",
        "      correct_preds += (predicted_labels == tgt).sum()\n",
        "      total_examples += tgt.size(0)\n",
        "  return correct_preds / total_examples * 100 \n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  for batch_idx, sample in enumerate(train_loader):\n",
        "    ids = sample['ids'].squeeze(1).to(device)\n",
        "    tgt = sample['tgt'].to(device)\n",
        "\n",
        "    logits = model(ids)\n",
        "    loss = F.cross_entropy(logits, tgt)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # LOGGING\n",
        "    if batch_idx % 200 == 0:\n",
        "      print('Batch: %04d/%04d || Epoch: %04d/%04d || Loss: %.2f' % (batch_idx, len(train_loader), epoch+1, EPOCHS, loss.item()))\n",
        "\n",
        "  model.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_accuracy = compute_accuracy(model, train_loader, device)\n",
        "    valid_accuracy = compute_accuracy(model, valid_loader, device)\n",
        "    print('Train Accuracy: %.2f%%' % (train_accuracy.item()))\n",
        "    print('Valid Accuracy: %.2f%%' % (valid_accuracy.item()))\n",
        "  epoch_elapsed_time = (time.time() - start_time) / 60\n",
        "  print('Epoch Elapsed Time: %.2f min' % (epoch_elapsed_time))\n",
        "total_training_time = (time.time() - start_time) / 60\n",
        "print('Total Training Time: %.2f min' % (total_training_time))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0899 || Epoch: 0001/0003 || Loss: 0.58\n",
            "Batch: 0200/0899 || Epoch: 0001/0003 || Loss: 0.17\n",
            "Batch: 0400/0899 || Epoch: 0001/0003 || Loss: 0.14\n",
            "Batch: 0600/0899 || Epoch: 0001/0003 || Loss: 0.14\n",
            "Batch: 0800/0899 || Epoch: 0001/0003 || Loss: 0.16\n",
            "Train Accuracy: 92.98%\n",
            "Valid Accuracy: 93.06%\n",
            "Epoch Elapsed Time: 0.70 min\n",
            "Batch: 0000/0899 || Epoch: 0002/0003 || Loss: 0.43\n",
            "Batch: 0200/0899 || Epoch: 0002/0003 || Loss: 0.06\n",
            "Batch: 0400/0899 || Epoch: 0002/0003 || Loss: 0.21\n",
            "Batch: 0600/0899 || Epoch: 0002/0003 || Loss: 0.06\n",
            "Batch: 0800/0899 || Epoch: 0002/0003 || Loss: 0.17\n",
            "Train Accuracy: 93.07%\n",
            "Valid Accuracy: 93.21%\n",
            "Epoch Elapsed Time: 1.40 min\n",
            "Batch: 0000/0899 || Epoch: 0003/0003 || Loss: 0.12\n",
            "Batch: 0200/0899 || Epoch: 0003/0003 || Loss: 0.10\n",
            "Batch: 0400/0899 || Epoch: 0003/0003 || Loss: 0.09\n",
            "Batch: 0600/0899 || Epoch: 0003/0003 || Loss: 0.35\n",
            "Batch: 0800/0899 || Epoch: 0003/0003 || Loss: 0.19\n",
            "Train Accuracy: 93.36%\n",
            "Valid Accuracy: 93.40%\n",
            "Epoch Elapsed Time: 2.10 min\n",
            "Total Training Time: 2.10 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjZHKB6ts9Kc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}