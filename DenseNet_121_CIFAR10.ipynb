{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet-121-CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwnkGa9e31tkjkitmWT8/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/DenseNet_121_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e08eDVJhUBKz"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import torch.utils.checkpoint as cp\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSKqwF2wUgqu"
      },
      "source": [
        "###################\n",
        "# Model Settings\n",
        "###################\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Architecture\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "grayscale = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly631DL-U7w5",
        "outputId": "91491c46-7452-4c6f-d753-1826ea39631e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "####################\n",
        "# CIFAR-10 Dataset\n",
        "####################\n",
        "\n",
        "# training samples ~ 48000 samples\n",
        "train_indices = torch.arange(0, 48000)\n",
        "# validation samples ~ 2000 samples\n",
        "valid_indices = torch.arange(48000, 50000)\n",
        "\n",
        "train_and_valid = datasets.CIFAR10(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "train_dataset = Subset(train_and_valid, train_indices)\n",
        "valid_dataset = Subset(train_and_valid, valid_indices)\n",
        "test_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                train=False,\n",
        "                                transform=transforms.ToTensor(),\n",
        "                                download=False)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=4,\n",
        "                          shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=4,\n",
        "                         shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVh2vtQkWx4x",
        "outputId": "55b4a9a6-062b-42f2-d84e-0f20039f84c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sanity check the loaders\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  for batch_idx, (features, label) in enumerate(train_loader):\n",
        "\n",
        "    print(\"Epoch: %d | Batch: %d | Batch Size: %d\" % (epoch+1, batch_idx, label.size()[0]))\n",
        "\n",
        "    features = features.to(device)\n",
        "    label = label.to(device)\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Batch: 0 | Batch Size: 128\n",
            "Epoch: 2 | Batch: 0 | Batch Size: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZacC1dbnX-Zu",
        "outputId": "33a7a241-cb1a-4c25-f1ed-b34af526f76a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sanity check shuffling\n",
        "# label indices should be in different order\n",
        "# label order should be different in second epoch\n",
        "\n",
        "for images, labels in train_loader:\n",
        "  pass\n",
        "print(labels[:10])\n",
        "\n",
        "for images, labels in train_loader:\n",
        "  pass\n",
        "print(labels[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 0, 1, 3, 3, 5, 0, 4, 9, 4])\n",
            "tensor([1, 0, 4, 1, 8, 2, 0, 3, 5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoEBXVvYx19",
        "outputId": "6a2735b3-88ec-44b2-f744-36701b29cbe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sanity check ~ validation and test set should be diverse\n",
        "# ~ should contain all classes\n",
        "\n",
        "for images, labels in valid_loader:\n",
        "  pass\n",
        "print(labels[:10])\n",
        "\n",
        "for images, labels in test_loader:\n",
        "  pass\n",
        "print(labels[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5, 0, 3, 6, 8, 7, 9, 5, 6, 6])\n",
            "tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf_VAMjVZZU3"
      },
      "source": [
        "#######################\n",
        "# Model\n",
        "######################\n",
        "\n",
        "def _bn_function_factory(norm, relu, conv):\n",
        "  def bn_function(*inputs):\n",
        "    # TODO Add dimension changes\n",
        "    concated_features = torch.cat(inputs, 1)\n",
        "    bottleneck_output = conv(relu(norm(concated_features)))\n",
        "    return bottleneck_output\n",
        "  return bn_function\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "  def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "    super(_DenseLayer, self).__init__()\n",
        "    self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "    self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "    self.add_module('conv1', nn.Conv2d(in_channels=num_input_features,\n",
        "                                       out_channels=bn_size * growth_rate,\n",
        "                                       kernel_size=1,\n",
        "                                       stride=1,\n",
        "                                       bias=False)),\n",
        "    self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "    self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "    self.add_module('conv2', nn.Conv2d(in_channels=bn_size * growth_rate,\n",
        "                                       out_channels=growth_rate,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1,\n",
        "                                       bias=False)),\n",
        "    self.drop_rate = drop_rate\n",
        "    self.memory_efficient = memory_efficient\n",
        "  \n",
        "  def forward(self, *prev_features):\n",
        "    # TODO Add dimensions\n",
        "    bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
        "    if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_efatures):\n",
        "      bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
        "    else:\n",
        "      bottleneck_output = bn_function(*prev_features)\n",
        "    new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "    if self.drop_rate > 0:\n",
        "      new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                               training=self.training)\n",
        "    return new_features"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX-PlFcGfJfm"
      },
      "source": [
        "class _DenseBlock(nn.Module):\n",
        "  def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "    super(_DenseBlock, self).__init__()\n",
        "    # num_layers = 6 (i = 0)\n",
        "    # num_layers = 12 (i = 1)\n",
        "    # num_layers = 24 (i = 2)\n",
        "    # num_layers = 16 (i = 3)\n",
        "    for i in range(num_layers):\n",
        "      layer = _DenseLayer(\n",
        "          num_input_features + i * growth_rate,\n",
        "          growth_rate=growth_rate,\n",
        "          bn_size=bn_size,\n",
        "          drop_rate=drop_rate,\n",
        "          memory_efficient=memory_efficient\n",
        "      )\n",
        "      self.add_module('denselayer%d' % (i + 1), layer)\n",
        "  \n",
        "  def forward(self, init_features):\n",
        "    # TODO Add dimension changes\n",
        "    features = [init_features]\n",
        "    for name, layer in self.named_children():\n",
        "      new_features = layer(*features)\n",
        "      features.append(new_features)\n",
        "    return torch.cat(features, 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ROnWum4rrg"
      },
      "source": [
        "class _Transition(nn.Sequential):\n",
        "  def __init__(self, num_input_features, num_output_features):\n",
        "    super(_Transition, self).__init__()\n",
        "    self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "    self.add_module('relu', nn.ReLU(inplace=True))\n",
        "    self.add_module('conv', nn.Conv2d(in_channels=num_input_features,\n",
        "                                      out_channels=num_output_features,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1,\n",
        "                                      bias=False))\n",
        "    self.add_module('pool', nn.AvgPool2d(kernel_size=2,\n",
        "                                         stride=2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5XFEnm6LPq"
      },
      "source": [
        "class DenseNet121(nn.Module):\n",
        "  def __init__(self, \n",
        "               growth_rate=32, \n",
        "               block_config=(6, 12, 24, 16),\n",
        "               num_init_featuremaps=64,\n",
        "               bn_size=4,\n",
        "               drop_rate=0,\n",
        "               num_classes=1000,\n",
        "               memory_efficient=False,\n",
        "               grayscale=False,\n",
        "               ):\n",
        "    super(DenseNet121, self).__init__()\n",
        "\n",
        "    # First Convolution\n",
        "    if grayscale:\n",
        "      in_channels = 1\n",
        "    else:\n",
        "      in_channels = 3\n",
        "    \n",
        "    self.features = nn.Sequential(OrderedDict([\n",
        "                                               ('conv0', nn.Conv2d(in_channels=in_channels,\n",
        "                                                                  out_channels=num_init_featuremaps,\n",
        "                                                                  kernel_size=7,\n",
        "                                                                  stride=2,\n",
        "                                                                  padding=3,\n",
        "                                                                  bias=False)),\n",
        "                                               ('norm0', nn.BatchNorm2d(num_features=num_init_featuremaps)),\n",
        "                                               ('relu0', nn.ReLU(inplace=True)),\n",
        "                                               ('pool0', nn.MaxPool2d(kernel_size=3,\n",
        "                                                                      stride=2,\n",
        "                                                                      padding=1)),\n",
        "    ]))\n",
        "\n",
        "    # Each DenseBlock\n",
        "    num_features = num_init_featuremaps\n",
        "    \n",
        "    # num_layers = 0 (6 _DenseBlock)\n",
        "    # num_layers = 1 (12 _DenseBlock)\n",
        "    # num_layers = 2 (24 _DenseBlock)\n",
        "    # num_layers = 3 (16, _DenseBlock)\n",
        "    for i, num_layers in enumerate(block_config):\n",
        "      block = _DenseBlock(\n",
        "          num_layers=num_layers,\n",
        "          num_input_features=num_features,\n",
        "          bn_size=bn_size,\n",
        "          growth_rate=growth_rate,\n",
        "          drop_rate=drop_rate,\n",
        "          memory_efficient=memory_efficient\n",
        "      )\n",
        "      self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "      num_features = num_features + num_layers * growth_rate\n",
        "      if i != len(block_config) - 1:\n",
        "        trans = _Transition(num_input_features=num_features,\n",
        "                            num_output_features=num_features // 2)\n",
        "        self.features.add_module('transition%d' % (i + 1), trans)\n",
        "        num_features = num_features // 2\n",
        "\n",
        "    # Final BatchNorm\n",
        "    self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "    # Linear Layer\n",
        "    self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    # Official init from torch repo\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    features = self.features(x)\n",
        "    out = F.relu(features, inplace=True)\n",
        "    out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "    out = torch.flatten(out, 1)\n",
        "    logits = self.classifier(out)\n",
        "    probas = F.softmax(logits, dim=1)\n",
        "    return logits, probas"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ORA1796Jgo"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = DenseNet121(num_classes=NUM_CLASSES, grayscale=grayscale)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPM9niZf6Hmn"
      },
      "source": [
        "# Training\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "  correct_pred, num_examples = 0., 0\n",
        "  model.eval()\n",
        "  for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    logits, probas = model(features)\n",
        "    _, predicted_labels = torch.max(probas, 1)\n",
        "    num_examples += targets.size(0)\n",
        "    assert predicted_labels.size() == targets.size()\n",
        "\n",
        "    correct_pred += (predicted_labels==targets).sum()\n",
        "  return correct_pred / num_examples * 100"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7NH4mTC6C8j",
        "outputId": "5e190b2f-6b62-4457-e0da-0482d69d8557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  \n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # FORWARD & BACKPROP\n",
        "    logits, probas = model(features)\n",
        "    cost = F.cross_entropy(logits, targets)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    cost.backward()\n",
        "\n",
        "    # UPDATE MODEL PARAMETERS\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 150 == 0:\n",
        "      print(\"Batch: %04d/%04d || Epoch: %04d/%04d || Cost: %.3f\" % (batch_idx, len(train_loader), epoch+1, NUM_EPOCHS, cost.item()))\n",
        "    \n",
        "  model.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_acc = compute_accuracy(model, train_loader, device)\n",
        "    valid_acc = compute_accuracy(model, valid_loader, device)\n",
        "\n",
        "    print(\"Train Accuracy: %.2f\" % (train_acc))\n",
        "    print(\"Valid Accuracy: %.2f\" % (valid_acc))\n",
        "  elapsed_time = (time.time() - start_time) / 60\n",
        "  print(\"Epoch Elapsed Time: \", elapsed_time, \" mins\")\n",
        "elapsed_time = (time.time() - start_time) / 60\n",
        "print(\"Total Training Time: \", elapsed_time, \" mins\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0375 || Epoch: 0001/0020 || Cost: 2.386\n",
            "Batch: 0150/0375 || Epoch: 0001/0020 || Cost: 1.471\n",
            "Batch: 0300/0375 || Epoch: 0001/0020 || Cost: 1.316\n",
            "Train Accuracy: 49.79\n",
            "Valid Accuracy: 49.45\n",
            "Epoch Elapsed Time:  0.944011918703715  mins\n",
            "Batch: 0000/0375 || Epoch: 0002/0020 || Cost: 1.045\n",
            "Batch: 0150/0375 || Epoch: 0002/0020 || Cost: 1.016\n",
            "Batch: 0300/0375 || Epoch: 0002/0020 || Cost: 0.720\n",
            "Train Accuracy: 62.37\n",
            "Valid Accuracy: 58.95\n",
            "Epoch Elapsed Time:  1.890422777334849  mins\n",
            "Batch: 0000/0375 || Epoch: 0003/0020 || Cost: 0.809\n",
            "Batch: 0150/0375 || Epoch: 0003/0020 || Cost: 0.685\n",
            "Batch: 0300/0375 || Epoch: 0003/0020 || Cost: 0.971\n",
            "Train Accuracy: 70.76\n",
            "Valid Accuracy: 66.35\n",
            "Epoch Elapsed Time:  2.8344410101572675  mins\n",
            "Batch: 0000/0375 || Epoch: 0004/0020 || Cost: 0.619\n",
            "Batch: 0150/0375 || Epoch: 0004/0020 || Cost: 0.778\n",
            "Batch: 0300/0375 || Epoch: 0004/0020 || Cost: 0.461\n",
            "Train Accuracy: 73.00\n",
            "Valid Accuracy: 67.55\n",
            "Epoch Elapsed Time:  3.772270361582438  mins\n",
            "Batch: 0000/0375 || Epoch: 0005/0020 || Cost: 0.526\n",
            "Batch: 0150/0375 || Epoch: 0005/0020 || Cost: 0.562\n",
            "Batch: 0300/0375 || Epoch: 0005/0020 || Cost: 0.648\n",
            "Train Accuracy: 80.47\n",
            "Valid Accuracy: 72.65\n",
            "Epoch Elapsed Time:  4.711497910817465  mins\n",
            "Batch: 0000/0375 || Epoch: 0006/0020 || Cost: 0.707\n",
            "Batch: 0150/0375 || Epoch: 0006/0020 || Cost: 0.343\n",
            "Batch: 0300/0375 || Epoch: 0006/0020 || Cost: 0.538\n",
            "Train Accuracy: 82.82\n",
            "Valid Accuracy: 72.55\n",
            "Epoch Elapsed Time:  5.658247315883637  mins\n",
            "Batch: 0000/0375 || Epoch: 0007/0020 || Cost: 0.347\n",
            "Batch: 0150/0375 || Epoch: 0007/0020 || Cost: 0.400\n",
            "Batch: 0300/0375 || Epoch: 0007/0020 || Cost: 0.497\n",
            "Train Accuracy: 82.95\n",
            "Valid Accuracy: 72.40\n",
            "Epoch Elapsed Time:  6.600369715690613  mins\n",
            "Batch: 0000/0375 || Epoch: 0008/0020 || Cost: 0.224\n",
            "Batch: 0150/0375 || Epoch: 0008/0020 || Cost: 0.384\n",
            "Batch: 0300/0375 || Epoch: 0008/0020 || Cost: 0.351\n",
            "Train Accuracy: 90.61\n",
            "Valid Accuracy: 75.80\n",
            "Epoch Elapsed Time:  7.54758669535319  mins\n",
            "Batch: 0000/0375 || Epoch: 0009/0020 || Cost: 0.222\n",
            "Batch: 0150/0375 || Epoch: 0009/0020 || Cost: 0.238\n",
            "Batch: 0300/0375 || Epoch: 0009/0020 || Cost: 0.340\n",
            "Train Accuracy: 92.46\n",
            "Valid Accuracy: 76.20\n",
            "Epoch Elapsed Time:  8.499366434415181  mins\n",
            "Batch: 0000/0375 || Epoch: 0010/0020 || Cost: 0.172\n",
            "Batch: 0150/0375 || Epoch: 0010/0020 || Cost: 0.191\n",
            "Batch: 0300/0375 || Epoch: 0010/0020 || Cost: 0.278\n",
            "Train Accuracy: 87.27\n",
            "Valid Accuracy: 72.45\n",
            "Epoch Elapsed Time:  9.444818580150605  mins\n",
            "Batch: 0000/0375 || Epoch: 0011/0020 || Cost: 0.249\n",
            "Batch: 0150/0375 || Epoch: 0011/0020 || Cost: 0.155\n",
            "Batch: 0300/0375 || Epoch: 0011/0020 || Cost: 0.187\n",
            "Train Accuracy: 91.54\n",
            "Valid Accuracy: 73.85\n",
            "Epoch Elapsed Time:  10.389055848121643  mins\n",
            "Batch: 0000/0375 || Epoch: 0012/0020 || Cost: 0.116\n",
            "Batch: 0150/0375 || Epoch: 0012/0020 || Cost: 0.101\n",
            "Batch: 0300/0375 || Epoch: 0012/0020 || Cost: 0.099\n",
            "Train Accuracy: 94.46\n",
            "Valid Accuracy: 75.25\n",
            "Epoch Elapsed Time:  11.331066997845967  mins\n",
            "Batch: 0000/0375 || Epoch: 0013/0020 || Cost: 0.046\n",
            "Batch: 0150/0375 || Epoch: 0013/0020 || Cost: 0.083\n",
            "Batch: 0300/0375 || Epoch: 0013/0020 || Cost: 0.205\n",
            "Train Accuracy: 95.09\n",
            "Valid Accuracy: 76.45\n",
            "Epoch Elapsed Time:  12.2885688940684  mins\n",
            "Batch: 0000/0375 || Epoch: 0014/0020 || Cost: 0.178\n",
            "Batch: 0150/0375 || Epoch: 0014/0020 || Cost: 0.056\n",
            "Batch: 0300/0375 || Epoch: 0014/0020 || Cost: 0.122\n",
            "Train Accuracy: 93.54\n",
            "Valid Accuracy: 75.80\n",
            "Epoch Elapsed Time:  13.254561527570088  mins\n",
            "Batch: 0000/0375 || Epoch: 0015/0020 || Cost: 0.063\n",
            "Batch: 0150/0375 || Epoch: 0015/0020 || Cost: 0.102\n",
            "Batch: 0300/0375 || Epoch: 0015/0020 || Cost: 0.257\n",
            "Train Accuracy: 94.44\n",
            "Valid Accuracy: 76.30\n",
            "Epoch Elapsed Time:  14.212009473641713  mins\n",
            "Batch: 0000/0375 || Epoch: 0016/0020 || Cost: 0.073\n",
            "Batch: 0150/0375 || Epoch: 0016/0020 || Cost: 0.106\n",
            "Batch: 0300/0375 || Epoch: 0016/0020 || Cost: 0.143\n",
            "Train Accuracy: 95.34\n",
            "Valid Accuracy: 76.70\n",
            "Epoch Elapsed Time:  15.174247014522553  mins\n",
            "Batch: 0000/0375 || Epoch: 0017/0020 || Cost: 0.019\n",
            "Batch: 0150/0375 || Epoch: 0017/0020 || Cost: 0.073\n",
            "Batch: 0300/0375 || Epoch: 0017/0020 || Cost: 0.070\n",
            "Train Accuracy: 94.61\n",
            "Valid Accuracy: 75.55\n",
            "Epoch Elapsed Time:  16.132233969370525  mins\n",
            "Batch: 0000/0375 || Epoch: 0018/0020 || Cost: 0.070\n",
            "Batch: 0150/0375 || Epoch: 0018/0020 || Cost: 0.076\n",
            "Batch: 0300/0375 || Epoch: 0018/0020 || Cost: 0.129\n",
            "Train Accuracy: 96.67\n",
            "Valid Accuracy: 77.15\n",
            "Epoch Elapsed Time:  17.088719860712686  mins\n",
            "Batch: 0000/0375 || Epoch: 0019/0020 || Cost: 0.042\n",
            "Batch: 0150/0375 || Epoch: 0019/0020 || Cost: 0.064\n",
            "Batch: 0300/0375 || Epoch: 0019/0020 || Cost: 0.046\n",
            "Train Accuracy: 95.64\n",
            "Valid Accuracy: 76.50\n",
            "Epoch Elapsed Time:  18.042224554220834  mins\n",
            "Batch: 0000/0375 || Epoch: 0020/0020 || Cost: 0.049\n",
            "Batch: 0150/0375 || Epoch: 0020/0020 || Cost: 0.067\n",
            "Batch: 0300/0375 || Epoch: 0020/0020 || Cost: 0.138\n",
            "Train Accuracy: 95.71\n",
            "Valid Accuracy: 75.65\n",
            "Epoch Elapsed Time:  18.998316943645477  mins\n",
            "Total Training Time:  18.99832513729731  mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8spxxFjysa_F",
        "outputId": "1f1a9e42-f315-41e6-ed40-8563675c799b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Testing Set\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False):\n",
        "  test_acc = compute_accuracy(model, test_loader, device)\n",
        "  print(\"Test Accuracy: %.2f\" % (test_acc.item()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 74.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbNAVVl0V_6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}