{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#############\n# Settings\n#############\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.001\nbatch_size = 128\nnum_epochs = 10\n\n# Architecture\nnum_features = 32*32\nnum_classes = 10\n\n# Other\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngrayscale = False","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################\n# CIFAR 10\n################\n\ntrain_dataset = datasets.CIFAR10(root = \"data\",\n                                 train = True,\n                                 transform = transforms.ToTensor(), # scales it to 0-1 range\n                                 download = True\n                                )\n\ntest_dataset = datasets.CIFAR10(root = \"data\",\n                                train = False,\n                                transform = transforms.ToTensor()\n                               )\n\ntrain_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, num_workers = 8, shuffle = True)\n\ntest_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, num_workers = 8, shuffle = False)\n\n# Checking the dataset\nfor images, labels in train_loader:\n    print(\"Image Dimensions: \", images.shape)\n    print(\"Label Dimensions: \", labels.shape)\n    break","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eaa78cbdc394802b508457e94cf1578"}},"metadata":{}},{"output_type":"stream","text":"Extracting data/cifar-10-python.tar.gz to data\nImage Dimensions:  torch.Size([128, 3, 32, 32])\nLabel Dimensions:  torch.Size([128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############\n# Model\n###############\n\nclass LeNet(nn.Module):\n    def __init__(self, num_classes, grayscale = False):\n        super(LeNet, self).__init__()\n        \n        self.grayscale = grayscale\n        self.num_classes = num_classes\n        \n        if self.grayscale:\n            in_channels = 1\n        else:\n            in_channels = 3\n        \n        self.features = nn.Sequential(\n                nn.Conv2d(in_channels = in_channels, out_channels = 6*in_channels, kernel_size = 5),\n                nn.Tanh(),\n                nn.MaxPool2d(kernel_size = 2),\n                nn.Conv2d(in_channels = 6*in_channels, out_channels = 16*in_channels, kernel_size = 5),\n                nn.Tanh(),\n                nn.MaxPool2d(kernel_size = 2)\n        )\n        \n        self.classifier = nn.Sequential(\n                nn.Linear(16*5*5*in_channels, 120*in_channels),\n                nn.Tanh(),\n                nn.Linear(120*in_channels, 84*in_channels),\n                nn.Tanh(),\n                nn.Linear(84*in_channels, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        logits = self.classifier(x)\n        probas = F.softmax(logits, dim = 1)\n        return logits, probas","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.manual_seed(random_seed)\n\nmodel = LeNet(num_classes, grayscale)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_accuracy(model, data_loader, device):\n    correct_pred, num_examples = 0, 0\n    for i, (features, targets) in enumerate(data_loader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n        \n        logits, probas = model(features)\n        \n        _, predicted_labels = torch.max(probas, 1)\n        num_examples += targets.size(0)\n        correct_pred += (predicted_labels == targets).sum()\n    \n    return correct_pred.float() / num_examples * 100\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    \n    model.train()\n    \n    for i, (features, targets) in enumerate(train_loader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n        \n        # Forward and Backprop\n        logits, probas = model(features)\n        cost = F.cross_entropy(logits, targets)\n        optimizer.zero_grad()\n        \n        cost.backward()\n        \n        # Update model parameters\n        optimizer.step()\n        \n        # LOGGING\n        if not i % 50:\n            print(\"Epoch: %03d/%03d || Batch: %03d/%03d || Cost: %.4f\" % (epoch+1, num_epochs, i, len(train_loader), cost))\n        \n    model.eval()\n    with torch.set_grad_enabled(False):\n        print(\"Epoch: %03d/%03d || Train: %.3f%%\" % (epoch+1, num_epochs, compute_accuracy(model, train_loader, device)))\n    \n    print(\"Time Elapsed: %.2f min\" % ((time.time()-start_time)/60))\n\nprint(\"Total Training Time: %.2f min\" % ((time.time()-start_time)/60))","execution_count":6,"outputs":[{"output_type":"stream","text":"Epoch: 001/010 || Batch: 000/391 || Cost: 2.3068\nEpoch: 001/010 || Batch: 050/391 || Cost: 1.8193\nEpoch: 001/010 || Batch: 100/391 || Cost: 1.6463\nEpoch: 001/010 || Batch: 150/391 || Cost: 1.5756\nEpoch: 001/010 || Batch: 200/391 || Cost: 1.4029\nEpoch: 001/010 || Batch: 250/391 || Cost: 1.3118\nEpoch: 001/010 || Batch: 300/391 || Cost: 1.3301\nEpoch: 001/010 || Batch: 350/391 || Cost: 1.2877\nEpoch: 001/010 || Train: 54.556%\nTime Elapsed: 0.23 min\nEpoch: 002/010 || Batch: 000/391 || Cost: 1.4057\nEpoch: 002/010 || Batch: 050/391 || Cost: 1.2842\nEpoch: 002/010 || Batch: 100/391 || Cost: 1.2604\nEpoch: 002/010 || Batch: 150/391 || Cost: 1.3171\nEpoch: 002/010 || Batch: 200/391 || Cost: 1.2911\nEpoch: 002/010 || Batch: 250/391 || Cost: 1.3559\nEpoch: 002/010 || Batch: 300/391 || Cost: 1.2006\nEpoch: 002/010 || Batch: 350/391 || Cost: 1.1475\nEpoch: 002/010 || Train: 60.400%\nTime Elapsed: 0.45 min\nEpoch: 003/010 || Batch: 000/391 || Cost: 1.1226\nEpoch: 003/010 || Batch: 050/391 || Cost: 1.1907\nEpoch: 003/010 || Batch: 100/391 || Cost: 1.3042\nEpoch: 003/010 || Batch: 150/391 || Cost: 1.0265\nEpoch: 003/010 || Batch: 200/391 || Cost: 1.0250\nEpoch: 003/010 || Batch: 250/391 || Cost: 0.8002\nEpoch: 003/010 || Batch: 300/391 || Cost: 1.0646\nEpoch: 003/010 || Batch: 350/391 || Cost: 1.1073\nEpoch: 003/010 || Train: 64.508%\nTime Elapsed: 0.68 min\nEpoch: 004/010 || Batch: 000/391 || Cost: 1.1400\nEpoch: 004/010 || Batch: 050/391 || Cost: 1.0820\nEpoch: 004/010 || Batch: 100/391 || Cost: 1.0441\nEpoch: 004/010 || Batch: 150/391 || Cost: 0.9813\nEpoch: 004/010 || Batch: 200/391 || Cost: 0.8517\nEpoch: 004/010 || Batch: 250/391 || Cost: 1.1391\nEpoch: 004/010 || Batch: 300/391 || Cost: 0.8319\nEpoch: 004/010 || Batch: 350/391 || Cost: 1.1703\nEpoch: 004/010 || Train: 66.998%\nTime Elapsed: 0.91 min\nEpoch: 005/010 || Batch: 000/391 || Cost: 1.1105\nEpoch: 005/010 || Batch: 050/391 || Cost: 0.8523\nEpoch: 005/010 || Batch: 100/391 || Cost: 0.8925\nEpoch: 005/010 || Batch: 150/391 || Cost: 0.9221\nEpoch: 005/010 || Batch: 200/391 || Cost: 0.9364\nEpoch: 005/010 || Batch: 250/391 || Cost: 1.0560\nEpoch: 005/010 || Batch: 300/391 || Cost: 0.9475\nEpoch: 005/010 || Batch: 350/391 || Cost: 0.9106\nEpoch: 005/010 || Train: 71.436%\nTime Elapsed: 1.13 min\nEpoch: 006/010 || Batch: 000/391 || Cost: 0.7030\nEpoch: 006/010 || Batch: 050/391 || Cost: 0.6964\nEpoch: 006/010 || Batch: 100/391 || Cost: 0.6762\nEpoch: 006/010 || Batch: 150/391 || Cost: 0.8225\nEpoch: 006/010 || Batch: 200/391 || Cost: 0.7917\nEpoch: 006/010 || Batch: 250/391 || Cost: 0.9656\nEpoch: 006/010 || Batch: 300/391 || Cost: 0.7744\nEpoch: 006/010 || Batch: 350/391 || Cost: 0.9550\nEpoch: 006/010 || Train: 73.936%\nTime Elapsed: 1.35 min\nEpoch: 007/010 || Batch: 000/391 || Cost: 0.8845\nEpoch: 007/010 || Batch: 050/391 || Cost: 0.8262\nEpoch: 007/010 || Batch: 100/391 || Cost: 0.6914\nEpoch: 007/010 || Batch: 150/391 || Cost: 0.6263\nEpoch: 007/010 || Batch: 200/391 || Cost: 0.6429\nEpoch: 007/010 || Batch: 250/391 || Cost: 0.6364\nEpoch: 007/010 || Batch: 300/391 || Cost: 0.7388\nEpoch: 007/010 || Batch: 350/391 || Cost: 0.7845\nEpoch: 007/010 || Train: 77.926%\nTime Elapsed: 1.57 min\nEpoch: 008/010 || Batch: 000/391 || Cost: 0.5141\nEpoch: 008/010 || Batch: 050/391 || Cost: 0.5971\nEpoch: 008/010 || Batch: 100/391 || Cost: 0.6044\nEpoch: 008/010 || Batch: 150/391 || Cost: 0.6753\nEpoch: 008/010 || Batch: 200/391 || Cost: 0.5792\nEpoch: 008/010 || Batch: 250/391 || Cost: 0.6234\nEpoch: 008/010 || Batch: 300/391 || Cost: 0.5937\nEpoch: 008/010 || Batch: 350/391 || Cost: 0.8088\nEpoch: 008/010 || Train: 82.222%\nTime Elapsed: 1.81 min\nEpoch: 009/010 || Batch: 000/391 || Cost: 0.4917\nEpoch: 009/010 || Batch: 050/391 || Cost: 0.4628\nEpoch: 009/010 || Batch: 100/391 || Cost: 0.6500\nEpoch: 009/010 || Batch: 150/391 || Cost: 0.5149\nEpoch: 009/010 || Batch: 200/391 || Cost: 0.6313\nEpoch: 009/010 || Batch: 250/391 || Cost: 0.7330\nEpoch: 009/010 || Batch: 300/391 || Cost: 0.5880\nEpoch: 009/010 || Batch: 350/391 || Cost: 0.6244\nEpoch: 009/010 || Train: 85.024%\nTime Elapsed: 2.03 min\nEpoch: 010/010 || Batch: 000/391 || Cost: 0.4817\nEpoch: 010/010 || Batch: 050/391 || Cost: 0.3867\nEpoch: 010/010 || Batch: 100/391 || Cost: 0.4881\nEpoch: 010/010 || Batch: 150/391 || Cost: 0.4456\nEpoch: 010/010 || Batch: 200/391 || Cost: 0.5672\nEpoch: 010/010 || Batch: 250/391 || Cost: 0.4253\nEpoch: 010/010 || Batch: 300/391 || Cost: 0.5051\nEpoch: 010/010 || Batch: 350/391 || Cost: 0.5122\nEpoch: 010/010 || Train: 88.598%\nTime Elapsed: 2.25 min\nTotal Training Time: 2.25 min\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print(\"Test Accuracy: %.2f%%\" % (compute_accuracy(model, test_loader, device)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}