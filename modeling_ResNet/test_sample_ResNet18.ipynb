{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_ResNet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrcf2tcSwZdY8lWm3m9dyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_ResNet/test_sample_ResNet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuzUR0tO_mRh",
        "outputId": "ca7f9277-f5d1-4a5d-845c-c180b2af519c"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch-Architectures'...\n",
            "remote: Enumerating objects: 221, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1159 (delta 105), reused 152 (delta 57), pack-reused 938\u001b[K\n",
            "Receiving objects: 100% (1159/1159), 8.50 MiB | 25.16 MiB/s, done.\n",
            "Resolving deltas: 100% (676/676), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdQKFvkIAtCF",
        "outputId": "d32250d1-d5ec-4ac6-8a74-57fef43aed0b"
      },
      "source": [
        "%cd PyTorch-Architectures/modeling_ResNet/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch-Architectures/modeling_ResNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M42wlySA2jo"
      },
      "source": [
        "# TODO --> paper needs to be read carefully\r\n",
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchvision import transforms, datasets\r\n",
        "\r\n",
        "from model_ResNet18 import ResNet, BasicBlock\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMgReTFGA5qs"
      },
      "source": [
        "# Hyper parameters\r\n",
        "RANDOM_SEED = 1\r\n",
        "LEARNING_RATE = 0.001 \r\n",
        "BATCH_SIZE = 128\r\n",
        "NUM_EPOCHS = 10\r\n",
        "\r\n",
        "# Architecture\r\n",
        "NUM_FEATURES = 28 * 28\r\n",
        "NUM_CLASSES = 10\r\n",
        "\r\n",
        "# Other\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "grayscale = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd_TbbB3tRMm"
      },
      "source": [
        "##################\r\n",
        "# MNIST Dataset\r\n",
        "##################\r\n",
        "\r\n",
        "train_dataset = datasets.MNIST(\r\n",
        "    root='data',\r\n",
        "    train=True,\r\n",
        "    transform=transforms.ToTensor(),\r\n",
        "    download=True,\r\n",
        ")\r\n",
        "test_dataset = datasets.MNIST(\r\n",
        "    root='data',\r\n",
        "    train=False,\r\n",
        "    transform=transforms.ToTensor(),\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = DataLoader(\r\n",
        "    dataset=train_dataset,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    shuffle=True,\r\n",
        "    )\r\n",
        "test_loader = DataLoader(\r\n",
        "    dataset=test_dataset,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    shuffle=False,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSO7vxiAuCBP",
        "outputId": "98a95c03-496c-4e49-f94f-b6b01c540d88"
      },
      "source": [
        "# Checking the dataset\r\n",
        "for images, labels in train_loader:\r\n",
        "  print('Image Batch Dimensions: ', images.shape)\r\n",
        "  print('Label Batch Dimensions: ', labels.shape)\r\n",
        "  break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Batch Dimensions:  torch.Size([128, 1, 28, 28])\n",
            "Label Batch Dimensions:  torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTFwqL72odE3"
      },
      "source": [
        "def resnet18(num_classes):\r\n",
        "  model = ResNet(block=BasicBlock,\r\n",
        "                 layers=[2, 2, 2, 2],\r\n",
        "                 num_classes=NUM_CLASSES,\r\n",
        "                 grayscale=grayscale)\r\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHlWu5ffpC80",
        "outputId": "6406cca4-6ad6-45df-f5b3-296b6bb4e9ea"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\r\n",
        "\r\n",
        "model = resnet18(NUM_CLASSES)\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(\"Total Trainable Parameters: \", params)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Trainable Parameters:  11175370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3qT56ydpeCw",
        "outputId": "f048b332-a0e4-46ff-f1bb-7a9eb47ec5a9"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\r\n",
        "  correct_pred, num_examples = 0, 0\r\n",
        "  for features, targets in data_loader:\r\n",
        "    features = features.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "    logits, probas = model(features)\r\n",
        "    _, predicted_labels = torch.max(probas, 1)\r\n",
        "    correct_pred += (predicted_labels == targets).sum()\r\n",
        "    num_examples += targets.size(0)\r\n",
        "  return correct_pred.float() / num_examples * 100\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "  model.train()\r\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\r\n",
        "    features = features.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "\r\n",
        "    logits, probas = model(features)\r\n",
        "    cost = F.cross_entropy(logits, targets)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    cost.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    #LOGGING\r\n",
        "    if batch_idx % 50 == 0:\r\n",
        "      print('Batch: %04d/%04d | Epoch: %04d/%04d | Cost: %.4f' % (batch_idx, len(train_loader), epoch+1, NUM_EPOCHS, cost.item()))\r\n",
        "  \r\n",
        "  model.eval()\r\n",
        "  with torch.set_grad_enabled(False):\r\n",
        "    print('Epoch: %04d/%04d | Train %.3f%%' % (epoch+1, NUM_EPOCHS, compute_accuracy(model, train_loader, device=device)))\r\n",
        "  \r\n",
        "  print(\"Epoch elapsed time %.2f min\" % ((time.time() - start_time) / 60))\r\n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0469 | Epoch: 0001/0010 | Cost: 2.6632\n",
            "Batch: 0050/0469 | Epoch: 0001/0010 | Cost: 0.1071\n",
            "Batch: 0100/0469 | Epoch: 0001/0010 | Cost: 0.1870\n",
            "Batch: 0150/0469 | Epoch: 0001/0010 | Cost: 0.0989\n",
            "Batch: 0200/0469 | Epoch: 0001/0010 | Cost: 0.1140\n",
            "Batch: 0250/0469 | Epoch: 0001/0010 | Cost: 0.0514\n",
            "Batch: 0300/0469 | Epoch: 0001/0010 | Cost: 0.0352\n",
            "Batch: 0350/0469 | Epoch: 0001/0010 | Cost: 0.0700\n",
            "Batch: 0400/0469 | Epoch: 0001/0010 | Cost: 0.0162\n",
            "Batch: 0450/0469 | Epoch: 0001/0010 | Cost: 0.0208\n",
            "Epoch: 0001/0010 | Train 98.200%\n",
            "Epoch elapsed time 0.38 min\n",
            "Batch: 0000/0469 | Epoch: 0002/0010 | Cost: 0.1594\n",
            "Batch: 0050/0469 | Epoch: 0002/0010 | Cost: 0.0758\n",
            "Batch: 0100/0469 | Epoch: 0002/0010 | Cost: 0.0262\n",
            "Batch: 0150/0469 | Epoch: 0002/0010 | Cost: 0.0183\n",
            "Batch: 0200/0469 | Epoch: 0002/0010 | Cost: 0.0109\n",
            "Batch: 0250/0469 | Epoch: 0002/0010 | Cost: 0.0750\n",
            "Batch: 0300/0469 | Epoch: 0002/0010 | Cost: 0.0159\n",
            "Batch: 0350/0469 | Epoch: 0002/0010 | Cost: 0.0323\n",
            "Batch: 0400/0469 | Epoch: 0002/0010 | Cost: 0.0086\n",
            "Batch: 0450/0469 | Epoch: 0002/0010 | Cost: 0.0430\n",
            "Epoch: 0002/0010 | Train 98.427%\n",
            "Epoch elapsed time 0.77 min\n",
            "Batch: 0000/0469 | Epoch: 0003/0010 | Cost: 0.0535\n",
            "Batch: 0050/0469 | Epoch: 0003/0010 | Cost: 0.0366\n",
            "Batch: 0100/0469 | Epoch: 0003/0010 | Cost: 0.0468\n",
            "Batch: 0150/0469 | Epoch: 0003/0010 | Cost: 0.0865\n",
            "Batch: 0200/0469 | Epoch: 0003/0010 | Cost: 0.0334\n",
            "Batch: 0250/0469 | Epoch: 0003/0010 | Cost: 0.0084\n",
            "Batch: 0300/0469 | Epoch: 0003/0010 | Cost: 0.0432\n",
            "Batch: 0350/0469 | Epoch: 0003/0010 | Cost: 0.0419\n",
            "Batch: 0400/0469 | Epoch: 0003/0010 | Cost: 0.0253\n",
            "Batch: 0450/0469 | Epoch: 0003/0010 | Cost: 0.0352\n",
            "Epoch: 0003/0010 | Train 98.875%\n",
            "Epoch elapsed time 1.17 min\n",
            "Batch: 0000/0469 | Epoch: 0004/0010 | Cost: 0.0545\n",
            "Batch: 0050/0469 | Epoch: 0004/0010 | Cost: 0.0333\n",
            "Batch: 0100/0469 | Epoch: 0004/0010 | Cost: 0.0346\n",
            "Batch: 0150/0469 | Epoch: 0004/0010 | Cost: 0.0391\n",
            "Batch: 0200/0469 | Epoch: 0004/0010 | Cost: 0.0157\n",
            "Batch: 0250/0469 | Epoch: 0004/0010 | Cost: 0.0418\n",
            "Batch: 0300/0469 | Epoch: 0004/0010 | Cost: 0.0279\n",
            "Batch: 0350/0469 | Epoch: 0004/0010 | Cost: 0.0529\n",
            "Batch: 0400/0469 | Epoch: 0004/0010 | Cost: 0.0374\n",
            "Batch: 0450/0469 | Epoch: 0004/0010 | Cost: 0.0047\n",
            "Epoch: 0004/0010 | Train 98.842%\n",
            "Epoch elapsed time 1.57 min\n",
            "Batch: 0000/0469 | Epoch: 0005/0010 | Cost: 0.0121\n",
            "Batch: 0050/0469 | Epoch: 0005/0010 | Cost: 0.0059\n",
            "Batch: 0100/0469 | Epoch: 0005/0010 | Cost: 0.0378\n",
            "Batch: 0150/0469 | Epoch: 0005/0010 | Cost: 0.0038\n",
            "Batch: 0200/0469 | Epoch: 0005/0010 | Cost: 0.0471\n",
            "Batch: 0250/0469 | Epoch: 0005/0010 | Cost: 0.1236\n",
            "Batch: 0300/0469 | Epoch: 0005/0010 | Cost: 0.0073\n",
            "Batch: 0350/0469 | Epoch: 0005/0010 | Cost: 0.0290\n",
            "Batch: 0400/0469 | Epoch: 0005/0010 | Cost: 0.0039\n",
            "Batch: 0450/0469 | Epoch: 0005/0010 | Cost: 0.0122\n",
            "Epoch: 0005/0010 | Train 98.842%\n",
            "Epoch elapsed time 1.97 min\n",
            "Batch: 0000/0469 | Epoch: 0006/0010 | Cost: 0.0073\n",
            "Batch: 0050/0469 | Epoch: 0006/0010 | Cost: 0.0215\n",
            "Batch: 0100/0469 | Epoch: 0006/0010 | Cost: 0.0167\n",
            "Batch: 0150/0469 | Epoch: 0006/0010 | Cost: 0.0475\n",
            "Batch: 0200/0469 | Epoch: 0006/0010 | Cost: 0.0422\n",
            "Batch: 0250/0469 | Epoch: 0006/0010 | Cost: 0.0142\n",
            "Batch: 0300/0469 | Epoch: 0006/0010 | Cost: 0.0035\n",
            "Batch: 0350/0469 | Epoch: 0006/0010 | Cost: 0.0069\n",
            "Batch: 0400/0469 | Epoch: 0006/0010 | Cost: 0.0160\n",
            "Batch: 0450/0469 | Epoch: 0006/0010 | Cost: 0.0080\n",
            "Epoch: 0006/0010 | Train 98.960%\n",
            "Epoch elapsed time 2.39 min\n",
            "Batch: 0000/0469 | Epoch: 0007/0010 | Cost: 0.0031\n",
            "Batch: 0050/0469 | Epoch: 0007/0010 | Cost: 0.0074\n",
            "Batch: 0100/0469 | Epoch: 0007/0010 | Cost: 0.0355\n",
            "Batch: 0150/0469 | Epoch: 0007/0010 | Cost: 0.0444\n",
            "Batch: 0200/0469 | Epoch: 0007/0010 | Cost: 0.0099\n",
            "Batch: 0250/0469 | Epoch: 0007/0010 | Cost: 0.0309\n",
            "Batch: 0300/0469 | Epoch: 0007/0010 | Cost: 0.0536\n",
            "Batch: 0350/0469 | Epoch: 0007/0010 | Cost: 0.0130\n",
            "Batch: 0400/0469 | Epoch: 0007/0010 | Cost: 0.0067\n",
            "Batch: 0450/0469 | Epoch: 0007/0010 | Cost: 0.0078\n",
            "Epoch: 0007/0010 | Train 99.387%\n",
            "Epoch elapsed time 2.80 min\n",
            "Batch: 0000/0469 | Epoch: 0008/0010 | Cost: 0.0059\n",
            "Batch: 0050/0469 | Epoch: 0008/0010 | Cost: 0.0016\n",
            "Batch: 0100/0469 | Epoch: 0008/0010 | Cost: 0.0004\n",
            "Batch: 0150/0469 | Epoch: 0008/0010 | Cost: 0.0032\n",
            "Batch: 0200/0469 | Epoch: 0008/0010 | Cost: 0.0016\n",
            "Batch: 0250/0469 | Epoch: 0008/0010 | Cost: 0.0129\n",
            "Batch: 0300/0469 | Epoch: 0008/0010 | Cost: 0.0151\n",
            "Batch: 0350/0469 | Epoch: 0008/0010 | Cost: 0.0075\n",
            "Batch: 0400/0469 | Epoch: 0008/0010 | Cost: 0.0025\n",
            "Batch: 0450/0469 | Epoch: 0008/0010 | Cost: 0.0075\n",
            "Epoch: 0008/0010 | Train 99.295%\n",
            "Epoch elapsed time 3.20 min\n",
            "Batch: 0000/0469 | Epoch: 0009/0010 | Cost: 0.0117\n",
            "Batch: 0050/0469 | Epoch: 0009/0010 | Cost: 0.0177\n",
            "Batch: 0100/0469 | Epoch: 0009/0010 | Cost: 0.0034\n",
            "Batch: 0150/0469 | Epoch: 0009/0010 | Cost: 0.0053\n",
            "Batch: 0200/0469 | Epoch: 0009/0010 | Cost: 0.0220\n",
            "Batch: 0250/0469 | Epoch: 0009/0010 | Cost: 0.0319\n",
            "Batch: 0300/0469 | Epoch: 0009/0010 | Cost: 0.0592\n",
            "Batch: 0350/0469 | Epoch: 0009/0010 | Cost: 0.0077\n",
            "Batch: 0400/0469 | Epoch: 0009/0010 | Cost: 0.0333\n",
            "Batch: 0450/0469 | Epoch: 0009/0010 | Cost: 0.0241\n",
            "Epoch: 0009/0010 | Train 99.672%\n",
            "Epoch elapsed time 3.61 min\n",
            "Batch: 0000/0469 | Epoch: 0010/0010 | Cost: 0.0261\n",
            "Batch: 0050/0469 | Epoch: 0010/0010 | Cost: 0.0027\n",
            "Batch: 0100/0469 | Epoch: 0010/0010 | Cost: 0.0025\n",
            "Batch: 0150/0469 | Epoch: 0010/0010 | Cost: 0.0095\n",
            "Batch: 0200/0469 | Epoch: 0010/0010 | Cost: 0.0098\n",
            "Batch: 0250/0469 | Epoch: 0010/0010 | Cost: 0.0213\n",
            "Batch: 0300/0469 | Epoch: 0010/0010 | Cost: 0.0004\n",
            "Batch: 0350/0469 | Epoch: 0010/0010 | Cost: 0.0444\n",
            "Batch: 0400/0469 | Epoch: 0010/0010 | Cost: 0.0009\n",
            "Batch: 0450/0469 | Epoch: 0010/0010 | Cost: 0.0091\n",
            "Epoch: 0010/0010 | Train 99.533%\n",
            "Epoch elapsed time 4.02 min\n",
            "Total Training Time: 4.02 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05JxibLBydtL",
        "outputId": "5359018a-beb2-4fe1-ae99-b7e506b3b9c5"
      },
      "source": [
        "with torch.set_grad_enabled(False):\r\n",
        "  print('Test Accuracy: %.4f%%' % (compute_accuracy(model, test_loader, device)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 99.0500%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvO6FEwtz5BI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}