{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKgkKwmiCFxPK5nY0IOYB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_Seq2Seq/test_sample_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XypkT2EmpMi",
        "outputId": "760e458c-416c-483d-9626-60ac3f7d8ea5"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 26 12:33:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f79ZeRSEjDK7"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/modeling_Seq2Seq/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1LL3xX7_jxC"
      },
      "source": [
        "! pip install datasets\n",
        "! pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWYjGbAQDTyc"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from model import Seq2Seq, Encoder, Decoder"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HDBiboTAezL"
      },
      "source": [
        "dataset = load_dataset('mt_eng_vietnamese', 'iwslt2015-en-vi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOwbnUkXArPp",
        "outputId": "0455d7a4-f5e3-4853-d8b1-2e9cba1b3dce"
      },
      "source": [
        "# Sample from the dataset\n",
        "dataset['train']['translation'][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Rachel Pike : The science behind a climate headline',\n",
              " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ST17Cw7BDHH"
      },
      "source": [
        "# Paper proposes that reversing source sentence helps in\n",
        "# LSTM long-range dependencies\n",
        "\n",
        "train_sentences = dataset['train']\n",
        "en_sentences = []\n",
        "vi_sentences = []\n",
        "for value in train_sentences:\n",
        "  en_sentence = value['translation']['en']\n",
        "  en_sentence = ' '.join(reversed(en_sentence.split()))\n",
        "  en_sentences.append(en_sentence)\n",
        "  vi_sentences.append(value['translation']['vi'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vczI_eamDIXu"
      },
      "source": [
        "tokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer_vi = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"])\n",
        "trainer_vi = BpeTrainer(special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff0KcPeoEFpa"
      },
      "source": [
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "tokenizer_vi.pre_tokenizer = Whitespace()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMpDSfiRERUm"
      },
      "source": [
        "tokenizer_en.train_from_iterator(en_sentences, trainer_en)\n",
        "tokenizer_vi.train_from_iterator(vi_sentences, trainer_vi)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DyIax-jEbkT"
      },
      "source": [
        "tokenizer_en.post_processor = TemplateProcessing(\n",
        "    single = \"[SOS] $A [EOS]\",\n",
        "    special_tokens = [\n",
        "                      (\"[SOS]\", tokenizer_en.token_to_id(\"[SOS]\")),\n",
        "                      (\"[EOS]\", tokenizer_en.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer_vi.post_processor = TemplateProcessing(\n",
        "    single = \"[SOS] $A [EOS]\",\n",
        "    special_tokens = [\n",
        "                      (\"[SOS]\", tokenizer_vi.token_to_id(\"[SOS]\")),\n",
        "                      (\"[EOS]\", tokenizer_vi.token_to_id(\"[EOS]\"))\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Paper uses the following format:\n",
        "# [SOS] en_sentence [EOS] --> [SOS] vi_sentence [EOS]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xW6QEPswCU0"
      },
      "source": [
        "# Cell for hyperparameters\n",
        "MAX_LENGTH = 16\n",
        "INPUT_DIM_ENCODER = tokenizer_en.get_vocab_size()\n",
        "INPUT_DIM_DECODER = tokenizer_vi.get_vocab_size()\n",
        "EMB_DIM_ENCODER = 256\n",
        "EMB_DIM_DECODER = 256\n",
        "HIDDEN_DIM_ENCODER = 512\n",
        "HIDDEN_DIM_DECODER = 512\n",
        "DROPOUT_ENCODER = 0.5\n",
        "DROPOUT_DECODER = 0.5\n",
        "NUM_LAYERS = 4\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 3e-5\n",
        "EPOCHS = 3\n",
        "CLIP = 1 # clip the gradients to max_norm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsgtM1O9vGn9"
      },
      "source": [
        "tokenizer_en.enable_padding(pad_id=2, pad_token=\"[PAD]\", len=MAX_LENGTH)\n",
        "tokenizer_vi.enable_padding(pad_id=2, pad_token=\"[PAD]\", len=MAX_LENGTH)\n",
        "tokenizer_en.enable_truncation(max_length=MAX_LENGTH)\n",
        "tokenizer_vi.enable_truncation(max_length=MAX_LENGTH)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhbj0naDHQCJ",
        "outputId": "610ce11e-f844-41d5-8516-ef77181d93db"
      },
      "source": [
        "# Sample tokenization of a batch of english sentences --> pad uses max_length in batch\n",
        "sample_sentence = \"This is amazing and great!\"\n",
        "print('Sentences --> ', sample_sentence)\n",
        "output = tokenizer_en.encode(sample_sentence)\n",
        "output.tokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences -->  This is amazing and great!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[SOS]', 'This', 'is', 'amazing', 'and', 'great', '!', '[EOS]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLZQE827JMHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403ba850-dc60-40c4-fb06-573668bfbda2"
      },
      "source": [
        "# Sample tokenization of a vietnamese sentence\n",
        "print('Sentence --> ', vi_sentences[0])\n",
        "output = tokenizer_vi.encode(vi_sentences[0])\n",
        "output.tokens"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence -->  Khoa học đằng sau một tiêu đề về khí hậu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[SOS]',\n",
              " 'Khoa',\n",
              " 'học',\n",
              " 'đằng',\n",
              " 'sau',\n",
              " 'một',\n",
              " 'tiêu',\n",
              " 'đề',\n",
              " 'về',\n",
              " 'khí',\n",
              " 'hậu',\n",
              " '[EOS]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSY3Hn4YJNGM"
      },
      "source": [
        "def collate_fn_en(batch):\n",
        "  sentences = []\n",
        "  for sent in batch:\n",
        "    sentences.append(sent[0])\n",
        "  outputs = tokenizer_en.encode_batch(sentences)\n",
        "  input_ids = []\n",
        "  for i in range(len(outputs)):\n",
        "    input_ids.append(outputs[i].ids)\n",
        "  return torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn_vi(batch):\n",
        "  sentences = []\n",
        "  for sent in batch:\n",
        "    sentences.append(sent[0])\n",
        "  outputs = tokenizer_vi.encode_batch(sentences)\n",
        "  input_ids = []\n",
        "  for i in range(len(outputs)):\n",
        "    input_ids.append(outputs[i].ids)\n",
        "  return torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, tokenizer, sentences, max_input_length=16):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.sentences = sentences\n",
        "    self.max_input_length = max_input_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    sents = [self.sentences[idx]]\n",
        "    sents_list = []\n",
        "    for sent in sents:\n",
        "      sents_list.append(sent)\n",
        "    return sents_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8vix3-toUs",
        "outputId": "0c055108-e2c2-4e89-dc42-77e08b50096b"
      },
      "source": [
        "# Sanity check DataLoader\n",
        "sample_sentences_en = en_sentences[:4]\n",
        "sample_dataset_en = CustomDataset(tokenizer_en, sample_sentences_en)\n",
        "sample_dataloader_en = DataLoader(dataset=sample_dataset_en, batch_size=2, shuffle=False, drop_last=True, collate_fn=collate_fn_en)\n",
        "\n",
        "for sample in sample_dataloader_en:\n",
        "  print(sample.shape)\n",
        "  break\n",
        "\n",
        "sample_sentences_vi = vi_sentences[:4]\n",
        "sample_dataset_vi = CustomDataset(tokenizer_vi, sample_sentences_vi)\n",
        "sample_dataloader_vi = DataLoader(dataset=sample_dataset_vi, batch_size=2, shuffle=False, drop_last=True, collate_fn=collate_fn_vi)\n",
        "\n",
        "for sample in sample_dataloader_vi:\n",
        "  print(sample.shape)\n",
        "  break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gil4zFQpJV6A"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder(input_dim=INPUT_DIM_DECODER,\n",
        "                  emb_dim=EMB_DIM_ENCODER,\n",
        "                  hidden_dim=HIDDEN_DIM_ENCODER,\n",
        "                  num_layers=NUM_LAYERS,\n",
        "                  p_drop=DROPOUT_ENCODER)\n",
        "\n",
        "decoder = Decoder(output_dim=INPUT_DIM_DECODER,\n",
        "                  emb_dim=EMB_DIM_DECODER,\n",
        "                  hidden_dim=HIDDEN_DIM_DECODER,\n",
        "                  num_layers=NUM_LAYERS,\n",
        "                  p_drop=DROPOUT_DECODER)\n",
        "\n",
        "model = Seq2Seq(encoder=encoder,\n",
        "                decoder=decoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXwxtUOqDSGy",
        "outputId": "3e5ad7d5-be2a-4b9d-a6dc-958cb0ecf452"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)\n",
        "model.to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(30000, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(30000, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=30000, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waOUl7P-3-v0",
        "outputId": "57d00534-ec16-4a75-d1cc-2f18fdcba2f0"
      },
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Trainable Parameters: ', params)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable Parameters:  46511408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ImwfodJ6szu",
        "outputId": "a2f6131a-a196-4154-c36b-ea944eb60d27"
      },
      "source": [
        "assert len(en_sentences) == len(vi_sentences)\n",
        "split = 90 * len(en_sentences) // 100\n",
        "en_sentences_train = en_sentences[:split]\n",
        "vi_sentences_train = vi_sentences[:split]\n",
        "en_sentences_valid = en_sentences[split:]\n",
        "vi_sentences_valid = vi_sentences[split:]\n",
        "\n",
        "print('Training samples: ', len(en_sentences_train))\n",
        "print('Valid samples: ', len(vi_sentences_valid))\n",
        "\n",
        "assert (len(en_sentences_train) + len(en_sentences_valid)) == \\\n",
        "        (len(vi_sentences_train) + len(vi_sentences_valid))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples:  119986\n",
            "Valid samples:  13332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r-PcGI47wKf"
      },
      "source": [
        "en_sentences_train_dataset = CustomDataset(tokenizer=tokenizer_en,\n",
        "                                          sentences=en_sentences_train)\n",
        "vi_sentences_train_dataset = CustomDataset(tokenizer=tokenizer_vi,\n",
        "                                           sentences=vi_sentences_train)\n",
        "\n",
        "en_sentences_valid_dataset = CustomDataset(tokenizer=tokenizer_en,\n",
        "                                           sentences=en_sentences_valid)\n",
        "vi_sentences_valid_dataset = CustomDataset(tokenizer=tokenizer_vi,\n",
        "                                           sentences=vi_sentences_valid)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G39RpiOiAR9k",
        "outputId": "869ca3b1-2cc0-4e01-fde3-c9fa214226f7"
      },
      "source": [
        "en_sentences_train_loader = DataLoader(dataset=en_sentences_train_dataset,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn_en,)\n",
        "vi_sentences_train_loader = DataLoader(dataset=vi_sentences_train_dataset,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn_vi)\n",
        "\n",
        "en_sentences_valid_loader = DataLoader(dataset=en_sentences_valid_dataset,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn_en)\n",
        "vi_sentences_valid_loader = DataLoader(dataset=vi_sentences_valid_dataset,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn_vi)\n",
        "\n",
        "assert len(en_sentences_train_loader) == len(vi_sentences_train_loader)\n",
        "assert len(en_sentences_valid_loader) == len(vi_sentences_valid_loader)\n",
        "\n",
        "print('Length of Train DataLoader: ', len(en_sentences_train_loader))\n",
        "print('Length of Valid DataLoader: ', len(en_sentences_valid_loader))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train DataLoader:  938\n",
            "Length of Valid DataLoader:  105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Q766uDB3Ls"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss = nn.CrossEntropyLoss(ignore_index=tokenizer_vi.token_to_id(\"[PAD]\"))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Kc4YJTCoZw",
        "outputId": "7f4ea8dc-fd5a-416d-e922-187c92cf5f73"
      },
      "source": [
        "# Sanity check forward pass\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False):\n",
        "  for en_sample, vi_sample in zip(en_sentences_train_loader, vi_sentences_train_loader):\n",
        "    en_sample = en_sample.transpose(0, 1).contiguous().to(device)\n",
        "    vi_sample = vi_sample.transpose(0, 1).contiguous().to(device)\n",
        "    output = model(src=en_sample, trg=vi_sample)\n",
        "\n",
        "    print(output[1:].view(-1, output.size(2)).shape)\n",
        "    print(vi_sample[1:].view(-1).shape)\n",
        "    assert output.dim() == 3\n",
        "    assert output.size(0) == en_sample.size(0)\n",
        "    assert output.size(1) == en_sample.size(1)\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1920, 30000])\n",
            "torch.Size([1920])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyvAcHykDNQB",
        "outputId": "3f5e044d-154b-4dc3-abab-e22e281bf648"
      },
      "source": [
        "def compute_loss(model, en_loader, vi_loader, device):\n",
        "  cost_list = []\n",
        "  with torch.set_grad_enabled(False):\n",
        "    for idx, (en_sample, vi_sample) in enumerate(zip(en_loader, vi_loader)):\n",
        "      en_sample = en_sample.transpose(0, 1).contiguous().to(device)\n",
        "      vi_sample = vi_sample.transpose(0, 1).contiguous().to(device)\n",
        "      output = model(src=en_sample, trg=vi_sample)\n",
        "      output = output[1:].view(-1, output.size(2))\n",
        "      trg = vi_sample[1:].view(-1)\n",
        "      cost = loss(output, trg)\n",
        "      cost_list.append(cost.item())\n",
        "  return torch.mean(torch.tensor(cost_list))\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  for idx, (sample_en, sample_vi) in enumerate(zip(en_sentences_train_loader, vi_sentences_train_loader)):\n",
        "    sample_en = sample_en.transpose(0, 1).contiguous().to(device)\n",
        "    sample_vi = sample_vi.transpose(0, 1).contiguous().to(device)\n",
        "\n",
        "    output = model(src=sample_en, trg=sample_vi)\n",
        "    output = output[1:].view(-1, output.size(2))\n",
        "    trg = sample_vi[1:].view(-1)\n",
        "    cost = loss(output, trg)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP)\n",
        "    optimizer.step()\n",
        "    \n",
        "    # LOGGING\n",
        "    if idx % 300 == 0:\n",
        "      print('Batch: %04d/%04d || Epoch: %04d/%04d || Loss: %.2f' % (idx, len(en_sentences_train_loader),\n",
        "                                                                    epoch+1, EPOCHS, cost.item()))\n",
        "      \n",
        "  model.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_loss = compute_loss(model, en_sentences_train_loader, \n",
        "                                    vi_sentences_train_loader, device)\n",
        "    valid_loss = compute_loss(model, en_sentences_valid_loader,\n",
        "                              vi_sentences_valid_loader, device)\n",
        "    print('Train Loss: %.2f || Valid Loss: %.2f' % (train_loss.item(), valid_loss.item()))\n",
        "  epoch_elapsed_time = (time.time() - start_time) / 60\n",
        "  print('Epoch Elapsed Time: %.2f min' % (epoch_elapsed_time))\n",
        "total_training_time = (time.time() - start_time) / 60\n",
        "print('Total Training Time: %.2f min' % (total_training_time))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0938 || Epoch: 0001/0003 || Loss: 10.30\n",
            "Batch: 0300/0938 || Epoch: 0001/0003 || Loss: 5.75\n",
            "Batch: 0600/0938 || Epoch: 0001/0003 || Loss: 5.36\n",
            "Batch: 0900/0938 || Epoch: 0001/0003 || Loss: 5.96\n",
            "Train Loss: 5.47 || Valid Loss: 5.49\n",
            "Epoch Elapsed Time: 7.20 min\n",
            "Batch: 0000/0938 || Epoch: 0002/0003 || Loss: 5.74\n",
            "Batch: 0300/0938 || Epoch: 0002/0003 || Loss: 5.47\n",
            "Batch: 0600/0938 || Epoch: 0002/0003 || Loss: 5.24\n",
            "Batch: 0900/0938 || Epoch: 0002/0003 || Loss: 5.88\n",
            "Train Loss: 5.39 || Valid Loss: 5.41\n",
            "Epoch Elapsed Time: 14.43 min\n",
            "Batch: 0000/0938 || Epoch: 0003/0003 || Loss: 5.66\n",
            "Batch: 0300/0938 || Epoch: 0003/0003 || Loss: 5.42\n",
            "Batch: 0600/0938 || Epoch: 0003/0003 || Loss: 5.22\n",
            "Batch: 0900/0938 || Epoch: 0003/0003 || Loss: 5.85\n",
            "Train Loss: 5.36 || Valid Loss: 5.39\n",
            "Epoch Elapsed Time: 21.67 min\n",
            "Total Training Time: 21.67 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0xfgHGsIYgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}