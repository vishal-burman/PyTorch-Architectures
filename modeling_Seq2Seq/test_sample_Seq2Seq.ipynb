{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzfoHP/ODoBBD42LD4Nn/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_Seq2Seq/test_sample_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f79ZeRSEjDK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c606ef-ebf0-4f68-95f8-1056e74972c6"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/modeling_Seq2Seq/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'PyTorch-Architectures' already exists and is not an empty directory.\n",
            "/content/PyTorch-Architectures/modeling_Seq2Seq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1LL3xX7_jxC"
      },
      "source": [
        "! pip install datasets\n",
        "! pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWYjGbAQDTyc"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from model import Seq2Seq"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HDBiboTAezL",
        "outputId": "1a59c2a5-227a-4e5e-fe70-8d97e6400884"
      },
      "source": [
        "dataset = load_dataset('mt_eng_vietnamese', 'iwslt2015-en-vi')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset mt_eng_vietnamese (/root/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOwbnUkXArPp",
        "outputId": "d24d39d5-6534-4cd3-d816-eb14d428dee7"
      },
      "source": [
        "# Sample from the dataset\n",
        "dataset['train']['translation'][0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Rachel Pike : The science behind a climate headline',\n",
              " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ST17Cw7BDHH"
      },
      "source": [
        "train_sentences = dataset['train']\n",
        "en_sentences = []\n",
        "vi_sentences = []\n",
        "for value in train_sentences:\n",
        "  en_sentences.append(value['translation']['en'])\n",
        "  vi_sentences.append(value['translation']['vi'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vczI_eamDIXu"
      },
      "source": [
        "tokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer_vi = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[EOS]\", \"[PAD]\"])\n",
        "trainer_vi = BpeTrainer(special_tokens=[\"[UNK]\", \"[EOS]\", \"[PAD]\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff0KcPeoEFpa"
      },
      "source": [
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "tokenizer_vi.pre_tokenizer = Whitespace()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMpDSfiRERUm"
      },
      "source": [
        "tokenizer_en.train_from_iterator(en_sentences, trainer_en)\n",
        "tokenizer_vi.train_from_iterator(vi_sentences, trainer_vi)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DyIax-jEbkT"
      },
      "source": [
        "tokenizer_en.post_processor = TemplateProcessing(\n",
        "    single = \"$A [EOS]\",\n",
        "    special_tokens = [\n",
        "                      (\"[EOS]\", tokenizer_en.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer_vi.post_processor = TemplateProcessing(\n",
        "    single = \"$A [EOS]\",\n",
        "    special_tokens = [\n",
        "                      (\"[EOS]\", tokenizer_vi.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Paper uses the following format:\n",
        "# en_sentence [EOS] --> vi_sentence [EOS]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xW6QEPswCU0"
      },
      "source": [
        "# Cell for hyperparameters\n",
        "MAX_LENGTH = 16"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsgtM1O9vGn9"
      },
      "source": [
        "tokenizer_en.enable_padding(pad_id=2, pad_token=\"[PAD]\", len=MAX_LENGTH)\n",
        "tokenizer_vi.enable_padding(pad_id=2, pad_token=\"[PAD]\", len=MAX_LENGTH)\n",
        "tokenizer_en.enable_truncation(max_length=MAX_LENGTH)\n",
        "tokenizer_vi.enable_truncation(max_length=MAX_LENGTH)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhbj0naDHQCJ",
        "outputId": "2abfa4ae-4d30-4172-9b3c-04ff0723c549"
      },
      "source": [
        "# Sample tokenization of a batch of english sentences --> pad uses max_length in batch\n",
        "sample_sentence = [\"This is amazing and great!\", \"This is good\"]\n",
        "print('Sentences --> ', sample_sentence)\n",
        "output = tokenizer_en.encode_batch(sample_sentence)\n",
        "tensor_list = []\n",
        "tensor_list.append(output[0].ids)\n",
        "tensor_list.append(output[1].ids)\n",
        "torch.tensor(tensor_list).shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences -->  ['This is amazing and great!', 'This is good']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLZQE827JMHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b595f23-3ed2-48a1-80f9-3c6a1b8d7048"
      },
      "source": [
        "# Sample tokenization of a vietnamese sentence\n",
        "print('Sentence --> ', vi_sentences[0])\n",
        "output = tokenizer_vi.encode(vi_sentences[0])\n",
        "output.tokens"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence -->  Khoa học đằng sau một tiêu đề về khí hậu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Khoa',\n",
              " 'học',\n",
              " 'đằng',\n",
              " 'sau',\n",
              " 'một',\n",
              " 'tiêu',\n",
              " 'đề',\n",
              " 'về',\n",
              " 'khí',\n",
              " 'hậu',\n",
              " '[EOS]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSY3Hn4YJNGM"
      },
      "source": [
        "def collate_fn_en(batch):\n",
        "  sentences = []\n",
        "  for sent in batch:\n",
        "    sentences.append(sent[0])\n",
        "  outputs = tokenizer_en.encode_batch(sentences)\n",
        "  input_ids = []\n",
        "  for i in range(len(outputs)):\n",
        "    input_ids.append(outputs[i].ids)\n",
        "  return torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn_vi(batch):\n",
        "  sentences = []\n",
        "  for sent in batch:\n",
        "    sentences.append(sent[0])\n",
        "  outputs = tokenizer_vi.encode_batch(sentences)\n",
        "  input_ids = []\n",
        "  for i in range(len(outputs)):\n",
        "    input_ids.append(outputs[i].ids)\n",
        "  return torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, tokenizer, sentences, max_input_length=16):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.sentences = sentences\n",
        "    self.max_input_length = max_input_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    sents = [self.sentences[idx]]\n",
        "    sents_list = []\n",
        "    for sent in sents:\n",
        "      sents_list.append(sent)\n",
        "    return sents_list"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8vix3-toUs",
        "outputId": "b041261e-33f6-4332-dde7-218976bfb801"
      },
      "source": [
        "# Sanity check DataLoader\n",
        "sample_sentences_en = en_sentences[:4]\n",
        "sample_dataset_en = CustomDataset(tokenizer_en, sample_sentences_en)\n",
        "sample_dataloader_en = DataLoader(dataset=sample_dataset_en, batch_size=2, shuffle=False, drop_last=True, collate_fn=collate_fn_en)\n",
        "\n",
        "for sample in sample_dataloader_en:\n",
        "  print(sample.shape)\n",
        "  break\n",
        "\n",
        "sample_sentences_vi = vi_sentences[:4]\n",
        "sample_dataset_vi = CustomDataset(tokenizer_vi, sample_sentences_vi)\n",
        "sample_dataloader_vi = DataLoader(dataset=sample_dataset_vi, batch_size=2, shuffle=False, drop_last=True, collate_fn=collate_fn_vi)\n",
        "\n",
        "for sample in sample_dataloader_vi:\n",
        "  print(sample.shape)\n",
        "  break"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gil4zFQpJV6A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}