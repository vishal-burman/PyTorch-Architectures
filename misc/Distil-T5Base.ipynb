{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"! pip install sentencepiece\n! pip install fire\n! git clone https://github.com/huggingface/transformers.git\n! pip install transformers\n! pip install datasets","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting fire\n  Downloading fire-0.4.0.tar.gz (87 kB)\n\u001b[K     |████████████████████████████████| 87 kB 1.1 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire) (1.1.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=59bc25c2152977ad5881d5338bd5d3134d3175e8b3619aea1c871a387c80de78\n  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.4.0\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCloning into 'transformers'...\nremote: Enumerating objects: 24, done.\u001b[K\nremote: Counting objects: 100% (24/24), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 61230 (delta 7), reused 3 (delta 0), pack-reused 61206\u001b[K\nReceiving objects: 100% (61230/61230), 46.02 MiB | 24.82 MiB/s, done.\nResolving deltas: 100% (43273/43273), done.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.0.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.55.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.4)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.0)\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting datasets\n  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n\u001b[K     |████████████████████████████████| 159 kB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\nCollecting xxhash\n  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 5.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\nRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\nCollecting tqdm<4.50.0,>=4.27\n  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 4.1 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: xxhash, tqdm, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.55.1\n    Uninstalling tqdm-4.55.1:\n      Successfully uninstalled tqdm-4.55.1\nSuccessfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.0\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"! python transformers/examples/research_projects/seq2seq-distillation/make_student.py t5-base --save_path distil_t5_base_12_6 --e 12 --d 6","execution_count":2,"outputs":[{"output_type":"stream","text":"2021-02-03 07:59:10.486611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\nDownloading: 100%|██████████████████████████| 1.20k/1.20k [00:00<00:00, 910kB/s]\nDownloading: 100%|███████████████████████████| 792k/792k [00:00<00:00, 2.32MB/s]\nDownloading: 100%|█████████████████████████| 1.39M/1.39M [00:00<00:00, 3.57MB/s]\nDownloading: 100%|███████████████████████████| 892M/892M [00:19<00:00, 44.8MB/s]\nSome weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n(T5ForConditionalGeneration(   (shared): Embedding(32128, 768)   (encoder): T5Stack(     (embed_tokens): Embedding(32128, 768)     (block): ModuleList(       (0): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)               (relative_attention_bias): Embedding(32, 12)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (1): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (2): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (3): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (4): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (5): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (6): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (7): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (8): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (9): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (10): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (11): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )     )     (final_layer_norm): T5LayerNorm()     (dropout): Dropout(p=0.1, inplace=False)   )   (decoder): T5Stack(     (embed_tokens): Embedding(32128, 768)     (block): ModuleList(       (0): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)               (relative_attention_bias): Embedding(32, 12)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (1): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (2): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (3): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (4): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (5): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (1): T5LayerCrossAttention(             (EncDecAttention): T5Attention(               (q): Linear(in_features=768, out_features=768, bias=False)               (k): Linear(in_features=768, out_features=768, bias=False)               (v): Linear(in_features=768, out_features=768, bias=False)               (o): Linear(in_features=768, out_features=768, bias=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )           (2): T5LayerFF(             (DenseReluDense): T5DenseReluDense(               (wi): Linear(in_features=768, out_features=3072, bias=False)               (wo): Linear(in_features=3072, out_features=768, bias=False)               (dropout): Dropout(p=0.1, inplace=False)             )             (layer_norm): T5LayerNorm()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )     )     (final_layer_norm): T5LayerNorm()     (dropout): Dropout(p=0.1, inplace=False)   )   (lm_head): Linear(in_features=768, out_features=32128, bias=False) ), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [0, 2, 4, 7, 9, 11])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"quora\")","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1097.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072b46c4e10c4b8bba7aad733c623efd"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=559.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dd40eabb3f443199851149a66f5bb5"}},"metadata":{}},{"output_type":"stream","text":"Using custom data configuration default\n","name":"stderr"},{"output_type":"stream","text":"\nDownloading and preparing dataset quora/default (download: 55.48 MiB, generated: 55.46 MiB, post-processed: Unknown size, total: 110.94 MiB) to /root/.cache/huggingface/datasets/quora/default/0.0.0/2be517cf0ac6de94b77a103a36b141347a13f40637fbebaccb56ddbe397876be...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=58176133.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba611a67dca4f058b9c4e76b64fc974"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Dataset quora downloaded and prepared to /root/.cache/huggingface/datasets/quora/default/0.0.0/2be517cf0ac6de94b77a103a36b141347a13f40637fbebaccb56ddbe397876be. Subsequent calls will reuse this data.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nfor sample in dataset['train']:\n  if sample['is_duplicate']:\n    sentences.append({\n        'sent_1': sample['questions']['text'][0],\n        'sent_2': sample['questions']['text'][1],\n    })","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = sentences[:18000]\nvalid_sentences = sentences[18000:20000]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n  def __init__(self, tokenizer, text_list, max_input_length=64, max_target_length=64):\n    self.tokenizer = tokenizer\n    self.text_list = text_list\n    self.max_input_length = max_input_length\n    self.max_target_length = max_target_length\n  \n  def __len__(self):\n    return len(self.text_list)\n  \n  def __getitem__(self, idx):\n    sample = self.text_list[idx]\n    sentence_1 = sample['sent_1']\n    sentence_2 = sample['sent_2']\n    tokens_1 = self.tokenizer(sentence_1, max_length=self.max_input_length, padding='max_length', truncation=True, return_tensors='pt')\n    tokens_2 = self.tokenizer(sentence_2, max_length=self.max_target_length, padding='max_length', truncation=True, return_tensors='pt')\n    return {\n        'inp_ids': tokens_1['input_ids'],\n        'tgt_ids': tokens_2['input_ids'],\n    }","execution_count":10,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = T5Tokenizer.from_pretrained('distil_t5_base_12_6')\nmodel = T5ForConditionalGeneration.from_pretrained('distil_t5_base_12_6')\nmodel.to(device)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('Total Trainable Parameters: ', params)","execution_count":12,"outputs":[{"output_type":"stream","text":"Total Trainable Parameters:  166266624\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nLEARNING_RATE = 3e-5\nEPOCHS = 2","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = CustomDataset(tokenizer, train_sentences, max_input_length=85, max_target_length=85)\nvalid_dataset = CustomDataset(tokenizer, valid_sentences, max_input_length=85, max_target_length=85)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint('Length of Train Loader: ', len(train_loader))\nprint('Length of Valid Loader: ', len(valid_loader))","execution_count":14,"outputs":[{"output_type":"stream","text":"Length of Train Loader:  563\nLength of Valid Loader:  63\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_loss(model, data_loader, device):\n  list_loss = []\n  for sample in data_loader:\n    inp_ids = sample['inp_ids'].squeeze(1).to(device)\n    tgt_ids = sample['tgt_ids'].squeeze(1).to(device)\n    loss = model(input_ids=inp_ids, labels=tgt_ids).loss\n    list_loss.append(loss.item())\n  return torch.mean(torch.tensor(list_loss))\n\nstart_time = time.time()\nfor epoch in range(EPOCHS):\n  model.train()\n  for idx, sample in enumerate(train_loader):\n    inp_ids = sample['inp_ids'].squeeze(1).to(device)\n    tgt_ids = sample['tgt_ids'].squeeze(1).to(device)\n    loss = model(input_ids=inp_ids, labels=tgt_ids).loss\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Logging\n    if idx % 200 == 0:\n      print('Epoch: %04d/%04d || Batch: %04d/%04d || Loss: %.2f' % (epoch+1, EPOCHS, idx, len(train_loader), loss.item()))\n  \n  model.eval()\n  with torch.set_grad_enabled(False):\n    valid_loss = compute_loss(model, valid_loader, device)\n    print('Valid Loss: %.2f' % (valid_loss))\n  epoch_elapsed_time = (time.time() - start_time) / 60\n  print('Epoch Elapsed Time: %.2f' % (epoch_elapsed_time))\ntotal_training_time = (time.time() - start_time) / 60\nprint('Total Training Time: %.2f' % (total_training_time))","execution_count":16,"outputs":[{"output_type":"stream","text":"Epoch: 0001/0003 || Batch: 0000/0563 || Loss: 20.70\nEpoch: 0001/0003 || Batch: 0200/0563 || Loss: 0.51\nEpoch: 0001/0003 || Batch: 0400/0563 || Loss: 0.39\nValid Loss: 0.30\nEpoch Elapsed Time: 5.52\nEpoch: 0002/0003 || Batch: 0000/0563 || Loss: 0.30\nEpoch: 0002/0003 || Batch: 0200/0563 || Loss: 0.38\nEpoch: 0002/0003 || Batch: 0400/0563 || Loss: 0.24\nValid Loss: 0.27\nEpoch Elapsed Time: 11.02\nEpoch: 0003/0003 || Batch: 0000/0563 || Loss: 0.33\nEpoch: 0003/0003 || Batch: 0200/0563 || Loss: 0.26\nEpoch: 0003/0003 || Batch: 0400/0563 || Loss: 0.30\nValid Loss: 0.26\nEpoch Elapsed Time: 16.53\nTotal Training Time: 16.53\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\ntext = \"Can I really become financial independent if I spend money wisely?\"\nreturn_seq = 3\nmodel.eval()\nwith torch.set_grad_enabled(False):\n  tokens = tokenizer(text, max_length=85, truncation=True, padding=True, return_tensors='pt')\n  ids = tokens['input_ids'].to(device)\n  mask = tokens['attention_mask'].to(device)\n  outputs = model.generate(input_ids=ids, num_beams=return_seq * 5, early_stopping=True, max_length=85, num_return_sequences=return_seq, top_k=50, top_p=0.98)\n  for i in range(return_seq):\n    print(tokenizer.decode(outputs[i], skip_special_tokens=True))\nend_time = (time.time() - start_time)\nprint('Elapsed Time: %.2f sec' % (end_time))","execution_count":19,"outputs":[{"output_type":"stream","text":"how can I become financially independent?\nhow do I become financially independent?\ncan I become financially independent?\nElapsed Time: 0.18 sec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}