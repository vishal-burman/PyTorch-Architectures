{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentenceSimilarity(DistilRoBERTa_DistilRoBERTa).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvCN2q6Uqt3Fk0i+JSVUJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/misc/SentenceSimilarity(DistilRoBERTa_DistilRoBERTa).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgCrLMn_Rjcm"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obXzgtA5E_bu"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtI73nn1Fdww"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGkAof1sF4bd"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from toolkit.utils import get_optimal_batchsize"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4TjrOBGPJ5"
      },
      "source": [
        "dataset = load_dataset(\"quora\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6Jo-84oGUA-",
        "outputId": "a57e9cd7-4469-4380-9c64-2818abe04a2b"
      },
      "source": [
        "train_p = []\n",
        "train_n = []\n",
        "test_list = []\n",
        "count_p, count_n = 0, 0\n",
        "for idx, sample in enumerate(dataset[\"train\"]):\n",
        "  text_1, text_2 = sample[\"questions\"][\"text\"][0], sample[\"questions\"][\"text\"][1]\n",
        "  if len(train_p) < 10000 and sample[\"is_duplicate\"]:\n",
        "    train_p.append((text_1, text_2, 1))\n",
        "  elif len(train_n) < 10000 and not sample[\"is_duplicate\"]:\n",
        "    train_n.append((text_1, text_2, 0))\n",
        "  elif len(test_list) < 10000:\n",
        "    is_duplicate = 1 if sample[\"is_duplicate\"] else 0\n",
        "    test_list.append((text_1, text_2, is_duplicate))\n",
        "train_list = []\n",
        "train_list.extend(train_p)\n",
        "train_list.extend(train_n)\n",
        "random.shuffle(train_list)\n",
        "print(f\"No. of Train Samples: {len(train_list)} || No. of Test Samples: {len(test_list)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of Train Samples: 20000 || No. of Test Samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urtItUP2p-YU"
      },
      "source": [
        "path_str = \"distilroberta-base\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF0tPyU_tnVL"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, path_str: str, list_samples: list, max_input_length: int = 16):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(path_str)\n",
        "    self.list_samples = list_samples\n",
        "    self.max_input_length = max_input_length\n",
        "  \n",
        "  def __len__(self,):\n",
        "    return len(self.list_samples)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.list_samples[idx]\n",
        "    return {\n",
        "        'text_1': sample[0],\n",
        "        'text_2': sample[1],\n",
        "        'is_duplicate': sample[2],\n",
        "    }\n",
        "  \n",
        "  def collate_fn(self, batch):\n",
        "    text_1 = []\n",
        "    text_2 = []\n",
        "    labels = []\n",
        "    for sample in batch:\n",
        "      text_1.append(sample[\"text_1\"])\n",
        "      text_2.append(sample[\"text_2\"])\n",
        "      labels.append(sample[\"is_duplicate\"])\n",
        "    tokens_1 = self.tokenizer(text_1,\n",
        "                              max_length=self.max_input_length,\n",
        "                              padding=True,\n",
        "                              truncation=True,\n",
        "                              return_tensors=\"pt\",\n",
        "                              )\n",
        "    tokens_2 = self.tokenizer(text_2,\n",
        "                              max_length=self.max_input_length,\n",
        "                              padding=True,\n",
        "                              truncation=True,\n",
        "                              return_tensors=\"pt\",\n",
        "                              )\n",
        "    ids_1, att_1 = tokens_1[\"input_ids\"], tokens_1[\"attention_mask\"]\n",
        "    ids_2, att_2 = tokens_2[\"input_ids\"], tokens_2[\"attention_mask\"]\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return {\n",
        "        \"input_ids_1\": ids_1,\n",
        "        \"attention_mask_1\": att_1,\n",
        "        \"input_ids_2\": ids_2,\n",
        "        \"attention_mask_2\": att_2,\n",
        "        \"labels\": labels,\n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb0vcbhXGtz9"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, in_size: int = 768, hidden_size: int = 512):\n",
        "    super().__init__()\n",
        "    self.W = nn.Linear(in_size, hidden_size)\n",
        "    self.V = nn.Linear(hidden_size, 1)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.tanh(self.W(x))\n",
        "    score = self.V(x)\n",
        "    attention_weights = score.softmax(dim=1)\n",
        "    context_vector = x * attention_weights\n",
        "    context_vector = torch.sum(context_vector, dim=1)\n",
        "    output = self.dropout(context_vector)\n",
        "    return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwKwlQjnJoNq"
      },
      "source": [
        "class SentenceSimilarity(nn.Module):\n",
        "  def __init__(self, path_str: str, in_size: int = 768, hidden_size: int = 768):\n",
        "    super().__init__()\n",
        "    self.encoder = AutoModel.from_pretrained(path_str)\n",
        "    self.attention = Attention(in_size, hidden_size)\n",
        "    self.ff = nn.Linear(hidden_size * 2, 2)\n",
        "  \n",
        "  def forward(self,\n",
        "              input_ids_1,\n",
        "              attention_mask_1,\n",
        "              input_ids_2,\n",
        "              attention_mask_2,\n",
        "              labels=None,\n",
        "              ):\n",
        "    \n",
        "    outputs_1 = self.encoder(input_ids=input_ids_1,\n",
        "                               attention_mask=attention_mask_1)\n",
        "    outputs_2 = self.encoder(input_ids=input_ids_2,\n",
        "                               attention_mask=attention_mask_2)\n",
        "    \n",
        "    enc_weights_1 = self.attention(outputs_1.last_hidden_state)\n",
        "    enc_weights_2 = self.attention(outputs_2.last_hidden_state)\n",
        "\n",
        "    output = torch.cat([enc_weights_1, enc_weights_2], dim=1)\n",
        "\n",
        "    logits = self.ff(output)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(logits.size(0), -1), labels.view(-1))\n",
        "    return (loss, logits)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyViEY4cQBJp"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentenceSimilarity(path_str=path_str)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPzpP8qQQY-A",
        "outputId": "4e355843-fdae-444b-be61-f8f128216dea"
      },
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable Parameters: {params}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable Parameters: 82712835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "759yadjXQncK"
      },
      "source": [
        "# Create Datasets\n",
        "dataset_train = CustomDataset(path_str=path_str,\n",
        "                              list_samples=train_list,\n",
        "                              max_input_length=16,\n",
        "                              )\n",
        "dataset_valid = CustomDataset(path_str=path_str,\n",
        "                              list_samples=test_list,\n",
        "                              max_input_length=16,\n",
        "                              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyl0neZFRENU",
        "outputId": "c73a6257-b220-4076-986a-6b1bccffe34e"
      },
      "source": [
        "# get_optimal_batchsize(dataset_train, model, fp16=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch-Size: 512\n",
            "Batch-Size with MVF: 768 --> EXPERIMENTAL!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMI0GqqbRdL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}