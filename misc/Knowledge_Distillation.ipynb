{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge_Distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhnvp3CistNEQ23fK4Pa/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/Knowledge_Distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTadUnQPtoDF"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjCtwhbLhf9",
        "outputId": "1ce69c60-845e-4a29-9475-8b2902357cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "batch_size = 64\n",
        "train_dataset = datasets.MNIST(root='data', \n",
        "                               train=True, \n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data', \n",
        "                              train=False, \n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=batch_size, \n",
        "                         shuffle=False)\n",
        "print(\"Length of train_loader: \", len(train_loader))\n",
        "print(\"Length of test_loader: \", len(test_loader))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_loader:  938\n",
            "Length of test_loader:  157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTyo36hhukih"
      },
      "source": [
        "# Create the teacher\n",
        "class Teacher(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Teacher, self).__init__()\n",
        "    # input_shape ~ [batch_size, 1, 28, 28]\n",
        "    # shape ~ [batch_size, 256, 14, 14]\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 256, 14, 14]\n",
        "    self.lr_1 = nn.LeakyReLU(inplace=True)\n",
        "    # shape ~ [batch_size, 256, 15, 15]\n",
        "    self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 512, 8, 8]\n",
        "    self.conv_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 10]\n",
        "    self.lin_1 = nn.Linear(in_features=512 * 8 * 8, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_1(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.lin_1(x)\n",
        "    return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUcz7icbGMB4"
      },
      "source": [
        "# Create Student\n",
        "class Student(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Student, self).__init__()\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    \n",
        "    self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "    self.conv_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    \n",
        "    self.lin_1 = nn.Linear(in_features=32 * 8 * 8, out_features=10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv_1(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.lin_1(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQj3hlrJfX6",
        "outputId": "5d7d876d-daa4-4287-ed91-5990a097b56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "teacher = Teacher(num_classes=10)\n",
        "student = Student(num_classes=10)\n",
        "student = student.to(device)\n",
        "teacher = teacher.to(device)\n",
        "total_params_t = sum(p.numel() for p in teacher.parameters())\n",
        "total_params_s = sum(p.numel() for p in student.parameters())\n",
        "print(\"Total Parameters in Teacher: \", total_params_t)\n",
        "print(\"Total Parameters in Student: \", total_params_s)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Parameters in Teacher:  1510410\n",
            "Total Parameters in Student:  25290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VQB2kDTunu2"
      },
      "source": [
        "# Creating a clone of student\n",
        "student_clone = Student(num_classes=10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjgZDN3NV9l",
        "outputId": "ab24aa91-aee0-4071-a277-0e8d69d571db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "# Train Teacher\n",
        "optimizer_t = torch.optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "loss_t = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_loss(model, data_loader, loss_generic, device):\n",
        "  tot = 0.\n",
        "  model.eval()\n",
        "  for features, targets in data_loader:\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "    logits = model(features)\n",
        "    loss = loss_generic(logits, targets)\n",
        "    tot += loss.item()\n",
        "  return tot/len(data_loader)\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            probas = F.softmax(logits, dim=1)\n",
        "            _, predicted_labels = torch.max(probas, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "EPOCHS = 4\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  teacher.train()\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer_t.zero_grad()\n",
        "    logits = teacher(features)\n",
        "    loss = loss_t(logits, targets)\n",
        "    \n",
        "    # LOGGING\n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Batch: %03d/%03d\" % (batch_idx, len(train_loader)))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_t.step()\n",
        "  teacher.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_average_loss = compute_loss(teacher, train_loader, loss_t, device)\n",
        "    test_average_loss = compute_loss(teacher, test_loader, loss_t, device)\n",
        "    test_accuracy = compute_accuracy(teacher, test_loader, device)\n",
        "    print(\"Epoch: %03d/%03d | Teacher Train Loss: %.3f | Teacher Test Loss: %.3f | Teacher Test Accuracy: %.2f\" % (epoch+1, EPOCHS, train_average_loss, test_average_loss, test_accuracy))\n",
        "  epoch_elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch Elapsed Time: \", epoch_elapsed_time)\n",
        "total_training_time = time.time() - start_time\n",
        "print(\"Total Training Time: \", total_training_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 001/004 | Teacher Train Loss: 0.095 | Teacher Test Loss: 0.087 | Teacher Test Accuracy: 97.40\n",
            "Epoch Elapsed Time:  17.808791637420654\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 002/004 | Teacher Train Loss: 0.062 | Teacher Test Loss: 0.059 | Teacher Test Accuracy: 98.20\n",
            "Epoch Elapsed Time:  35.56360054016113\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 003/004 | Teacher Train Loss: 0.062 | Teacher Test Loss: 0.062 | Teacher Test Accuracy: 97.92\n",
            "Epoch Elapsed Time:  53.29674530029297\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 004/004 | Teacher Train Loss: 0.048 | Teacher Test Loss: 0.053 | Teacher Test Accuracy: 98.22\n",
            "Epoch Elapsed Time:  71.29234600067139\n",
            "Total Training Time:  71.29257702827454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtaKfPXYtnTR",
        "outputId": "a11e794e-7bf3-40a9-c3c3-217d19e1b98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Create distil training\n",
        "optimizer_distil = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "loss_s = nn.CrossEntropyLoss()\n",
        "loss_distil = nn.KLDivLoss(reduction='batchmean')\n",
        "alpha = 0.1\n",
        "temperature = 10\n",
        "EPOCHS = 3\n",
        "\n",
        "# Freezing the layers of Teacher\n",
        "teacher.eval()\n",
        "for parameter in teacher.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  student.train()\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    teacher_logits = teacher(features)\n",
        "    \n",
        "    optimizer_distil.zero_grad()\n",
        "    student_logits = student(features)\n",
        "\n",
        "    student_loss = loss_s(student_logits, targets)\n",
        "    distillation_loss = loss_distil(F.log_softmax(input=(student_logits/temperature), dim=1), F.log_softmax(input=(teacher_logits/temperature), dim=1))\n",
        "    loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "    # LOGGING\n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Batch: %03d/%03d\" % (batch_idx, len(train_loader)))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_distil.step()\n",
        "\n",
        "  student.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_average_loss = compute_loss(student, train_loader, loss_s, device)\n",
        "    test_average_loss = compute_loss(student, test_loader, loss_s, device)\n",
        "    test_accuracy = compute_accuracy(student, test_loader, device)\n",
        "    print(\"Epoch: %03d/%03d | Student Train Loss: %.3f | Student Test Loss: %.3f | Student Test Accuracy: %.2f\" % (epoch+1, EPOCHS, train_average_loss, test_average_loss, test_accuracy))\n",
        "  epoch_elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch Elapsed Time: \", epoch_elapsed_time)\n",
        "total_training_time = time.time() - start_time\n",
        "print(\"Total Training Time: \", total_training_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 001/003 | Student Train Loss: 0.120 | Student Test Loss: 0.108 | Student Test Accuracy: 96.83\n",
            "Epoch Elapsed Time:  10.960257053375244\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 002/003 | Student Train Loss: 0.078 | Student Test Loss: 0.072 | Student Test Accuracy: 97.69\n",
            "Epoch Elapsed Time:  22.495341300964355\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 003/003 | Student Train Loss: 0.067 | Student Test Loss: 0.063 | Student Test Accuracy: 98.00\n",
            "Epoch Elapsed Time:  33.67755627632141\n",
            "Total Training Time:  33.67793083190918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snEul80yqTnY",
        "outputId": "d33fadb4-b299-4a53-da3c-5e8ee00b96b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Train Student clone from scratch for comparison\n",
        "student_clone = student_clone.to(device)\n",
        "optimizer_clone = torch.optim.Adam(student_clone.parameters(), lr=0.001)\n",
        "loss_clone = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 3\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  student_clone.train()\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer_clone.zero_grad()\n",
        "    logits = student_clone(features)\n",
        "    loss = loss_clone(logits, targets)\n",
        "    \n",
        "    # LOGGING\n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Batch: %03d/%03d\" % (batch_idx, len(train_loader)))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_clone.step()\n",
        "  student_clone.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_average_loss = compute_loss(student_clone, train_loader, loss_clone, device)\n",
        "    test_average_loss = compute_loss(student_clone, test_loader, loss_clone, device)\n",
        "    test_accuracy = compute_accuracy(student_clone, test_loader, device)\n",
        "    print(\"Epoch: %03d/%03d | Clone_Student Train Loss: %.3f | Clone_Student Test Loss: %.3f | Clone_Student Test Accuracy: %.2f\" % (epoch+1, EPOCHS, train_average_loss, test_average_loss, test_accuracy))\n",
        "  epoch_elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch Elapsed Time: \", epoch_elapsed_time)\n",
        "total_training_time = time.time() - start_time\n",
        "print(\"Total Training Time: \", total_training_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 001/003 | Clone_Student Train Loss: 0.118 | Clone_Student Test Loss: 0.110 | Clone_Student Test Accuracy: 96.74\n",
            "Epoch Elapsed Time:  10.838661193847656\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 002/003 | Clone_Student Train Loss: 0.079 | Clone_Student Test Loss: 0.072 | Clone_Student Test Accuracy: 97.69\n",
            "Epoch Elapsed Time:  21.47327733039856\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 003/003 | Clone_Student Train Loss: 0.069 | Clone_Student Test Loss: 0.068 | Clone_Student Test Accuracy: 97.83\n",
            "Epoch Elapsed Time:  31.9901065826416\n",
            "Total Training Time:  31.99079704284668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KVy1ZuW1ho1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}