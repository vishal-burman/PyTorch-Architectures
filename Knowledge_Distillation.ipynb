{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge_Distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwWwEEzbPgKp1E4tZ1VHl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/Knowledge_Distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1gMCkcBdG3Z"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTadUnQPtoDF"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjCtwhbLhf9",
        "outputId": "1c64c9d3-5a9a-49e7-94a5-d9eea89731c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "batch_size = 64\n",
        "train_dataset = datasets.MNIST(root='data', \n",
        "                               train=True, \n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data', \n",
        "                              train=False, \n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=batch_size, \n",
        "                         shuffle=False)\n",
        "print(\"Length of train_loader: \", len(train_loader))\n",
        "print(\"Length of test_loader: \", len(test_loader))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_loader:  938\n",
            "Length of test_loader:  157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTyo36hhukih"
      },
      "source": [
        "# Create the teacher\n",
        "class Teacher(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Teacher, self).__init__()\n",
        "    # input_shape ~ [batch_size, 1, 28, 28]\n",
        "    # shape ~ [batch_size, 256, 14, 14]\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 256, 14, 14]\n",
        "    self.lr_1 = nn.LeakyReLU(inplace=True)\n",
        "    # shape ~ [batch_size, 256, 15, 15]\n",
        "    self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 512, 8, 8]\n",
        "    self.conv_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    # shape ~ [batch_size, 10]\n",
        "    self.lin_1 = nn.Linear(in_features=512 * 8 * 8, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_1(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.lin_1(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUcz7icbGMB4"
      },
      "source": [
        "class Student(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Student, self).__init__()\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    \n",
        "    self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "    self.conv_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "    \n",
        "    self.lin_1 = nn.Linear(in_features=32 * 8 * 8, out_features=10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv_1(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.lin_1(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQj3hlrJfX6",
        "outputId": "dd485d36-2f9b-4e49-a1d5-3b9ecdefde7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "teacher = Teacher(num_classes=10)\n",
        "student = Student(num_classes=10)\n",
        "student = student.to(device)\n",
        "teacher = teacher.to(device)\n",
        "total_params_t = sum(p.numel() for p in teacher.parameters())\n",
        "total_params_s = sum(p.numel() for p in student.parameters())\n",
        "print(\"Total Parameters in Teacher: \", total_params_t)\n",
        "print(\"Total Parameters in Student: \", total_params_s)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Parameters in Teacher:  1510410\n",
            "Total Parameters in Student:  25290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjgZDN3NV9l",
        "outputId": "dafa3bdd-5676-484b-d76f-c04008e39144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# Train Teacher\n",
        "optimizer_t = torch.optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "loss_t = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_loss(model, data_loader, device):\n",
        "  tot = 0.\n",
        "  model.eval()\n",
        "  for features, targets in data_loader:\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "    logits = model(features)\n",
        "    loss = loss_t(logits, targets)\n",
        "    tot += loss.item()\n",
        "  return tot/len(data_loader)\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  teacher.train()\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    features = features.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer_t.zero_grad()\n",
        "    logits = teacher(features)\n",
        "    loss = loss_t(logits, targets)\n",
        "    \n",
        "    # LOGGING\n",
        "    if batch_idx % 200 == 0:\n",
        "      print(\"Batch: %03d/%03d\" % (batch_idx, len(train_loader)))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_t.step()\n",
        "  teacher.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_average_loss = compute_loss(teacher, train_loader, device)\n",
        "    test_average_loss = compute_loss(teacher, test_loader, device)\n",
        "    print(\"Epoch: %03d/%03d | Train Loss: %.3f | Test Loss: %.3f\" % (epoch+1, EPOCHS, train_average_loss, test_average_loss))\n",
        "  epoch_elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch Elapsed Time: \", epoch_elapsed_time)\n",
        "total_training_time = time.time() - start_time\n",
        "print(\"Total Training Time: \", total_training_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 001/005 | Train Loss: 0.089 | Test Loss: 0.081\n",
            "Epoch Elapsed Time:  18.70784282684326\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 002/005 | Train Loss: 0.061 | Test Loss: 0.057\n",
            "Epoch Elapsed Time:  37.91933751106262\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 003/005 | Train Loss: 0.054 | Test Loss: 0.052\n",
            "Epoch Elapsed Time:  57.7604284286499\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 004/005 | Train Loss: 0.054 | Test Loss: 0.060\n",
            "Epoch Elapsed Time:  77.10402655601501\n",
            "Batch: 000/938\n",
            "Batch: 200/938\n",
            "Batch: 400/938\n",
            "Batch: 600/938\n",
            "Batch: 800/938\n",
            "Epoch: 005/005 | Train Loss: 0.042 | Test Loss: 0.051\n",
            "Epoch Elapsed Time:  95.94168615341187\n",
            "Total Training Time:  95.94187355041504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtaKfPXYtnTR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}