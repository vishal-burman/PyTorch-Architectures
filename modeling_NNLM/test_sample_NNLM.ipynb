{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNl/p+ZQrGEHTMh38LsV8LG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_NNLM/test_sample_NNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmW_PBhAFxL"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zR4Y1nFtiUG"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSAm3fBAu7L_",
        "outputId": "d7ab5186-3136-4a89-eae8-26ec4a44cedf"
      },
      "source": [
        "%cd PyTorch-Architectures/modeling_NNLM/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch-Architectures/modeling_NNLM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts2CGtKqtpXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e08a876-1f1b-495d-b8aa-e62fad7e50e1"
      },
      "source": [
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from model import NNLM\r\n",
        "\r\n",
        "from datasets import load_dataset\r\n",
        "dataset = load_dataset('quora')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset quora (/root/.cache/huggingface/datasets/quora/default/0.0.0/2be517cf0ac6de94b77a103a36b141347a13f40637fbebaccb56ddbe397876be)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTpqcq2OtwJn"
      },
      "source": [
        "sentences = []\r\n",
        "for sample in dataset['train']:\r\n",
        "  if len(sentences) == 10000:\r\n",
        "    break\r\n",
        "  sent = sample['questions']['text'][0]\r\n",
        "  if len(sent.split()) >= 4:\r\n",
        "    sentences.append(sent)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkcdgZAyJmV",
        "outputId": "18d1065a-3786-43f6-e237-a8bcde60c455"
      },
      "source": [
        "word_list = ' '.join(sentences).split()\r\n",
        "word_list = list(set(word_list))\r\n",
        "\r\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\r\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\r\n",
        "n_class = len(word_dict)\r\n",
        "print('Vocabulary Size: ', n_class)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  18198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FUFQ-upt7f4"
      },
      "source": [
        "class CustomDataset(Dataset):\r\n",
        "  def __init__(self, list_sentences, max_inp_length=4):\r\n",
        "    self.list_sentences = list_sentences\r\n",
        "    self.max_inp_length = max_inp_length\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.list_sentences)\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    sentences = self.list_sentences[idx]\r\n",
        "    tokens = self.tokenize_into_tensors(sentences)\r\n",
        "    return {\r\n",
        "        'input_batch': tokens['inp_batch'],\r\n",
        "        'target_batch': tokens['tgt_batch'],\r\n",
        "    }\r\n",
        "  \r\n",
        "  def tokenize_into_tensors(self, sentence):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    word = sentence.split()\r\n",
        "    word = word[:self.max_inp_length]\r\n",
        "    input_tokens = [word_dict[n] for n in word[:-1]]\r\n",
        "    target_tokens = word_dict[word[-1]]\r\n",
        "    input_batch.append(input_tokens)\r\n",
        "    target_batch.append(target_tokens)\r\n",
        "    return {\r\n",
        "        'inp_batch': torch.tensor(input_batch),\r\n",
        "        'tgt_batch': torch.tensor(target_batch),\r\n",
        "    }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AyF8FTy3SX",
        "outputId": "11c6fae9-0663-4f7e-fbed-0c16aa025fbb"
      },
      "source": [
        "lim = 90 * len(sentences) // 100\r\n",
        "train_sentences = sentences[:lim]\r\n",
        "valid_sentences = sentences[lim:]\r\n",
        "print('Train Samples: ', len(train_sentences))\r\n",
        "print('Valid Samples: ', len(valid_sentences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples:  9000\n",
            "Valid Samples:  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdYwU6m1kQM"
      },
      "source": [
        "train_dataset = CustomDataset(train_sentences, max_inp_length=4)\r\n",
        "valid_dataset = CustomDataset(valid_sentences, max_inp_length=4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9I-VPH1-XG"
      },
      "source": [
        "# Hyperparameters\r\n",
        "m = 200\r\n",
        "n_hidden = 100\r\n",
        "n_step = 3\r\n",
        "BATCH_SIZE = 32\r\n",
        "LEARNING_RATE = 0.001\r\n",
        "EPOCHS = 10"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNRcpi6EBUE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245cb918-b9be-4207-e6ab-2391a558acab"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = NNLM(n_class=n_class, m=m, n_hidden=n_hidden, n_step=n_step)\r\n",
        "model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NNLM(\n",
              "  (C): Embedding(18198, 200)\n",
              "  (H): Linear(in_features=600, out_features=100, bias=False)\n",
              "  (U): Linear(in_features=100, out_features=18198, bias=False)\n",
              "  (W): Linear(in_features=600, out_features=18198, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxIQ7WaWk9UP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566be4e5-0686-4f63-b401-c273cc945f58"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "# Sanity check DataLoader\r\n",
        "for sample in train_loader:\r\n",
        "  assert sample['input_batch'].squeeze(1).dim() == 2\r\n",
        "  assert sample['target_batch'].dim() == 2\r\n",
        "  break\r\n",
        "\r\n",
        "print('Length of Train Loader: ', len(train_loader))\r\n",
        "print('Length of Valid Loader: ', len(valid_loader))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train Loader:  282\n",
            "Length of Valid Loader:  32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBKQNgHflQv5"
      },
      "source": [
        "# Sanity check model outputs\r\n",
        "model.eval()\r\n",
        "with torch.set_grad_enabled(False):\r\n",
        "  outputs = model(sample['input_batch'].squeeze(1))\r\n",
        "  assert outputs.size(1) == n_class"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcfAFgu4nH5V"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRnz-v4pmINb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5561e304-a6f3-4096-cae9-6aadecc2214a"
      },
      "source": [
        "def compute_loss(model, data_loader, device):\r\n",
        "  list_loss = []\r\n",
        "  with torch.set_grad_enabled(False):\r\n",
        "    for sample in data_loader:\r\n",
        "      features = sample['input_batch'].squeeze(1)\r\n",
        "      targets = sample['target_batch'].squeeze(1)\r\n",
        "\r\n",
        "      logits = model(features)\r\n",
        "      loss = F.cross_entropy(logits, targets)\r\n",
        "      list_loss.append(loss.item())\r\n",
        "  return torch.tensor(list_loss).mean()\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  model.train()\r\n",
        "  for idx, sample in enumerate(train_loader):\r\n",
        "    features = sample['input_batch'].squeeze(1)\r\n",
        "    targets = sample['target_batch'].squeeze(1)\r\n",
        "\r\n",
        "    logits = model(features)\r\n",
        "    loss = F.cross_entropy(logits, targets)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # LOGGING\r\n",
        "    if idx % 50 == 0:\r\n",
        "      print('Batch: %04d/%04d || Epoch: %04d/%04d || Loss: %.2f' % (idx, len(train_loader), epoch+1, EPOCHS, loss.item()))\r\n",
        "  \r\n",
        "  model.eval()\r\n",
        "  with torch.set_grad_enabled(False):\r\n",
        "    train_loss = compute_loss(model, train_loader, device)\r\n",
        "    valid_loss = compute_loss(model, valid_loader, device)\r\n",
        "\r\n",
        "    print('Train Loss: %.2f' % (train_loss.item()))\r\n",
        "    print('Valid Loss: %.2f' % (valid_loss.item()))\r\n",
        "  epoch_elapsed_time = (time.time() - start_time) / 60\r\n",
        "  print('Epoch Elapsed Time: %.2f min' % (epoch_elapsed_time))\r\n",
        "total_training_time = (time.time() - start_time) / 60\r\n",
        "print('Total Training Time: %.2f min' % (total_training_time))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0282 || Epoch: 0001/0010 || Loss: 9.95\n",
            "Batch: 0050/0282 || Epoch: 0001/0010 || Loss: 9.98\n",
            "Batch: 0100/0282 || Epoch: 0001/0010 || Loss: 9.90\n",
            "Batch: 0150/0282 || Epoch: 0001/0010 || Loss: 9.46\n",
            "Batch: 0200/0282 || Epoch: 0001/0010 || Loss: 9.51\n",
            "Batch: 0250/0282 || Epoch: 0001/0010 || Loss: 9.49\n",
            "Train Loss: 9.22\n",
            "Valid Loss: 9.22\n",
            "Epoch Elapsed Time: 0.65 min\n",
            "Batch: 0000/0282 || Epoch: 0002/0010 || Loss: 9.35\n",
            "Batch: 0050/0282 || Epoch: 0002/0010 || Loss: 9.36\n",
            "Batch: 0100/0282 || Epoch: 0002/0010 || Loss: 8.77\n",
            "Batch: 0150/0282 || Epoch: 0002/0010 || Loss: 8.60\n",
            "Batch: 0200/0282 || Epoch: 0002/0010 || Loss: 8.47\n",
            "Batch: 0250/0282 || Epoch: 0002/0010 || Loss: 9.19\n",
            "Train Loss: 8.90\n",
            "Valid Loss: 8.90\n",
            "Epoch Elapsed Time: 1.28 min\n",
            "Batch: 0000/0282 || Epoch: 0003/0010 || Loss: 9.78\n",
            "Batch: 0050/0282 || Epoch: 0003/0010 || Loss: 8.93\n",
            "Batch: 0100/0282 || Epoch: 0003/0010 || Loss: 7.81\n",
            "Batch: 0150/0282 || Epoch: 0003/0010 || Loss: 8.87\n",
            "Batch: 0200/0282 || Epoch: 0003/0010 || Loss: 9.08\n",
            "Batch: 0250/0282 || Epoch: 0003/0010 || Loss: 8.21\n",
            "Train Loss: 8.62\n",
            "Valid Loss: 8.63\n",
            "Epoch Elapsed Time: 1.91 min\n",
            "Batch: 0000/0282 || Epoch: 0004/0010 || Loss: 8.75\n",
            "Batch: 0050/0282 || Epoch: 0004/0010 || Loss: 8.74\n",
            "Batch: 0100/0282 || Epoch: 0004/0010 || Loss: 7.82\n",
            "Batch: 0150/0282 || Epoch: 0004/0010 || Loss: 8.43\n",
            "Batch: 0200/0282 || Epoch: 0004/0010 || Loss: 8.41\n",
            "Batch: 0250/0282 || Epoch: 0004/0010 || Loss: 7.91\n",
            "Train Loss: 8.41\n",
            "Valid Loss: 8.43\n",
            "Epoch Elapsed Time: 2.55 min\n",
            "Batch: 0000/0282 || Epoch: 0005/0010 || Loss: 8.09\n",
            "Batch: 0050/0282 || Epoch: 0005/0010 || Loss: 8.31\n",
            "Batch: 0100/0282 || Epoch: 0005/0010 || Loss: 8.62\n",
            "Batch: 0150/0282 || Epoch: 0005/0010 || Loss: 7.45\n",
            "Batch: 0200/0282 || Epoch: 0005/0010 || Loss: 8.78\n",
            "Batch: 0250/0282 || Epoch: 0005/0010 || Loss: 8.28\n",
            "Train Loss: 8.23\n",
            "Valid Loss: 8.26\n",
            "Epoch Elapsed Time: 3.17 min\n",
            "Batch: 0000/0282 || Epoch: 0006/0010 || Loss: 8.18\n",
            "Batch: 0050/0282 || Epoch: 0006/0010 || Loss: 8.02\n",
            "Batch: 0100/0282 || Epoch: 0006/0010 || Loss: 8.51\n",
            "Batch: 0150/0282 || Epoch: 0006/0010 || Loss: 8.79\n",
            "Batch: 0200/0282 || Epoch: 0006/0010 || Loss: 8.67\n",
            "Batch: 0250/0282 || Epoch: 0006/0010 || Loss: 7.92\n",
            "Train Loss: 8.07\n",
            "Valid Loss: 8.12\n",
            "Epoch Elapsed Time: 3.79 min\n",
            "Batch: 0000/0282 || Epoch: 0007/0010 || Loss: 8.53\n",
            "Batch: 0050/0282 || Epoch: 0007/0010 || Loss: 8.27\n",
            "Batch: 0100/0282 || Epoch: 0007/0010 || Loss: 8.12\n",
            "Batch: 0150/0282 || Epoch: 0007/0010 || Loss: 7.51\n",
            "Batch: 0200/0282 || Epoch: 0007/0010 || Loss: 7.29\n",
            "Batch: 0250/0282 || Epoch: 0007/0010 || Loss: 7.50\n",
            "Train Loss: 7.93\n",
            "Valid Loss: 8.01\n",
            "Epoch Elapsed Time: 4.41 min\n",
            "Batch: 0000/0282 || Epoch: 0008/0010 || Loss: 7.23\n",
            "Batch: 0050/0282 || Epoch: 0008/0010 || Loss: 7.27\n",
            "Batch: 0100/0282 || Epoch: 0008/0010 || Loss: 7.32\n",
            "Batch: 0150/0282 || Epoch: 0008/0010 || Loss: 8.18\n",
            "Batch: 0200/0282 || Epoch: 0008/0010 || Loss: 8.91\n",
            "Batch: 0250/0282 || Epoch: 0008/0010 || Loss: 6.85\n",
            "Train Loss: 7.81\n",
            "Valid Loss: 7.90\n",
            "Epoch Elapsed Time: 5.03 min\n",
            "Batch: 0000/0282 || Epoch: 0009/0010 || Loss: 7.57\n",
            "Batch: 0050/0282 || Epoch: 0009/0010 || Loss: 7.88\n",
            "Batch: 0100/0282 || Epoch: 0009/0010 || Loss: 7.78\n",
            "Batch: 0150/0282 || Epoch: 0009/0010 || Loss: 8.10\n",
            "Batch: 0200/0282 || Epoch: 0009/0010 || Loss: 7.40\n",
            "Batch: 0250/0282 || Epoch: 0009/0010 || Loss: 8.23\n",
            "Train Loss: 7.70\n",
            "Valid Loss: 7.81\n",
            "Epoch Elapsed Time: 5.67 min\n",
            "Batch: 0000/0282 || Epoch: 0010/0010 || Loss: 8.23\n",
            "Batch: 0050/0282 || Epoch: 0010/0010 || Loss: 8.31\n",
            "Batch: 0100/0282 || Epoch: 0010/0010 || Loss: 6.92\n",
            "Batch: 0150/0282 || Epoch: 0010/0010 || Loss: 6.91\n",
            "Batch: 0200/0282 || Epoch: 0010/0010 || Loss: 7.28\n",
            "Batch: 0250/0282 || Epoch: 0010/0010 || Loss: 7.50\n",
            "Train Loss: 7.61\n",
            "Valid Loss: 7.73\n",
            "Epoch Elapsed Time: 6.28 min\n",
            "Total Training Time: 6.28 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVYciFsnuPvx",
        "outputId": "d07e9444-3438-49f9-e31f-6b787b0058f0"
      },
      "source": [
        "model.eval()\r\n",
        "with torch.set_grad_enabled(False):\r\n",
        "  text = \"What will be\".split()\r\n",
        "  input_tokens = [word_dict[n] for n in text]\r\n",
        "  input_tokens = torch.tensor(input_tokens).unsqueeze(0)\r\n",
        "  logits = model(input_tokens)\r\n",
        "  probas = F.softmax(logits, dim=1)\r\n",
        "  _, predicted_word_idx = torch.max(probas, 1)\r\n",
        "  print('Your input --> ', ' '.join(text))\r\n",
        "  print('Predicted next token --> ', number_dict[predicted_word_idx.item()])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your input -->  What will be\n",
            "Predicted next token -->  the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SCHXyyAugEq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}