{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOewwcRWEne0mKEh7/utsUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_NNLM/test_sample_NNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zR4Y1nFtiUG"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts2CGtKqtpXH",
        "outputId": "6437b812-1e72-43b7-edc9-a92d8e63d729"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "from datasets import load_dataset\r\n",
        "dataset = load_dataset('quora')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset quora (/root/.cache/huggingface/datasets/quora/default/0.0.0/2be517cf0ac6de94b77a103a36b141347a13f40637fbebaccb56ddbe397876be)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTpqcq2OtwJn"
      },
      "source": [
        "sentences = []\r\n",
        "for sample in dataset['train']:\r\n",
        "  if len(sentences) == 10000:\r\n",
        "    break\r\n",
        "  sent = sample['questions']['text'][0]\r\n",
        "  if len(sent.split()) >= 4:\r\n",
        "    sentences.append(sent)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkcdgZAyJmV",
        "outputId": "8a569ffe-391b-4ee6-d333-c1ba4146a728"
      },
      "source": [
        "word_list = ' '.join(sentences).split()\r\n",
        "word_list = list(set(word_list))\r\n",
        "\r\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\r\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\r\n",
        "n_class = len(word_dict)\r\n",
        "print('Vocabulary Size: ', n_class)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  18198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FUFQ-upt7f4"
      },
      "source": [
        "class CustomDataset(Dataset):\r\n",
        "  def __init__(self, list_sentences, max_inp_length=4):\r\n",
        "    self.list_sentences = list_sentences\r\n",
        "    self.max_inp_length = max_inp_length\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.list_sentences)\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    sentences = self.list_sentences[idx]\r\n",
        "    tokens = self.tokenize_into_tensors(sentences)\r\n",
        "    return {\r\n",
        "        'input_batch': tokens['inp_batch'],\r\n",
        "        'target_batch': tokens['tgt_batch'],\r\n",
        "    }\r\n",
        "  \r\n",
        "  def tokenize_into_tensors(self, sentence):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    word = sentence.split()\r\n",
        "    word = word[:self.max_inp_length]\r\n",
        "    input_tokens = [word_dict[n] for n in word[:-1]]\r\n",
        "    target_tokens = word_dict[word[-1]]\r\n",
        "    input_batch.append(input_tokens)\r\n",
        "    target_batch.append(target_tokens)\r\n",
        "    return {\r\n",
        "        'inp_batch': torch.tensor(input_batch),\r\n",
        "        'tgt_batch': torch.tensor(target_batch),\r\n",
        "    }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AyF8FTy3SX",
        "outputId": "148c27a6-aa5e-4309-f6b7-ce05e98e6a0a"
      },
      "source": [
        "lim = 90 * len(sentences) // 100\r\n",
        "train_sentences = sentences[:lim]\r\n",
        "valid_sentences = sentences[lim:]\r\n",
        "print('Train Samples: ', len(train_sentences))\r\n",
        "print('Valid Samples: ', len(valid_sentences))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples:  9000\n",
            "Valid Samples:  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdYwU6m1kQM"
      },
      "source": [
        "train_dataset = CustomDataset(train_sentences)\r\n",
        "valid_dataset = CustomDataset(valid_sentences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9I-VPH1-XG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}