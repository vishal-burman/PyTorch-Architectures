{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOu6/CENS76cfxx2WlLFdN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_NNLM/test_sample_NNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmW_PBhAFxL"
      },
      "source": [
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\r\n",
        "%cd PyTorch-Architectures/modeling_NNLM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zR4Y1nFtiUG"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts2CGtKqtpXH"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from model import NNLM\r\n",
        "\r\n",
        "from datasets import load_dataset\r\n",
        "dataset = load_dataset('quora')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTpqcq2OtwJn"
      },
      "source": [
        "sentences = []\r\n",
        "for sample in dataset['train']:\r\n",
        "  if len(sentences) == 10000:\r\n",
        "    break\r\n",
        "  sent = sample['questions']['text'][0]\r\n",
        "  if len(sent.split()) >= 4:\r\n",
        "    sentences.append(sent)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkcdgZAyJmV",
        "outputId": "6cc68853-5073-4b42-eae6-e2d2485b3761"
      },
      "source": [
        "word_list = ' '.join(sentences).split()\r\n",
        "word_list = list(set(word_list))\r\n",
        "\r\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\r\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\r\n",
        "n_class = len(word_dict)\r\n",
        "print('Vocabulary Size: ', n_class)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  18198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FUFQ-upt7f4"
      },
      "source": [
        "class CustomDataset(Dataset):\r\n",
        "  def __init__(self, list_sentences, max_inp_length=4):\r\n",
        "    self.list_sentences = list_sentences\r\n",
        "    self.max_inp_length = max_inp_length\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.list_sentences)\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    sentences = self.list_sentences[idx]\r\n",
        "    tokens = self.tokenize_into_tensors(sentences)\r\n",
        "    return {\r\n",
        "        'input_batch': tokens['inp_batch'],\r\n",
        "        'target_batch': tokens['tgt_batch'],\r\n",
        "    }\r\n",
        "  \r\n",
        "  def tokenize_into_tensors(self, sentence):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "    word = sentence.split()\r\n",
        "    word = word[:self.max_inp_length]\r\n",
        "    input_tokens = [word_dict[n] for n in word[:-1]]\r\n",
        "    target_tokens = word_dict[word[-1]]\r\n",
        "    input_batch.append(input_tokens)\r\n",
        "    target_batch.append(target_tokens)\r\n",
        "    return {\r\n",
        "        'inp_batch': torch.tensor(input_batch),\r\n",
        "        'tgt_batch': torch.tensor(target_batch),\r\n",
        "    }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AyF8FTy3SX",
        "outputId": "480275fa-abee-4dbb-d655-2324d85e4228"
      },
      "source": [
        "lim = 90 * len(sentences) // 100\r\n",
        "train_sentences = sentences[:lim]\r\n",
        "valid_sentences = sentences[lim:]\r\n",
        "print('Train Samples: ', len(train_sentences))\r\n",
        "print('Valid Samples: ', len(valid_sentences))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples:  9000\n",
            "Valid Samples:  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdYwU6m1kQM"
      },
      "source": [
        "train_dataset = CustomDataset(train_sentences, max_inp_length=3)\r\n",
        "valid_dataset = CustomDataset(valid_sentences, max_inp_length=3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9I-VPH1-XG"
      },
      "source": [
        "# Hyperparameters\r\n",
        "m = 200\r\n",
        "n_hidden = 100\r\n",
        "n_step = 2\r\n",
        "BATCH_SIZE = 32\r\n",
        "LEARNING_RATE = 0.001\r\n",
        "EPOCHS = 5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNRcpi6EBUE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55fe9a5-77ca-4bc5-bf2a-8e4e95e304be"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = NNLM(n_class=n_class, m=m, n_hidden=n_hidden, n_step=n_step)\r\n",
        "model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NNLM(\n",
              "  (C): Embedding(18198, 200)\n",
              "  (H): Linear(in_features=400, out_features=100, bias=False)\n",
              "  (U): Linear(in_features=100, out_features=18198, bias=False)\n",
              "  (W): Linear(in_features=400, out_features=18198, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxIQ7WaWk9UP"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "# Sanity check DataLoader\r\n",
        "for sample in train_loader:\r\n",
        "  assert sample['input_batch'].squeeze(1).dim() == 2\r\n",
        "  assert sample['target_batch'].dim() == 2\r\n",
        "  break"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBKQNgHflQv5"
      },
      "source": [
        "# Sanity check model outputs\r\n",
        "model.eval()\r\n",
        "with torch.set_grad_enabled(False):\r\n",
        "  outputs = model(sample['input_batch'].squeeze(1))\r\n",
        "  assert outputs.size(1) == n_class"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcfAFgu4nH5V"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRnz-v4pmINb"
      },
      "source": [
        "def compute_loss(model, data_loader, device):\r\n",
        "  pass\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  model.train()\r\n",
        "  for idx, sample in enumerate(train_loader):\r\n",
        "    features = sample['input_batch'].squeeze(1)\r\n",
        "    target = sample['target_batch']\r\n",
        "\r\n",
        "    logits = model(features)\r\n",
        "    loss = F.cross_entropy(logits, targets)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # LOGGING\r\n",
        "    if idx % 50 == 0:\r\n",
        "      print('Batch: %04d/%04d || Epoch: %04d/%04d || Loss: %.2f' % (idx, len(train_loader), epoch+1, EPOCHS, loss.item()))\r\n",
        "  \r\n",
        "  model.eval()\r\n",
        "  with torch.set_grad_enabled(False):\r\n",
        "    valid_loss = compute_loss(model, valid_loader, device)\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}