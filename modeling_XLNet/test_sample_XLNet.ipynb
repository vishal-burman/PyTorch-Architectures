{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample_XLNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPF6LL6Byk8a3k+rhm7LI5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-burman/PyTorch-Architectures/blob/master/modeling_XLNet/test_sample_XLNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlbEcjraA5Lp"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olfQdyvgbKkg"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJKqIVjYbWXe"
      },
      "source": [
        "# ! rm -rf PyTorch-Architectures/\n",
        "! git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n",
        "%cd PyTorch-Architectures/modeling_XLNet/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7ERFegbyys"
      },
      "source": [
        "import time\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLNetTokenizer\n",
        "from model import XLNetClassify\n",
        "from config import XLNetConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMAg-VdHcOJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef509d80-3e72-46b0-8443-50e3f6107fce"
      },
      "source": [
        "dataset = load_dataset('tweets_hate_speech_detection')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/b85ae55489e4a8c3531632a1b4e654546689115add2a15f8bbf0ecbd779ef3ff)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zn7T5aScdDw",
        "outputId": "da8335c4-7eba-44b5-c551-c21cedf86651"
      },
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "for data in dataset['train']:\n",
        "  sentences.append(data['tweet'])\n",
        "  labels.append(data['label'])\n",
        "  \n",
        "assert len(sentences) == len(labels)\n",
        "print('Total Samples: ', len(sentences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples:  31962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9-3jgycdNda"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, tokenizer, list_sentences, labels=None, max_len=16):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.list_sentences = list_sentences\n",
        "    self.labels = labels\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.list_sentences)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    texts = self.list_sentences[idx]\n",
        "    tokens = tokenizer(texts, max_length=self.max_len,\n",
        "                        padding='max_length', truncation=True,\n",
        "                        return_tensors='pt')\n",
        "    if self.labels is not None:\n",
        "      tgt = torch.tensor(self.labels[idx])\n",
        "    else:\n",
        "      tgt = None\n",
        "    \n",
        "    return {\n",
        "        'inp_ids': tokens['input_ids'],\n",
        "        'inp_mask': tokens['attention_mask'],\n",
        "        'tgt_ids' : tgt,\n",
        "    }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oBfAg6d8SY"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7zNi80PexyP"
      },
      "source": [
        "sample_sentences = sentences[:100]\n",
        "sample_labels = labels[:100]\n",
        "sample_dataset = CustomDataset(tokenizer,\n",
        "                               list_sentences=sample_sentences,\n",
        "                               labels=sample_labels,\n",
        "                               max_len=16)\n",
        "sample_loader = DataLoader(dataset=sample_dataset,\n",
        "                           batch_size=2,\n",
        "                           shuffle=False)\n",
        "for sample in sample_loader:\n",
        "  assert sample['inp_ids'].squeeze(1).dim() == 2\n",
        "  assert sample['inp_ids'].squeeze(1).size(1) == sample['inp_mask'].squeeze(1).size(1)\n",
        "  assert sample['tgt_ids'].size(0) == sample['inp_ids'].size(0)\n",
        "  break"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G7inuKGkorX"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "config = XLNetConfig()\n",
        "config.n_layer = 8\n",
        "model = XLNetClassify(config)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmfpgThZ684Y",
        "outputId": "5611de6b-0c81-43c8-c476-8a398bf14786"
      },
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Trainable Parameters: ', params)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable Parameters:  91581442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VVwzQF07Op0",
        "outputId": "86ee41f3-b9ae-427b-e5b8-f057fd780880"
      },
      "source": [
        "split = 90 * len(sentences) // 100\n",
        "train_sentences = sentences[:split]\n",
        "train_labels = labels[:split]\n",
        "assert len(train_sentences) == len(train_labels)\n",
        "\n",
        "valid_sentences = sentences[split:]\n",
        "valid_labels = labels[split:]\n",
        "assert len(valid_sentences) == len(valid_labels)\n",
        "\n",
        "print('Train Samples: ', len(train_sentences))\n",
        "print('Valid Samples: ', len(valid_sentences))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Samples:  28765\n",
            "Valid Samples:  3197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pskQYXtV8E7w"
      },
      "source": [
        "# Space for Hyperparameters for training\n",
        "BATCH_SIZE = 32\n",
        "MAX_INP_LEN = 16\n",
        "LEARNING_RATE = 3e-5\n",
        "EPOCHS = 2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od-k6Hco8Usd"
      },
      "source": [
        "train_dataset = CustomDataset(tokenizer,\n",
        "                              list_sentences=train_sentences,\n",
        "                              labels=train_labels,\n",
        "                              max_len=MAX_INP_LEN)\n",
        "\n",
        "valid_dataset = CustomDataset(tokenizer,\n",
        "                              list_sentences=valid_sentences,\n",
        "                              labels=valid_labels,\n",
        "                              max_len=MAX_INP_LEN)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eKPfc9Z85ok",
        "outputId": "b8b59700-a27d-409a-cb4b-38c973c42bf5"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=False)\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=False)\n",
        "print('Train Loader: ', len(train_loader), \" samples\")\n",
        "print('Valid Loader: ', len(valid_loader), \" samples\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loader:  899  samples\n",
            "Valid Loader:  100  samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtCICGpj9Vjn"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-awnxzCBs3V",
        "outputId": "1529243c-923b-442a-c99f-03237493b8cf"
      },
      "source": [
        "# Sanity check forward pass\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False):\n",
        "  for sample in train_loader:\n",
        "    input_ids = sample['inp_ids'].squeeze(1).to(device)\n",
        "    attention_mask = sample['inp_mask'].squeeze(1).to(device)\n",
        "    target_ids = sample['tgt_ids'].to(device)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                    labels=target_ids)\n",
        "    print(outputs[0].shape, outputs[1])\n",
        "    break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 2]) tensor(1.0111, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKOpvO079he9",
        "outputId": "07ee0d55-5ce3-4fd8-c201-238b393c867d"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "  correct_preds, num_examples = 0, 0\n",
        "  with torch.set_grad_enabled(False):\n",
        "    for sample in data_loader:\n",
        "      input_ids = sample['inp_ids'].squeeze(1).to(device)\n",
        "      attention_mask = sample['inp_mask'].squeeze(1).to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      target_ids = sample['tgt_ids'].to(device)\n",
        "      logits = outputs[0]\n",
        "      prob = F.softmax(logits, dim=-1)\n",
        "      _, preds = torch.max(prob, dim=1)\n",
        "      correct_preds += (preds == target_ids).sum()\n",
        "      num_examples += target_ids.size(0)\n",
        "  return correct_preds.float() / num_examples * 100\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  for idx, sample in enumerate(train_loader):\n",
        "    input_ids = sample['inp_ids'].squeeze(1).to(device)\n",
        "    attention_mask = sample['inp_mask'].squeeze(1).to(device)\n",
        "    target_ids = sample['tgt_ids'].unsqueeze(0).to(device)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, \n",
        "                    labels=target_ids)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.cuda.amp.autocast():\n",
        "      loss = outputs[1]\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # LOGGING\n",
        "    if idx % 200 == 0:\n",
        "      print('Batch: %04d/%04d || Epoch: %04d/%04d || Loss: %.2f' % (idx,\n",
        "                                                                    len(train_loader),\n",
        "                                                                    epoch+1,\n",
        "                                                                    EPOCHS,\n",
        "                                                                    loss.item()))\n",
        "  model.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    train_acc = compute_accuracy(model, train_loader, device)\n",
        "    valid_acc = compute_accuracy(model, valid_loader, device)\n",
        "    print('Train Accuracy: %.2f%% || Valid Accuracy: %.2f%%' % (train_acc,\n",
        "                                                                valid_acc))\n",
        "  epoch_elapsed_time = (time.time() - start_time) / 60\n",
        "  print('Epoch Elapsed Time: %.2f min' % (epoch_elapsed_time))\n",
        "total_training_time = (time.time() - start_time) / 60\n",
        "print('Total Training Time: %.2f min' % (total_training_time))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0000/0899 || Epoch: 0001/0002 || Loss: 0.91\n",
            "Batch: 0200/0899 || Epoch: 0001/0002 || Loss: 0.08\n",
            "Batch: 0400/0899 || Epoch: 0001/0002 || Loss: 0.16\n",
            "Batch: 0600/0899 || Epoch: 0001/0002 || Loss: 0.24\n",
            "Batch: 0800/0899 || Epoch: 0001/0002 || Loss: 0.30\n",
            "Train Accuracy: 95.84% || Valid Accuracy: 94.59%\n",
            "Epoch Elapsed Time: 2.11 min\n",
            "Batch: 0000/0899 || Epoch: 0002/0002 || Loss: 0.14\n",
            "Batch: 0200/0899 || Epoch: 0002/0002 || Loss: 0.04\n",
            "Batch: 0400/0899 || Epoch: 0002/0002 || Loss: 0.10\n",
            "Batch: 0600/0899 || Epoch: 0002/0002 || Loss: 0.14\n",
            "Batch: 0800/0899 || Epoch: 0002/0002 || Loss: 0.34\n",
            "Train Accuracy: 96.63% || Valid Accuracy: 94.49%\n",
            "Epoch Elapsed Time: 4.23 min\n",
            "Total Training Time: 4.23 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPPoJlVbPGz8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}