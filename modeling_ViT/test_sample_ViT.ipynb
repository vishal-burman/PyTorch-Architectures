{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!git clone https://github.com/vishal-burman/PyTorch-Architectures.git\n%cd PyTorch-Architectures/modeling_ViT","execution_count":1,"outputs":[{"output_type":"stream","text":"Cloning into 'PyTorch-Architectures'...\nremote: Enumerating objects: 249, done.\u001b[K\nremote: Counting objects: 100% (249/249), done.\u001b[K\nremote: Compressing objects: 100% (157/157), done.\u001b[K\nremote: Total 680 (delta 140), reused 174 (delta 79), pack-reused 431\u001b[K\nReceiving objects: 100% (680/680), 8.39 MiB | 5.15 MiB/s, done.\nResolving deltas: 100% (416/416), done.\n/kaggle/working/PyTorch-Architectures/modeling_ViT\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import Subset\n\nfrom torchvision import datasets, transforms\n\nfrom model import ViT","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# SETTINGS\n\n# Model Settings\nlearning_rate = 3e-4\nbatch_size = 32\nnum_epochs = 5\n\n# Architecture\nnum_classes = 10\n\n# Other\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ViT(\n    image_size=64,\n    patch_size=8,\n    num_classes=10,\n    dim=1024,\n    depth=6,\n    heads=8,\n    mlp_dim=2048,\n    dropout=0.1,\n    emb_dropout=0.1,\n).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters = \", total_params)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":6,"outputs":[{"output_type":"stream","text":"Total Parameters =  54833320\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset\ntrain_indices = torch.arange(0, 48000)\nvalid_indices = torch.arange(48000, 50000)\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((70, 70)),\n    transforms.RandomCrop((64, 64)),\n    transforms.ToTensor(),\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((70, 70)),\n    transforms.RandomCrop((64, 64)),\n    transforms.ToTensor(),\n])\n\ntrain_and_valid = datasets.CIFAR10(root=\"data\", train=True, transform=train_transform, download=True)\n\ntrain_dataset = Subset(train_and_valid, train_indices)\nvalid_dataset = Subset(train_and_valid, valid_indices)\ntest_dataset = datasets.CIFAR10(root=\"data\", train=False, transform=test_transform, download=False)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","execution_count":9,"outputs":[{"output_type":"stream","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f997debce324a0e82240ef44af4da5f"}},"metadata":{}},{"output_type":"stream","text":"Extracting data/cifar-10-python.tar.gz to data\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the train_loader\nfor images, labels in train_loader:\n    print(\"Image Dimensions: \", images.shape)\n    print(\"Label Dimensions: \", labels.shape)\n    break\nprint(\"Length of Train DataLoader: \", len(train_loader))\nprint(\"Length of Valid DataLoader: \", len(valid_loader))","execution_count":10,"outputs":[{"output_type":"stream","text":"Image Dimensions:  torch.Size([32, 3, 64, 64])\nLabel Dimensions:  torch.Size([32])\nLength of Train DataLoader:  1500\nLength of Valid DataLoader:  63\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_accuracy(model, data_loader, device):\n    pass\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (features, labels) in train_loader:\n        \n        features = features.to(device)\n        labels = labels.to(device)\n        logits = model(features)\n        cost = F.cross_entropy(logits, labels)\n        \n        optimizer.zero_grad()\n        cost.backward()\n        optimizer.step()\n        \n        # LOGGING\n        if batch_idx % 200:\n            print(\"Batch: %04d/%04d || Epoch: %04d/%04d\" % (batch_idx, len(train_loader), epoch+1, num_epochs))\n    model.eval()\n    with torch.set_grad_enabled(False):\n        train_acc = compute_accuracy(model, train_loader, device)\n        valid_acc = compute_accuracy(model, valid_loader, device)\n        print(\"Train Accuracy: \", train_acc)\n        print(\"Valid Accuracy: \", valid_acc)\n    elapsed_time = (time.time() - start_time) / 60\n    print(\"Epoch Elapsed Time: \", elapsed_time)\nelapsed_time = (time.time() - start_time) / 60\nprint(\"Total Training Time: \", elapsed_time)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}