{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"# ! rm -rf PyTorch-Architectures/\n! git clone https://github.com/vishal-burman/PyTorch-Architectures.git","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%cd PyTorch-Architectures/modeling_xlm/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"! pip install datasets\n! pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from os import path\nimport time\nimport csv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import XLMTokenizer\nfrom config_xlm import XLMConfig\nfrom model import XLMWithLMHeadModel\nfrom datasets import load_dataset\nfrom transformers import top_k_top_p_filtering\n\ndataset = load_dataset('cnn_dailymail', '3.0.0')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Defining the model and tokenizer\nconfig = XLMConfig()\nconfig.n_layers = 6\ntokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\nmodel = XLMWithLMHeadModel(config).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters = \", total_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, texts, tokenizer, seq_length=64):\n        self.tokenizer = tokenizer\n        self.texts = texts\n        self.seq_length = seq_length\n        self.train_list = []\n        self.build()\n\n    def __len__(self):\n        return len(self.train_list)\n\n    def __getitem__(self, index):\n        ids = self.train_list[index]['input_ids']\n        mask = self.train_list[index]['attention_mask']\n\n        return{\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(ids, dtype=torch.long),\n                }\n\n    def build(self):\n        for t in self.texts:\n            self.train_list.append(tokenizer(t, max_length=self.seq_length, pad_to_max_length=True, truncation=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! rm -rf *.pt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if path.exists('train_dataset.pt') and path.exists('valid_dataset.pt'):\n  print(\"Datasets is already present!\")\n  train_dataset = torch.load('train_dataset.pt')\n  valid_dataset = torch.load('valid_dataset.pt')\n\nelse:\n  texts_train = dataset['train']['article'][:10000]\n\n  texts_valid = dataset['validation']['article'][:1000]\n\n  start_time = time.time()\n  train_dataset = CustomDataset(texts_train, tokenizer, seq_length=256)\n  valid_dataset = CustomDataset(texts_valid, tokenizer)\n  torch.save(train_dataset, 'train_dataset.pt')\n  torch.save(valid_dataset, 'valid_dataset.pt')\n  print(\"Dataset Conversion Done!!\")\n  print(\"Time Taken = \", (time.time() - start_time)/60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nLR = 2e-04\nEPOCHS = 15\n\ntrain_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=BATCH_SIZE)\nvalid_loader = DataLoader(dataset=valid_dataset, shuffle=False, batch_size=BATCH_SIZE)\nprint(\"Length of Train DataLoader: \", len(train_loader))\nprint(\"Length of Valid DataLoader: \", len(valid_loader))\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_loss(model, data_loader, device):\n  total_loss = 0\n  model.eval()\n  with torch.set_grad_enabled(False):\n    for sample in data_loader:\n      ids = sample['ids'].to(device)\n      mask = sample['mask'].to(device)\n      labels = sample['ids'].to(device)\n\n      outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n      loss = outputs[0]\n      total_loss += loss.item()\n  return (total_loss / len(data_loader))\n\nstart_time = time.time()\nfor epoch in range(EPOCHS):\n  model.train()\n  for idx, sample in enumerate(train_loader):\n    ids = sample['ids'].to(device)\n    mask = sample['mask'].to(device)\n    labels = sample['ids'].to(device)\n    \n    optimizer.zero_grad()\n    \n    logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n    loss = logits[0]\n\n    # LOGGING\n    if idx % 100 == 0:\n      print(\"Batch: %04d/%04d || Epoch: %03d/%03d\" % (idx, len(train_loader), epoch+1, EPOCHS))\n\n    loss.backward()\n    optimizer.step()\n\n  model.eval()\n  with torch.set_grad_enabled(False):\n    train_loss = compute_loss(model, train_loader, device)\n    valid_loss = compute_loss(model, valid_loader, device)\n    print(\"Train Loss: %.3f\" % (train_loss))\n    print(\"Valid Loss: %.3f\" % (valid_loss))\n  elapsed_epoch_time = (time.time() - start_time) / 60\n  print(\"Epoch Elapsed Time: %d mins\" % (elapsed_epoch_time))\ntotal_training_time = (time.time() - start_time) / 60\nprint(\"Total Training Time: %d mins\" % (elapsed_epoch_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the language model\n# text = \"he is\"\ntext = \"What is\"\ninput_ids = tokenizer.encode(text, return_tensors='pt')\nmask = torch.ones(input_ids.shape)\nmask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\nmodel.eval()\nwith torch.set_grad_enabled(False):\n\n  outputs = model(input_ids=input_ids.to(device), attention_mask=mask.to(device))\n  logits = outputs[0]\n  logits = logits[:, -1, :]\n  filtered_next_token_logits = top_k_top_p_filtering(logits, top_k=5, top_p=1.0)\n  probs = F.softmax(filtered_next_token_logits, dim=-1)\n  next_token = torch.multinomial(probs, num_samples=1)\n  generated = torch.cat([input_ids.to(device), next_token], dim=-1)\n\n  resulting_string = tokenizer.decode(generated.tolist()[0], skip_special_tokens=True)\n  print(resulting_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Notebook --> For the casual modeling objective it still needs a lot of training","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}