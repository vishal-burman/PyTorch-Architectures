{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# ! rm -rf PyTorch-Architectures/\n! git clone https://github.com/vishal-burman/PyTorch-Architectures.git","execution_count":1,"outputs":[{"output_type":"stream","text":"Cloning into 'PyTorch-Architectures'...\nremote: Enumerating objects: 426, done.\u001b[K\nremote: Counting objects: 100% (426/426), done.\u001b[K\nremote: Compressing objects: 100% (263/263), done.\u001b[K\nremote: Total 857 (delta 233), reused 312 (delta 138), pack-reused 431\u001b[K\nReceiving objects: 100% (857/857), 8.43 MiB | 12.51 MiB/s, done.\nResolving deltas: 100% (509/509), done.\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%cd PyTorch-Architectures/modeling_xlm/","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/working/PyTorch-Architectures/modeling_xlm\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install datasets\n! pip install transformers","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting datasets\n  Downloading datasets-1.1.2-py3-none-any.whl (147 kB)\n\u001b[K     |████████████████████████████████| 147 kB 1.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.2)\nCollecting pyarrow>=0.17.1\n  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n\u001b[K     |████████████████████████████████| 17.7 MB 23.5 MB/s eta 0:00:01   |███████████████▌                | 8.6 MB 4.5 MB/s eta 0:00:03     |█████████████████████████▍      | 14.0 MB 4.5 MB/s eta 0:00:01     |███████████████████████████▋    | 15.3 MB 4.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.18.5)\nRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.45.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.10)\nCollecting xxhash\n  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 45.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.10)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\nInstalling collected packages: pyarrow, xxhash, datasets\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 0.16.0\n    Uninstalling pyarrow-0.16.0:\n      Successfully uninstalled pyarrow-0.16.0\nSuccessfully installed datasets-1.1.2 pyarrow-2.0.0 xxhash-2.0.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.0.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: tokenizers==0.8.1.rc1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import XLMTokenizer\nfrom config_xlm import XLMConfig\nfrom model import XLMWithLMHeadModel\nfrom datasets import load_dataset\nfrom transformers import top_k_top_p_filtering\n\ndataset = load_dataset('cnn_dailymail', '3.0.0')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":4,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3528.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f1108ffff1418d831348c72b0a90ed"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1610.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c16c17d61394ff7a77fbe3a31898869"}},"metadata":{}},{"output_type":"stream","text":"\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Downloading', max=1.0, style=ProgressSt…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a7a9a2d95e4efdbc93fe67acf72f26"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Downloading', max=1.0, style=ProgressSt…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16607d5cdd04458b3715130a20a60f0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=572061.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee5a2ed05db449c8e6550dceb79d3a5"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=12259516.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5cadd7dfdbb4b5aa4955faf453b1103"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=660943.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d767261c0ede4b608594d8a82acb30c8"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Dataset cnn_dailymail downloaded and prepared to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602. Subsequent calls will reuse this data.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model and tokenizer\nconfig = XLMConfig()\nconfig.n_layers = 6\ntokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\ndef _init_weights(module):\n    \"\"\" Initialize the weights. \"\"\"\n    if isinstance(module, nn.Embedding):\n        nn.init.normal_(module.weight, mean=0, std=2048 ** -0.5)\n    if isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0, std=0.02)\n        if hasattr(module, \"bias\") and module.bias is not None:\n            nn.init.constant_(module.bias, 0.0)\n    if isinstance(module, nn.LayerNorm):\n        module.bias.data.zero_()\n        module.weight.data.fill_(1.0)\nmodel = XLMWithLMHeadModel(config)\nmodel.apply(_init_weights)\nmodel = model.to(device)","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=646181.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a10c77f71c41b4b1f26763a19eac8a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=486639.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9691cc6fda444a43b6bf30c84023ff26"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Parameters = \", total_params)","execution_count":6,"outputs":[{"output_type":"stream","text":"Total Parameters =  50076097\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, texts, tokenizer, seq_length=512):\n        self.tokenizer = tokenizer\n        self.texts = texts\n        self.seq_length = seq_length\n        self.train_list = []\n        self.build()\n\n    def __len__(self):\n        return len(self.train_list)\n\n    def __getitem__(self, index):\n        ids = self.train_list[index]['input_ids']\n        mask = self.train_list[index]['attention_mask']\n\n        return{\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(ids, dtype=torch.long),\n                }\n\n    def build(self):\n        for t in self.texts:\n            self.train_list.append(tokenizer(t, max_length=self.seq_length, pad_to_max_length=True, truncation=True))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts_train = dataset['train']['article'][:10000]\n\ntexts_valid = dataset['validation']['article'][:1000]\n\nstart_time = time.time()\ntrain_dataset = CustomDataset(texts_train, tokenizer, seq_length=512)\nvalid_dataset = CustomDataset(texts_valid, tokenizer)\ntorch.save(train_dataset, 'train_dataset.pt')\ntorch.save(valid_dataset, 'valid_dataset.pt')\nprint(\"Dataset Conversion Done!!\")\nprint(\"Time Taken = \", (time.time() - start_time)/60)","execution_count":8,"outputs":[{"output_type":"stream","text":"Dataset Conversion Done!!\nTime Taken =  5.156346150239309\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\nLR = 3e-05\nEPOCHS = 15\n\ntrain_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=BATCH_SIZE)\nvalid_loader = DataLoader(dataset=valid_dataset, shuffle=False, batch_size=BATCH_SIZE)\nprint(\"Length of Train DataLoader: \", len(train_loader))\nprint(\"Length of Valid DataLoader: \", len(valid_loader))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)","execution_count":9,"outputs":[{"output_type":"stream","text":"Length of Train DataLoader:  1250\nLength of Valid DataLoader:  125\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_loss(model, data_loader, device):\n  total_loss = 0\n  model.eval()\n  with torch.set_grad_enabled(False):\n    for sample in data_loader:\n      ids = sample['ids'].to(device)\n      mask = sample['mask'].to(device)\n      labels = sample['ids'].to(device)\n\n      outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n      loss = outputs[0]\n      total_loss += loss.item()\n  return (total_loss / len(data_loader))\n\nstart_time = time.time()\nfor epoch in range(EPOCHS):\n  model.train()\n  for idx, sample in enumerate(train_loader):\n    ids = sample['ids'].to(device)\n    mask = sample['mask'].to(device)\n    labels = sample['ids'].to(device)\n    \n    optimizer.zero_grad()\n    \n    logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n    loss = logits[0]\n\n    # LOGGING\n    if idx % 100 == 0:\n      print(\"Batch: %04d/%04d || Epoch: %03d/%03d\" % (idx, len(train_loader), epoch+1, EPOCHS))\n\n    loss.backward()\n    optimizer.step()\n\n  model.eval()\n  with torch.set_grad_enabled(False):\n    train_loss = compute_loss(model, train_loader, device)\n    valid_loss = compute_loss(model, valid_loader, device)\n    print(\"Train Loss: %.3f\" % (train_loss))\n    print(\"Valid Loss: %.3f\" % (valid_loss))\n  elapsed_epoch_time = (time.time() - start_time) / 60\n  print(\"Epoch Elapsed Time: %d mins\" % (elapsed_epoch_time))\ntotal_training_time = (time.time() - start_time) / 60\nprint(\"Total Training Time: %d mins\" % (elapsed_epoch_time))","execution_count":10,"outputs":[{"output_type":"stream","text":"Batch: 0000/1250 || Epoch: 001/015\nBatch: 0100/1250 || Epoch: 001/015\nBatch: 0200/1250 || Epoch: 001/015\nBatch: 0300/1250 || Epoch: 001/015\nBatch: 0400/1250 || Epoch: 001/015\nBatch: 0500/1250 || Epoch: 001/015\nBatch: 0600/1250 || Epoch: 001/015\nBatch: 0700/1250 || Epoch: 001/015\nBatch: 0800/1250 || Epoch: 001/015\nBatch: 0900/1250 || Epoch: 001/015\nBatch: 1000/1250 || Epoch: 001/015\nBatch: 1100/1250 || Epoch: 001/015\nBatch: 1200/1250 || Epoch: 001/015\nTrain Loss: 480.469\nValid Loss: 494.384\nEpoch Elapsed Time: 5 mins\nBatch: 0000/1250 || Epoch: 002/015\nBatch: 0100/1250 || Epoch: 002/015\nBatch: 0200/1250 || Epoch: 002/015\nBatch: 0300/1250 || Epoch: 002/015\nBatch: 0400/1250 || Epoch: 002/015\nBatch: 0500/1250 || Epoch: 002/015\nBatch: 0600/1250 || Epoch: 002/015\nBatch: 0700/1250 || Epoch: 002/015\nBatch: 0800/1250 || Epoch: 002/015\nBatch: 0900/1250 || Epoch: 002/015\nBatch: 1000/1250 || Epoch: 002/015\nBatch: 1100/1250 || Epoch: 002/015\nBatch: 1200/1250 || Epoch: 002/015\nTrain Loss: 299.359\nValid Loss: 311.725\nEpoch Elapsed Time: 11 mins\nBatch: 0000/1250 || Epoch: 003/015\nBatch: 0100/1250 || Epoch: 003/015\nBatch: 0200/1250 || Epoch: 003/015\nBatch: 0300/1250 || Epoch: 003/015\nBatch: 0400/1250 || Epoch: 003/015\nBatch: 0500/1250 || Epoch: 003/015\nBatch: 0600/1250 || Epoch: 003/015\nBatch: 0700/1250 || Epoch: 003/015\nBatch: 0800/1250 || Epoch: 003/015\nBatch: 0900/1250 || Epoch: 003/015\nBatch: 1000/1250 || Epoch: 003/015\nBatch: 1100/1250 || Epoch: 003/015\nBatch: 1200/1250 || Epoch: 003/015\nTrain Loss: 177.622\nValid Loss: 191.378\nEpoch Elapsed Time: 17 mins\nBatch: 0000/1250 || Epoch: 004/015\nBatch: 0100/1250 || Epoch: 004/015\nBatch: 0200/1250 || Epoch: 004/015\nBatch: 0300/1250 || Epoch: 004/015\nBatch: 0400/1250 || Epoch: 004/015\nBatch: 0500/1250 || Epoch: 004/015\nBatch: 0600/1250 || Epoch: 004/015\nBatch: 0700/1250 || Epoch: 004/015\nBatch: 0800/1250 || Epoch: 004/015\nBatch: 0900/1250 || Epoch: 004/015\nBatch: 1000/1250 || Epoch: 004/015\nBatch: 1100/1250 || Epoch: 004/015\nBatch: 1200/1250 || Epoch: 004/015\nTrain Loss: 95.731\nValid Loss: 110.142\nEpoch Elapsed Time: 23 mins\nBatch: 0000/1250 || Epoch: 005/015\nBatch: 0100/1250 || Epoch: 005/015\nBatch: 0200/1250 || Epoch: 005/015\nBatch: 0300/1250 || Epoch: 005/015\nBatch: 0400/1250 || Epoch: 005/015\nBatch: 0500/1250 || Epoch: 005/015\nBatch: 0600/1250 || Epoch: 005/015\nBatch: 0700/1250 || Epoch: 005/015\nBatch: 0800/1250 || Epoch: 005/015\nBatch: 0900/1250 || Epoch: 005/015\nBatch: 1000/1250 || Epoch: 005/015\nBatch: 1100/1250 || Epoch: 005/015\nBatch: 1200/1250 || Epoch: 005/015\nTrain Loss: 54.211\nValid Loss: 66.253\nEpoch Elapsed Time: 29 mins\nBatch: 0000/1250 || Epoch: 006/015\nBatch: 0100/1250 || Epoch: 006/015\nBatch: 0200/1250 || Epoch: 006/015\nBatch: 0300/1250 || Epoch: 006/015\nBatch: 0400/1250 || Epoch: 006/015\nBatch: 0500/1250 || Epoch: 006/015\nBatch: 0600/1250 || Epoch: 006/015\nBatch: 0700/1250 || Epoch: 006/015\nBatch: 0800/1250 || Epoch: 006/015\nBatch: 0900/1250 || Epoch: 006/015\nBatch: 1000/1250 || Epoch: 006/015\nBatch: 1100/1250 || Epoch: 006/015\nBatch: 1200/1250 || Epoch: 006/015\nTrain Loss: 35.272\nValid Loss: 45.452\nEpoch Elapsed Time: 34 mins\nBatch: 0000/1250 || Epoch: 007/015\nBatch: 0100/1250 || Epoch: 007/015\nBatch: 0200/1250 || Epoch: 007/015\nBatch: 0300/1250 || Epoch: 007/015\nBatch: 0400/1250 || Epoch: 007/015\nBatch: 0500/1250 || Epoch: 007/015\nBatch: 0600/1250 || Epoch: 007/015\nBatch: 0700/1250 || Epoch: 007/015\nBatch: 0800/1250 || Epoch: 007/015\nBatch: 0900/1250 || Epoch: 007/015\nBatch: 1000/1250 || Epoch: 007/015\nBatch: 1100/1250 || Epoch: 007/015\nBatch: 1200/1250 || Epoch: 007/015\nTrain Loss: 26.757\nValid Loss: 36.150\nEpoch Elapsed Time: 40 mins\nBatch: 0000/1250 || Epoch: 008/015\nBatch: 0100/1250 || Epoch: 008/015\nBatch: 0200/1250 || Epoch: 008/015\nBatch: 0300/1250 || Epoch: 008/015\nBatch: 0400/1250 || Epoch: 008/015\nBatch: 0500/1250 || Epoch: 008/015\nBatch: 0600/1250 || Epoch: 008/015\nBatch: 0700/1250 || Epoch: 008/015\nBatch: 0800/1250 || Epoch: 008/015\nBatch: 0900/1250 || Epoch: 008/015\nBatch: 1000/1250 || Epoch: 008/015\nBatch: 1100/1250 || Epoch: 008/015\nBatch: 1200/1250 || Epoch: 008/015\nTrain Loss: 20.760\nValid Loss: 29.207\nEpoch Elapsed Time: 46 mins\nBatch: 0000/1250 || Epoch: 009/015\nBatch: 0100/1250 || Epoch: 009/015\nBatch: 0200/1250 || Epoch: 009/015\nBatch: 0300/1250 || Epoch: 009/015\nBatch: 0400/1250 || Epoch: 009/015\nBatch: 0500/1250 || Epoch: 009/015\nBatch: 0600/1250 || Epoch: 009/015\nBatch: 0700/1250 || Epoch: 009/015\nBatch: 0800/1250 || Epoch: 009/015\nBatch: 0900/1250 || Epoch: 009/015\nBatch: 1000/1250 || Epoch: 009/015\nBatch: 1100/1250 || Epoch: 009/015\nBatch: 1200/1250 || Epoch: 009/015\nTrain Loss: 17.331\nValid Loss: 25.615\nEpoch Elapsed Time: 52 mins\nBatch: 0000/1250 || Epoch: 010/015\nBatch: 0100/1250 || Epoch: 010/015\nBatch: 0200/1250 || Epoch: 010/015\nBatch: 0300/1250 || Epoch: 010/015\nBatch: 0400/1250 || Epoch: 010/015\nBatch: 0500/1250 || Epoch: 010/015\nBatch: 0600/1250 || Epoch: 010/015\nBatch: 0700/1250 || Epoch: 010/015\nBatch: 0800/1250 || Epoch: 010/015\nBatch: 0900/1250 || Epoch: 010/015\nBatch: 1000/1250 || Epoch: 010/015\nBatch: 1100/1250 || Epoch: 010/015\nBatch: 1200/1250 || Epoch: 010/015\nTrain Loss: 14.980\nValid Loss: 23.298\nEpoch Elapsed Time: 57 mins\nBatch: 0000/1250 || Epoch: 011/015\nBatch: 0100/1250 || Epoch: 011/015\nBatch: 0200/1250 || Epoch: 011/015\nBatch: 0300/1250 || Epoch: 011/015\nBatch: 0400/1250 || Epoch: 011/015\nBatch: 0500/1250 || Epoch: 011/015\nBatch: 0600/1250 || Epoch: 011/015\nBatch: 0700/1250 || Epoch: 011/015\nBatch: 0800/1250 || Epoch: 011/015\nBatch: 0900/1250 || Epoch: 011/015\nBatch: 1000/1250 || Epoch: 011/015\nBatch: 1100/1250 || Epoch: 011/015\nBatch: 1200/1250 || Epoch: 011/015\nTrain Loss: 13.564\nValid Loss: 22.147\nEpoch Elapsed Time: 63 mins\nBatch: 0000/1250 || Epoch: 012/015\nBatch: 0100/1250 || Epoch: 012/015\nBatch: 0200/1250 || Epoch: 012/015\nBatch: 0300/1250 || Epoch: 012/015\nBatch: 0400/1250 || Epoch: 012/015\nBatch: 0500/1250 || Epoch: 012/015\nBatch: 0600/1250 || Epoch: 012/015\nBatch: 0700/1250 || Epoch: 012/015\nBatch: 0800/1250 || Epoch: 012/015\nBatch: 0900/1250 || Epoch: 012/015\nBatch: 1000/1250 || Epoch: 012/015\nBatch: 1100/1250 || Epoch: 012/015\nBatch: 1200/1250 || Epoch: 012/015\nTrain Loss: 12.402\nValid Loss: 21.769\nEpoch Elapsed Time: 69 mins\nBatch: 0000/1250 || Epoch: 013/015\nBatch: 0100/1250 || Epoch: 013/015\nBatch: 0200/1250 || Epoch: 013/015\nBatch: 0300/1250 || Epoch: 013/015\nBatch: 0400/1250 || Epoch: 013/015\nBatch: 0500/1250 || Epoch: 013/015\nBatch: 0600/1250 || Epoch: 013/015\nBatch: 0700/1250 || Epoch: 013/015\nBatch: 0800/1250 || Epoch: 013/015\nBatch: 0900/1250 || Epoch: 013/015\nBatch: 1000/1250 || Epoch: 013/015\nBatch: 1100/1250 || Epoch: 013/015\nBatch: 1200/1250 || Epoch: 013/015\nTrain Loss: 11.732\nValid Loss: 21.934\nEpoch Elapsed Time: 75 mins\nBatch: 0000/1250 || Epoch: 014/015\nBatch: 0100/1250 || Epoch: 014/015\nBatch: 0200/1250 || Epoch: 014/015\nBatch: 0300/1250 || Epoch: 014/015\nBatch: 0400/1250 || Epoch: 014/015\nBatch: 0500/1250 || Epoch: 014/015\nBatch: 0600/1250 || Epoch: 014/015\nBatch: 0700/1250 || Epoch: 014/015\nBatch: 0800/1250 || Epoch: 014/015\nBatch: 0900/1250 || Epoch: 014/015\nBatch: 1000/1250 || Epoch: 014/015\nBatch: 1100/1250 || Epoch: 014/015\nBatch: 1200/1250 || Epoch: 014/015\nTrain Loss: 11.074\nValid Loss: 22.456\nEpoch Elapsed Time: 81 mins\nBatch: 0000/1250 || Epoch: 015/015\nBatch: 0100/1250 || Epoch: 015/015\nBatch: 0200/1250 || Epoch: 015/015\nBatch: 0300/1250 || Epoch: 015/015\nBatch: 0400/1250 || Epoch: 015/015\nBatch: 0500/1250 || Epoch: 015/015\nBatch: 0600/1250 || Epoch: 015/015\nBatch: 0700/1250 || Epoch: 015/015\nBatch: 0800/1250 || Epoch: 015/015\nBatch: 0900/1250 || Epoch: 015/015\nBatch: 1000/1250 || Epoch: 015/015\nBatch: 1100/1250 || Epoch: 015/015\nBatch: 1200/1250 || Epoch: 015/015\nTrain Loss: 10.436\nValid Loss: 23.094\nEpoch Elapsed Time: 87 mins\nTotal Training Time: 87 mins\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the language model\n# text = \"he is\"\ntext = \"how\"\ninput_ids = tokenizer.encode(text, return_tensors='pt')\nmask = torch.ones(input_ids.shape)\nmask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\nmodel.eval()\nwith torch.set_grad_enabled(False):\n\n  outputs = model(input_ids=input_ids.to(device), attention_mask=mask.to(device))\n  logits = outputs[0]\n  logits = logits[:, -1, :]\n  filtered_next_token_logits = top_k_top_p_filtering(logits, top_k=5, top_p=1.0)\n  probs = F.softmax(filtered_next_token_logits, dim=-1)\n  next_token = torch.multinomial(probs, num_samples=1)\n  generated = torch.cat([input_ids.to(device), next_token], dim=-1)\n\n  resulting_string = tokenizer.decode(generated.tolist()[0], skip_special_tokens=True)\n  print(resulting_string)","execution_count":18,"outputs":[{"output_type":"stream","text":"how but\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Notebook --> For the casual modeling objective it still needs a lot of training","execution_count":12,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}