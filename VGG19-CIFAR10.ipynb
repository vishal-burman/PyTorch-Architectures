{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################\n# Settings\n########################\n\n# Device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.001\nnum_epochs = 20\nbatch_size = 128\n\n# Architecture\nnum_features = 784\nnum_classes = 10","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################\n# CIFAR-10\n#####################\n\ntrain_dataset = datasets.CIFAR10(root=\"data\", \n                                 train=True, \n                                 transform=transforms.ToTensor(), \n                                 download=True)\n\ntest_dataset = datasets.CIFAR10(root=\"data\", \n                                train=False, \n                                transform=transforms.ToTensor())\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          shuffle=True, \n                          batch_size=batch_size)\n\ntest_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)\n\n# Checking the dataset\nfor images, labels in train_loader:\n    print(\"Image Dimensions: \", images.shape)\n    print(\"Label Dimensions: \", labels.shape)\n    break","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c62ff5d1e0499286f4039b1ae64ad7"}},"metadata":{}},{"output_type":"stream","text":"Extracting data/cifar-10-python.tar.gz to data\nImage Dimensions:  torch.Size([128, 3, 32, 32])\nLabel Dimensions:  torch.Size([128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################\n# Model\n######################\n\nclass VGG19(nn.Module):\n    \n    def __init__(self, num_features, num_classes):\n        super(VGG19, self).__init__()\n        \n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, \n                      out_channels=64, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, \n                      out_channels=64, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, \n                      out_channels=128,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, \n                      out_channels=128, \n                      kernel_size=(3, 3), \n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2),\n                         stride=(2, 2))\n        )\n        \n        self.block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128,\n                      out_channels=256,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256,\n                      out_channels=256,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256,\n                      out_channels=256,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256,\n                      out_channels=256,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2),\n                         stride=(2, 2))\n        )\n        \n        self.block_4 = nn.Sequential(\n            nn.Conv2d(in_channels=256,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2),\n                         stride=(2, 2))\n        )\n        \n        self.block_5 = nn.Sequential(\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512,\n                      out_channels=512,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.detach().normal_(0, 0.05)\n                if m.bias is not None:\n                    m.bias.detach().zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.detach().normal_(0, 0.05)\n                m.bias.detach().zero_()\n    \n    def forward(self, x):\n        x = self.block_1(x)\n        x = self.block_2(x)\n        x = self.block_3(x)\n        x = self.block_4(x)\n        x = self.block_5(x)\n        logits = self.classifier(x.view(-1, 512))\n        probas = F.softmax(logits, dim=1)\n\n        return logits, probas\n\ntorch.manual_seed(random_seed)\nmodel = VGG19(num_features=num_features, num_classes=num_classes)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":4,"outputs":[{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################\n# Training\n########################\n\ndef compute_accuracy(model, dataloader, device):\n    model.eval()\n    correct_pred, num_examples = 0, 0\n    for i, (features, targets) in enumerate(dataloader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n        \n        logits, probas = model(features)\n        _, predicted_labels = torch.max(probas, 1)\n        num_examples += targets.size(0)\n        correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float()/num_examples * 100\n\ndef compute_epoch_loss(model, dataloader, device):\n    model.eval()\n    curr_loss, num_examples = 0., 0\n    with torch.no_grad():\n        for features, targets in dataloader:\n            features = features.to(device)\n            targets = targets.to(device)\n            logits, probas = model(features)\n            loss = F.cross_entropy(logits, targets, reduction=\"sum\")\n            num_examples += targets.size(0)\n            curr_loss += loss\n        curr_loss = curr_loss/num_examples\n    return curr_loss\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for batch_idx, (features, targets) in enumerate(train_loader):\n        features = features.to(device)\n        targets = targets.to(device)\n        \n        # Forward and Back prop\n        logits, probas = model(features)\n        cost = F.cross_entropy(logits, targets)\n        optimizer.zero_grad()\n        \n        cost.backward()\n        \n        # Update model parameters\n        optimizer.step()\n        \n        # LOGGING\n        if not batch_idx % 50:\n            print(\"Epoch: %03d/%03d || Batch: %04d/%04d || Cost: %.4f\" % (epoch+1, num_epochs, batch_idx, len(train_loader), cost))\n        \n    model.eval()\n    with torch.set_grad_enabled(False):\n        print(\"Epoch: %03d/%03d || Train: %.3f%% | Loss: %.3f\" % (epoch+1, num_epochs, compute_accuracy(model, train_loader, device), compute_epoch_loss(model, train_loader, device)))\n    \n    print(\"Time elapsed: %.2f min\" % ((time.time()-start_time)/60))\n\nprint(\"Total Training Time: %.2f min\" % ((time.time()-start_time)/60))","execution_count":5,"outputs":[{"output_type":"stream","text":"Epoch: 001/020 || Batch: 0000/0391 || Cost: 1061.4156\nEpoch: 001/020 || Batch: 0050/0391 || Cost: 2.3025\nEpoch: 001/020 || Batch: 0100/0391 || Cost: 1.9715\nEpoch: 001/020 || Batch: 0150/0391 || Cost: 1.9078\nEpoch: 001/020 || Batch: 0200/0391 || Cost: 1.8211\nEpoch: 001/020 || Batch: 0250/0391 || Cost: 1.8317\nEpoch: 001/020 || Batch: 0300/0391 || Cost: 1.8077\nEpoch: 001/020 || Batch: 0350/0391 || Cost: 1.6687\nEpoch: 001/020 || Train: 33.396% | Loss: 1.695\nTime elapsed: 0.63 min\nEpoch: 002/020 || Batch: 0000/0391 || Cost: 1.7488\nEpoch: 002/020 || Batch: 0050/0391 || Cost: 1.6875\nEpoch: 002/020 || Batch: 0100/0391 || Cost: 1.5090\nEpoch: 002/020 || Batch: 0150/0391 || Cost: 1.5868\nEpoch: 002/020 || Batch: 0200/0391 || Cost: 1.4560\nEpoch: 002/020 || Batch: 0250/0391 || Cost: 1.4756\nEpoch: 002/020 || Batch: 0300/0391 || Cost: 1.6276\nEpoch: 002/020 || Batch: 0350/0391 || Cost: 1.4148\nEpoch: 002/020 || Train: 46.072% | Loss: 1.450\nTime elapsed: 1.26 min\nEpoch: 003/020 || Batch: 0000/0391 || Cost: 1.4828\nEpoch: 003/020 || Batch: 0050/0391 || Cost: 1.3040\nEpoch: 003/020 || Batch: 0100/0391 || Cost: 1.4289\nEpoch: 003/020 || Batch: 0150/0391 || Cost: 1.4223\nEpoch: 003/020 || Batch: 0200/0391 || Cost: 1.3909\nEpoch: 003/020 || Batch: 0250/0391 || Cost: 1.3942\nEpoch: 003/020 || Batch: 0300/0391 || Cost: 1.5325\nEpoch: 003/020 || Batch: 0350/0391 || Cost: 1.3598\nEpoch: 003/020 || Train: 49.796% | Loss: 1.366\nTime elapsed: 1.88 min\nEpoch: 004/020 || Batch: 0000/0391 || Cost: 1.3931\nEpoch: 004/020 || Batch: 0050/0391 || Cost: 1.3303\nEpoch: 004/020 || Batch: 0100/0391 || Cost: 1.4546\nEpoch: 004/020 || Batch: 0150/0391 || Cost: 1.1036\nEpoch: 004/020 || Batch: 0200/0391 || Cost: 1.1209\nEpoch: 004/020 || Batch: 0250/0391 || Cost: 1.2728\nEpoch: 004/020 || Batch: 0300/0391 || Cost: 1.2358\nEpoch: 004/020 || Batch: 0350/0391 || Cost: 1.2212\nEpoch: 004/020 || Train: 57.810% | Loss: 1.144\nTime elapsed: 2.50 min\nEpoch: 005/020 || Batch: 0000/0391 || Cost: 1.1390\nEpoch: 005/020 || Batch: 0050/0391 || Cost: 1.0095\nEpoch: 005/020 || Batch: 0100/0391 || Cost: 0.9510\nEpoch: 005/020 || Batch: 0150/0391 || Cost: 1.1998\nEpoch: 005/020 || Batch: 0200/0391 || Cost: 0.9227\nEpoch: 005/020 || Batch: 0250/0391 || Cost: 1.0179\nEpoch: 005/020 || Batch: 0300/0391 || Cost: 1.1926\nEpoch: 005/020 || Batch: 0350/0391 || Cost: 1.1902\nEpoch: 005/020 || Train: 63.218% | Loss: 1.010\nTime elapsed: 3.13 min\nEpoch: 006/020 || Batch: 0000/0391 || Cost: 0.8952\nEpoch: 006/020 || Batch: 0050/0391 || Cost: 0.9495\nEpoch: 006/020 || Batch: 0100/0391 || Cost: 0.9144\nEpoch: 006/020 || Batch: 0150/0391 || Cost: 1.0670\nEpoch: 006/020 || Batch: 0200/0391 || Cost: 0.9143\nEpoch: 006/020 || Batch: 0250/0391 || Cost: 0.8239\nEpoch: 006/020 || Batch: 0300/0391 || Cost: 0.9856\nEpoch: 006/020 || Batch: 0350/0391 || Cost: 1.0474\nEpoch: 006/020 || Train: 66.174% | Loss: 0.950\nTime elapsed: 3.73 min\nEpoch: 007/020 || Batch: 0000/0391 || Cost: 1.0345\nEpoch: 007/020 || Batch: 0050/0391 || Cost: 1.0153\nEpoch: 007/020 || Batch: 0100/0391 || Cost: 1.0117\nEpoch: 007/020 || Batch: 0150/0391 || Cost: 1.0856\nEpoch: 007/020 || Batch: 0200/0391 || Cost: 0.9242\nEpoch: 007/020 || Batch: 0250/0391 || Cost: 0.9198\nEpoch: 007/020 || Batch: 0300/0391 || Cost: 1.0151\nEpoch: 007/020 || Batch: 0350/0391 || Cost: 0.8984\nEpoch: 007/020 || Train: 65.398% | Loss: 0.946\nTime elapsed: 4.37 min\nEpoch: 008/020 || Batch: 0000/0391 || Cost: 1.1602\nEpoch: 008/020 || Batch: 0050/0391 || Cost: 0.8792\nEpoch: 008/020 || Batch: 0100/0391 || Cost: 0.7765\nEpoch: 008/020 || Batch: 0150/0391 || Cost: 0.9477\nEpoch: 008/020 || Batch: 0200/0391 || Cost: 1.0941\nEpoch: 008/020 || Batch: 0250/0391 || Cost: 0.9902\nEpoch: 008/020 || Batch: 0300/0391 || Cost: 0.7780\nEpoch: 008/020 || Batch: 0350/0391 || Cost: 0.9763\nEpoch: 008/020 || Train: 71.794% | Loss: 0.811\nTime elapsed: 4.98 min\nEpoch: 009/020 || Batch: 0000/0391 || Cost: 0.7702\nEpoch: 009/020 || Batch: 0050/0391 || Cost: 0.6462\nEpoch: 009/020 || Batch: 0100/0391 || Cost: 0.8698\nEpoch: 009/020 || Batch: 0150/0391 || Cost: 0.8418\nEpoch: 009/020 || Batch: 0200/0391 || Cost: 0.8814\nEpoch: 009/020 || Batch: 0250/0391 || Cost: 0.8978\nEpoch: 009/020 || Batch: 0300/0391 || Cost: 0.6661\nEpoch: 009/020 || Batch: 0350/0391 || Cost: 0.6781\nEpoch: 009/020 || Train: 76.826% | Loss: 0.672\nTime elapsed: 5.60 min\nEpoch: 010/020 || Batch: 0000/0391 || Cost: 0.6360\nEpoch: 010/020 || Batch: 0050/0391 || Cost: 0.8369\nEpoch: 010/020 || Batch: 0100/0391 || Cost: 0.6888\nEpoch: 010/020 || Batch: 0150/0391 || Cost: 0.6364\nEpoch: 010/020 || Batch: 0200/0391 || Cost: 0.8453\nEpoch: 010/020 || Batch: 0250/0391 || Cost: 0.5975\nEpoch: 010/020 || Batch: 0300/0391 || Cost: 0.7399\nEpoch: 010/020 || Batch: 0350/0391 || Cost: 0.7032\nEpoch: 010/020 || Train: 75.024% | Loss: 0.707\nTime elapsed: 6.23 min\nEpoch: 011/020 || Batch: 0000/0391 || Cost: 0.6517\nEpoch: 011/020 || Batch: 0050/0391 || Cost: 0.6491\nEpoch: 011/020 || Batch: 0100/0391 || Cost: 0.6896\nEpoch: 011/020 || Batch: 0150/0391 || Cost: 0.7743\nEpoch: 011/020 || Batch: 0200/0391 || Cost: 0.6602\nEpoch: 011/020 || Batch: 0250/0391 || Cost: 0.6638\nEpoch: 011/020 || Batch: 0300/0391 || Cost: 0.7198\nEpoch: 011/020 || Batch: 0350/0391 || Cost: 0.6420\nEpoch: 011/020 || Train: 75.076% | Loss: 0.709\nTime elapsed: 6.84 min\nEpoch: 012/020 || Batch: 0000/0391 || Cost: 0.6865\nEpoch: 012/020 || Batch: 0050/0391 || Cost: 0.6252\nEpoch: 012/020 || Batch: 0100/0391 || Cost: 0.6018\nEpoch: 012/020 || Batch: 0150/0391 || Cost: 0.6398\nEpoch: 012/020 || Batch: 0200/0391 || Cost: 0.6128\nEpoch: 012/020 || Batch: 0250/0391 || Cost: 0.6975\nEpoch: 012/020 || Batch: 0300/0391 || Cost: 0.6602\nEpoch: 012/020 || Batch: 0350/0391 || Cost: 0.7103\nEpoch: 012/020 || Train: 82.296% | Loss: 0.508\nTime elapsed: 7.46 min\nEpoch: 013/020 || Batch: 0000/0391 || Cost: 0.4227\nEpoch: 013/020 || Batch: 0050/0391 || Cost: 0.4474\nEpoch: 013/020 || Batch: 0100/0391 || Cost: 0.5210\nEpoch: 013/020 || Batch: 0150/0391 || Cost: 0.5048\nEpoch: 013/020 || Batch: 0200/0391 || Cost: 0.6530\nEpoch: 013/020 || Batch: 0250/0391 || Cost: 0.6788\nEpoch: 013/020 || Batch: 0300/0391 || Cost: 0.4752\nEpoch: 013/020 || Batch: 0350/0391 || Cost: 0.6266\nEpoch: 013/020 || Train: 82.600% | Loss: 0.519\nTime elapsed: 8.07 min\nEpoch: 014/020 || Batch: 0000/0391 || Cost: 0.3488\nEpoch: 014/020 || Batch: 0050/0391 || Cost: 0.4485\nEpoch: 014/020 || Batch: 0100/0391 || Cost: 0.5644\nEpoch: 014/020 || Batch: 0150/0391 || Cost: 0.5577\nEpoch: 014/020 || Batch: 0200/0391 || Cost: 0.6581\nEpoch: 014/020 || Batch: 0250/0391 || Cost: 0.6945\nEpoch: 014/020 || Batch: 0300/0391 || Cost: 0.5558\nEpoch: 014/020 || Batch: 0350/0391 || Cost: 0.7700\nEpoch: 014/020 || Train: 79.716% | Loss: 0.585\nTime elapsed: 8.69 min\nEpoch: 015/020 || Batch: 0000/0391 || Cost: 0.5291\nEpoch: 015/020 || Batch: 0050/0391 || Cost: 0.5004\nEpoch: 015/020 || Batch: 0100/0391 || Cost: 0.5694\nEpoch: 015/020 || Batch: 0150/0391 || Cost: 0.5308\nEpoch: 015/020 || Batch: 0200/0391 || Cost: 0.6667\nEpoch: 015/020 || Batch: 0250/0391 || Cost: 0.5823\nEpoch: 015/020 || Batch: 0300/0391 || Cost: 0.6025\nEpoch: 015/020 || Batch: 0350/0391 || Cost: 0.5379\nEpoch: 015/020 || Train: 83.928% | Loss: 0.463\nTime elapsed: 9.33 min\nEpoch: 016/020 || Batch: 0000/0391 || Cost: 0.5477\nEpoch: 016/020 || Batch: 0050/0391 || Cost: 0.4614\nEpoch: 016/020 || Batch: 0100/0391 || Cost: 0.4030\nEpoch: 016/020 || Batch: 0150/0391 || Cost: 0.4408\nEpoch: 016/020 || Batch: 0200/0391 || Cost: 0.4655\nEpoch: 016/020 || Batch: 0250/0391 || Cost: 0.4991\nEpoch: 016/020 || Batch: 0300/0391 || Cost: 0.5968\nEpoch: 016/020 || Batch: 0350/0391 || Cost: 0.4851\nEpoch: 016/020 || Train: 83.956% | Loss: 0.478\nTime elapsed: 9.94 min\nEpoch: 017/020 || Batch: 0000/0391 || Cost: 0.5601\nEpoch: 017/020 || Batch: 0050/0391 || Cost: 0.4327\nEpoch: 017/020 || Batch: 0100/0391 || Cost: 0.4977\nEpoch: 017/020 || Batch: 0150/0391 || Cost: 0.5847\nEpoch: 017/020 || Batch: 0200/0391 || Cost: 0.4495\nEpoch: 017/020 || Batch: 0250/0391 || Cost: 0.3618\nEpoch: 017/020 || Batch: 0300/0391 || Cost: 0.5533\nEpoch: 017/020 || Batch: 0350/0391 || Cost: 0.4590\nEpoch: 017/020 || Train: 86.964% | Loss: 0.389\nTime elapsed: 10.55 min\nEpoch: 018/020 || Batch: 0000/0391 || Cost: 0.2505\nEpoch: 018/020 || Batch: 0050/0391 || Cost: 0.4487\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 018/020 || Batch: 0100/0391 || Cost: 0.6566\nEpoch: 018/020 || Batch: 0150/0391 || Cost: 0.3629\nEpoch: 018/020 || Batch: 0200/0391 || Cost: 0.3451\nEpoch: 018/020 || Batch: 0250/0391 || Cost: 0.3916\nEpoch: 018/020 || Batch: 0300/0391 || Cost: 0.4990\nEpoch: 018/020 || Batch: 0350/0391 || Cost: 0.6505\nEpoch: 018/020 || Train: 87.032% | Loss: 0.374\nTime elapsed: 11.18 min\nEpoch: 019/020 || Batch: 0000/0391 || Cost: 0.3114\nEpoch: 019/020 || Batch: 0050/0391 || Cost: 0.5267\nEpoch: 019/020 || Batch: 0100/0391 || Cost: 0.4429\nEpoch: 019/020 || Batch: 0150/0391 || Cost: 0.3149\nEpoch: 019/020 || Batch: 0200/0391 || Cost: 0.4357\nEpoch: 019/020 || Batch: 0250/0391 || Cost: 0.5742\nEpoch: 019/020 || Batch: 0300/0391 || Cost: 0.5342\nEpoch: 019/020 || Batch: 0350/0391 || Cost: 0.4330\nEpoch: 019/020 || Train: 87.090% | Loss: 0.381\nTime elapsed: 11.78 min\nEpoch: 020/020 || Batch: 0000/0391 || Cost: 0.3811\nEpoch: 020/020 || Batch: 0050/0391 || Cost: 0.3974\nEpoch: 020/020 || Batch: 0100/0391 || Cost: 0.5267\nEpoch: 020/020 || Batch: 0150/0391 || Cost: 0.4021\nEpoch: 020/020 || Batch: 0200/0391 || Cost: 0.5209\nEpoch: 020/020 || Batch: 0250/0391 || Cost: 0.3280\nEpoch: 020/020 || Batch: 0300/0391 || Cost: 0.4617\nEpoch: 020/020 || Batch: 0350/0391 || Cost: 0.4279\nEpoch: 020/020 || Train: 89.636% | Loss: 0.311\nTime elapsed: 12.40 min\nTotal Training Time: 12.40 min\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print(\"Test Accuracy: %.2f%%\" % (compute_accuracy(model, test_loader, device)))","execution_count":6,"outputs":[{"output_type":"stream","text":"Test Accuracy: 76.49%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip freeze | grep \"torch\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}