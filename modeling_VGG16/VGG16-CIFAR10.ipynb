{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"##################\n# Settings\n#################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 128\n\n# Architecture\nnum_features = 784\nnum_classes = 10","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################\n# CIFAR10\n#####################\n\ntrain_dataset = datasets.CIFAR10(root=\"data\",\n                                 train=True,\n                                 transform=transforms.ToTensor(),\n                                 download=True\n                                )\n\ntest_dataset = datasets.CIFAR10(root=\"data\",\n                                train=False,\n                                transform=transforms.ToTensor()\n                               )\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=4\n                         )\n\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         num_workers=4\n                        )\n\nfor images, labels in train_loader:\n    print(\"Image Dimensions: \", images.shape)\n    print(\"Label Dimensions: \", labels.shape)\n    break","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ed43e2d007490fa89ff7746da42686"}},"metadata":{}},{"output_type":"stream","text":"Extracting data/cifar-10-python.tar.gz to data\n\n\n\n\nImage Dimensions:  torch.Size([128, 3, 32, 32])\nLabel Dimensions:  torch.Size([128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VGG16(nn.Module):\n    \n    def __init__(self, num_classes, num_features):\n        super(VGG16, self).__init__()\n        \n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, \n                      out_channels=64,\n                      kernel_size=(3, 3), \n                      stride=(1, 1),\n                      # (1(32-1)-32 + 3)/2 = 1\n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, \n                      out_channels=64, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, \n                      out_channels=128, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128,\n                      out_channels=128, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, \n                      out_channels=256, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.block_4 = nn.Sequential(\n            nn.Conv2d(in_channels=256, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.block_5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, \n                      out_channels=512, \n                      kernel_size=(3, 3), \n                      stride=(1, 1), \n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), \n                         stride=(2, 2))\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512, 4096),\n            nn.ReLU(True),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Linear(4096, num_classes)\n        )\n        \n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    m.bias.detach().zero_()\n    \n    def forward(self, x):\n        x = self.block_1(x)\n        x = self.block_2(x)\n        x = self.block_3(x)\n        x = self.block_4(x)\n        x = self.block_5(x)\n        x = x.view(x.size(0), -1)\n        logits = self.classifier(x)\n        probas = F.softmax(logits, dim=1)\n        return logits, probas\n\n\ntorch.manual_seed(random_seed)\nmodel = VGG16(num_features = num_features, num_classes = num_classes)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\ndef compute_accuracy(model, data_loader):\n    model.eval()\n    correct_pred, num_examples = 0, 0\n    for i, (features, targets) in enumerate(data_loader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n        \n        logits, probas = model(features)\n        _, predicted_labels = torch.max(probas, 1)\n        num_examples += targets.size(0)\n        correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float() / num_examples * 100\n\ndef compute_epoch_loss(model, data_loader):\n    model.eval()\n    curr_loss, num_examples = 0., 0\n    with torch.no_grad():\n        for features, targets in data_loader:\n            features = features.to(device)\n            targets = targets.to(device)\n            logits, probas = model(features)\n            loss = F.cross_entropy(logits, targets, reduction=\"sum\")\n            num_examples += targets.size(0)\n            curr_loss += loss\n        curr_loss = curr_loss / num_examples\n        return curr_loss\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (features, targets) in enumerate(train_loader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n        \n        # Forward and Back Prop\n        logits, probas = model(features)\n        cost = F.cross_entropy(logits, targets)\n        optimizer.zero_grad()\n        \n        cost.backward()\n        \n        # Update model parameters\n        optimizer.step()\n        \n        # LOGGING\n        if not batch_idx % 50:\n            print(\"Epoch: %03d/%03d || Batch: %03d/%03d || Cost: %.4f\" % (epoch+1, num_epochs, batch_idx, len(train_loader), cost))\n    \n    model.eval()\n    with torch.set_grad_enabled(False):\n        print(\"Epoch: %03d/%03d || Train: %.3f%% || Loss: %.3f\" % (epoch+1, num_epochs, compute_accuracy(model, train_loader), compute_epoch_loss(model, train_loader)))\n    \n    print(\"Time Elapsed: %.2f min\" % ((time.time()-start_time)/60))\nprint(\"Total Training Time: %.2f min\" % ((time.time()-start_time)/60))","execution_count":5,"outputs":[{"output_type":"stream","text":"Epoch: 001/010 || Batch: 000/391 || Cost: 2.4443\nEpoch: 001/010 || Batch: 050/391 || Cost: 2.1414\nEpoch: 001/010 || Batch: 100/391 || Cost: 2.0844\nEpoch: 001/010 || Batch: 150/391 || Cost: 1.9001\nEpoch: 001/010 || Batch: 200/391 || Cost: 1.9626\nEpoch: 001/010 || Batch: 250/391 || Cost: 1.7157\nEpoch: 001/010 || Batch: 300/391 || Cost: 1.7874\nEpoch: 001/010 || Batch: 350/391 || Cost: 1.7342\nEpoch: 001/010 || Train: 37.350% || Loss: 1.603\nTime Elapsed: 0.71 min\nEpoch: 002/010 || Batch: 000/391 || Cost: 1.6161\nEpoch: 002/010 || Batch: 050/391 || Cost: 1.6217\nEpoch: 002/010 || Batch: 100/391 || Cost: 1.5814\nEpoch: 002/010 || Batch: 150/391 || Cost: 1.4906\nEpoch: 002/010 || Batch: 200/391 || Cost: 1.2700\nEpoch: 002/010 || Batch: 250/391 || Cost: 1.4568\nEpoch: 002/010 || Batch: 300/391 || Cost: 1.2954\nEpoch: 002/010 || Batch: 350/391 || Cost: 1.2199\nEpoch: 002/010 || Train: 53.450% || Loss: 1.295\nTime Elapsed: 1.40 min\nEpoch: 003/010 || Batch: 000/391 || Cost: 1.3097\nEpoch: 003/010 || Batch: 050/391 || Cost: 1.2110\nEpoch: 003/010 || Batch: 100/391 || Cost: 1.2770\nEpoch: 003/010 || Batch: 150/391 || Cost: 1.2915\nEpoch: 003/010 || Batch: 200/391 || Cost: 1.2287\nEpoch: 003/010 || Batch: 250/391 || Cost: 1.0488\nEpoch: 003/010 || Batch: 300/391 || Cost: 1.1009\nEpoch: 003/010 || Batch: 350/391 || Cost: 1.1072\nEpoch: 003/010 || Train: 65.546% || Loss: 0.974\nTime Elapsed: 2.09 min\nEpoch: 004/010 || Batch: 000/391 || Cost: 0.8407\nEpoch: 004/010 || Batch: 050/391 || Cost: 0.8977\nEpoch: 004/010 || Batch: 100/391 || Cost: 1.0478\nEpoch: 004/010 || Batch: 150/391 || Cost: 0.9562\nEpoch: 004/010 || Batch: 200/391 || Cost: 0.8200\nEpoch: 004/010 || Batch: 250/391 || Cost: 1.0271\nEpoch: 004/010 || Batch: 300/391 || Cost: 0.8445\nEpoch: 004/010 || Batch: 350/391 || Cost: 0.8759\nEpoch: 004/010 || Train: 68.648% || Loss: 0.892\nTime Elapsed: 2.78 min\nEpoch: 005/010 || Batch: 000/391 || Cost: 0.8707\nEpoch: 005/010 || Batch: 050/391 || Cost: 0.7266\nEpoch: 005/010 || Batch: 100/391 || Cost: 0.8249\nEpoch: 005/010 || Batch: 150/391 || Cost: 0.7969\nEpoch: 005/010 || Batch: 200/391 || Cost: 0.8336\nEpoch: 005/010 || Batch: 250/391 || Cost: 0.6655\nEpoch: 005/010 || Batch: 300/391 || Cost: 0.6739\nEpoch: 005/010 || Batch: 350/391 || Cost: 0.7225\nEpoch: 005/010 || Train: 75.620% || Loss: 0.706\nTime Elapsed: 3.47 min\nEpoch: 006/010 || Batch: 000/391 || Cost: 0.6448\nEpoch: 006/010 || Batch: 050/391 || Cost: 0.6916\nEpoch: 006/010 || Batch: 100/391 || Cost: 0.8241\nEpoch: 006/010 || Batch: 150/391 || Cost: 0.5239\nEpoch: 006/010 || Batch: 200/391 || Cost: 0.5010\nEpoch: 006/010 || Batch: 250/391 || Cost: 0.6448\nEpoch: 006/010 || Batch: 300/391 || Cost: 0.7804\nEpoch: 006/010 || Batch: 350/391 || Cost: 0.7369\nEpoch: 006/010 || Train: 83.730% || Loss: 0.504\nTime Elapsed: 4.15 min\nEpoch: 007/010 || Batch: 000/391 || Cost: 0.4548\nEpoch: 007/010 || Batch: 050/391 || Cost: 0.4950\nEpoch: 007/010 || Batch: 100/391 || Cost: 0.7251\nEpoch: 007/010 || Batch: 150/391 || Cost: 0.7578\nEpoch: 007/010 || Batch: 200/391 || Cost: 0.5026\nEpoch: 007/010 || Batch: 250/391 || Cost: 0.5906\nEpoch: 007/010 || Batch: 300/391 || Cost: 0.6490\nEpoch: 007/010 || Batch: 350/391 || Cost: 0.6062\nEpoch: 007/010 || Train: 84.716% || Loss: 0.452\nTime Elapsed: 4.84 min\nEpoch: 008/010 || Batch: 000/391 || Cost: 0.4977\nEpoch: 008/010 || Batch: 050/391 || Cost: 0.4268\nEpoch: 008/010 || Batch: 100/391 || Cost: 0.3664\nEpoch: 008/010 || Batch: 150/391 || Cost: 0.4788\nEpoch: 008/010 || Batch: 200/391 || Cost: 0.5027\nEpoch: 008/010 || Batch: 250/391 || Cost: 0.5179\nEpoch: 008/010 || Batch: 300/391 || Cost: 0.6418\nEpoch: 008/010 || Batch: 350/391 || Cost: 0.4500\nEpoch: 008/010 || Train: 87.416% || Loss: 0.382\nTime Elapsed: 5.53 min\nEpoch: 009/010 || Batch: 000/391 || Cost: 0.4011\nEpoch: 009/010 || Batch: 050/391 || Cost: 0.4231\nEpoch: 009/010 || Batch: 100/391 || Cost: 0.4717\nEpoch: 009/010 || Batch: 150/391 || Cost: 0.4843\nEpoch: 009/010 || Batch: 200/391 || Cost: 0.4827\nEpoch: 009/010 || Batch: 250/391 || Cost: 0.3873\nEpoch: 009/010 || Batch: 300/391 || Cost: 0.4541\nEpoch: 009/010 || Batch: 350/391 || Cost: 0.3592\nEpoch: 009/010 || Train: 89.928% || Loss: 0.308\nTime Elapsed: 6.23 min\nEpoch: 010/010 || Batch: 000/391 || Cost: 0.2523\nEpoch: 010/010 || Batch: 050/391 || Cost: 0.2048\nEpoch: 010/010 || Batch: 100/391 || Cost: 0.2806\nEpoch: 010/010 || Batch: 150/391 || Cost: 0.3124\nEpoch: 010/010 || Batch: 200/391 || Cost: 0.4267\nEpoch: 010/010 || Batch: 250/391 || Cost: 0.3718\nEpoch: 010/010 || Batch: 300/391 || Cost: 0.4478\nEpoch: 010/010 || Batch: 350/391 || Cost: 0.3876\nEpoch: 010/010 || Train: 89.530% || Loss: 0.330\nTime Elapsed: 6.91 min\nTotal Training Time: 6.91 min\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":" with torch.set_grad_enabled(False):\n        print(\"Test accuracy: %.2f%%\" % (compute_accuracy(model, test_loader)))","execution_count":6,"outputs":[{"output_type":"stream","text":"Test accuracy: 76.04%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip freeze | grep \"torch\"","execution_count":7,"outputs":[{"output_type":"stream","text":"UsageError: Line magic function `%watermark` not found.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}